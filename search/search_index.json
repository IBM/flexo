{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Flexo","text":"<p>The Flexo Agent Library is a powerful and flexible codebase that enables users to configure, customize, and deploy a generative AI agent. Designed for adaptability, the library can be tailored to a wide range of use cases, from conversational AI to specialized automation.</p>"},{"location":"#why-flexo","title":"Why Flexo?","text":"<ul> <li>Simplified Deployment: Deploy anywhere with comprehensive platform guides</li> <li>Production Ready: Built for scalability and reliability</li> <li>Extensible: Add custom tools and capabilities</li> <li>Well Documented: Clear guides for every step</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Configurable Agent: YAML-based configuration for custom behaviors</li> <li>Tool Integration: Execute Python functions and REST API calls</li> <li>Streaming Support: Real-time streaming with pattern detection</li> <li>Production Ready: Containerized deployment support with logging</li> <li>FastAPI Backend: Modern async API with comprehensive docs</li> </ul>"},{"location":"#supported-llm-providers","title":"Supported LLM Providers","text":"\u2601\ufe0f Cloud Providers <p>OpenAI</p> <p>GPT-powered models</p> <p>watsonx.ai</p> <p>Enterprise AI solutions</p> <p>Anthropic</p> <p>Claude family models</p> <p>xAI</p> <p>Grok and beyond</p> <p>Mistral AI</p> <p>Efficient open models</p> \ud83d\udda5\ufe0f Local &amp; Self-Hosted Options <p>High-throughput serving</p> <p>Ollama</p> <p>Easy local LLMs</p> <p>Optimized C++ runtime</p> <p>LM Studio</p> <p>User-friendly interface</p> <p>LocalAI</p> <p>Self-hosted versatility</p>"},{"location":"#unified-configuration-interface","title":"\u2699\ufe0f Unified Configuration Interface","text":"<p>Switch providers effortlessly with Flexo's adapter layer. Customize your LLM settings in one place:</p> <pre><code>gpt-4o:\n  provider: \"openai\"  # Choose your provider\n  model: \"gpt-4o\"     # Select specific model\n  temperature: 0.7\n  max_tokens: 4000    # Additional model-specific parameters\n</code></pre> <p>Need more details? Check our comprehensive Model Configuration Guide for provider-specific settings and optimization tips.</p>"},{"location":"#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"#1-local-development","title":"1. Local Development","text":"<p>Start developing with Flexo locally:</p> <ul> <li>Configure Your Agent</li> <li>Run the Server</li> <li>Build from Source</li> </ul>"},{"location":"#2-production-deployment","title":"2. Production Deployment","text":"<p>Deploy Flexo to your preferred platform:</p> Platform Best For Guide IBM Code Engine Serverless, pay-per-use Deploy \u2192 AWS Fargate AWS integration Deploy \u2192 OpenShift Enterprise, hybrid cloud Deploy \u2192 Kubernetes Custom infrastructure Deploy \u2192"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#deployment-guides","title":"Deployment Guides","text":"<ul> <li>Container Registries</li> <li>Platform Deployment</li> </ul>"},{"location":"#code-reference","title":"Code Reference","text":"<ul> <li>Agent</li> <li>API Reference</li> <li>Model Configuration</li> <li>Tools System</li> <li>Data Models</li> <li>Database Integration</li> </ul>"},{"location":"#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    Client[Client] --&gt; API[FastAPI Server]\n\n    subgraph API[\"API Layer\"]\n        Router[Chat Completions Router]\n        SSE[SSE Models]\n        Validation[Request Validation]\n    end\n\n    subgraph Agent[\"Agent Layer\"]\n        ChatAgent[Streaming Chat Agent]\n        State[State Management]\n        Config[Configuration]\n    end\n\n    subgraph LLM[\"LLM Layer\"]\n        Factory[LLM Factory]\n        Detection[Tool Detection]\n        Builders[Prompt Builders]\n\n        subgraph Adapters[\"LLM Adapters\"]\n            WatsonX\n            OpenAI\n            Anthropic\n            Mistral\n        end\n    end\n\n    subgraph Tools[\"Tools Layer\"]\n        Registry[Tool Registry]\n        REST[REST Tools]\n        NonREST[\"Non-REST Tools: Python etc\"]\n        ExampleTools[\"Included Example \n        Tools: RAG + Weather\"]\n    end\n\n    subgraph Database[\"Database Layer\"]\n        ES[Elasticsearch]\n        Milvus[Milvus]\n    end\n\n    API --&gt; Agent\n    Agent --&gt; LLM\n    Agent --&gt; Tools\n    Tools --&gt; Database\n\n    style API stroke:#ff69b4,stroke-width:4px\n    style Agent stroke:#4169e1,stroke-width:4px\n    style LLM stroke:#228b22,stroke-width:4px\n    style Tools stroke:#cd853f,stroke-width:4px\n    style Database stroke:#4682b4,stroke-width:4px\n\n    style Router stroke:#ff69b4,stroke-width:2px\n    style SSE stroke:#ff69b4,stroke-width:2px\n    style Validation stroke:#ff69b4,stroke-width:2px\n\n    style ChatAgent stroke:#4169e1,stroke-width:2px\n    style State stroke:#4169e1,stroke-width:2px\n    style Config stroke:#4169e1,stroke-width:2px\n\n    style Factory stroke:#228b22,stroke-width:2px\n    style Detection stroke:#228b22,stroke-width:2px\n    style Builders stroke:#228b22,stroke-width:2px\n    style Adapters stroke:#228b22,stroke-width:2px\n    style WatsonX stroke:#228b22,stroke-width:2px\n    style OpenAI stroke:#228b22,stroke-width:2px\n    style Anthropic stroke:#228b22,stroke-width:2px\n    style Mistral stroke:#228b22,stroke-width:2px\n\n    style Registry stroke:#cd853f,stroke-width:2px\n    style ExampleTools stroke:#cd853f,stroke-width:2px\n    style REST stroke:#cd853f,stroke-width:2px\n    style NonREST stroke:#cd853f,stroke-width:2px\n\n    style ES stroke:#4682b4,stroke-width:2px\n    style Milvus stroke:#4682b4,stroke-width:2px\n\n    style Client stroke:#333,stroke-width:2px</code></pre>"},{"location":"#chat-agent-state-flow","title":"Chat Agent State Flow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; STREAMING: Initialize Agent\n\n    STREAMING --&gt; TOOL_DETECTION: Tool Call Found\n    STREAMING --&gt; COMPLETING: Generation Done\n\n    TOOL_DETECTION --&gt; EXECUTING_TOOLS: Process Tool Call\n    EXECUTING_TOOLS --&gt; INTERMEDIATE: Tool Execution Complete\n\n    INTERMEDIATE --&gt; STREAMING: Continue Generation\n    INTERMEDIATE --&gt; COMPLETING: Max Iterations\n\n    COMPLETING --&gt; [*]: End Response\n\n    note right of STREAMING\n        Main generation state\n        Handles LLM responses\n    end note\n\n    note right of EXECUTING_TOOLS\n        Concurrent tool execution\n        Error handling\n    end note</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>See our Contributing Guide for details.</p>"},{"location":"#security","title":"Security","text":"<p>For security concerns, please review our Security Policy.</p>"},{"location":"agent-configuration/","title":"Agent Configuration Guide","text":""},{"location":"agent-configuration/#overview","title":"Overview","text":"<p>This guide explains how to configure the Flexo agent's behavior through the <code>agent.yaml</code> configuration file.</p>"},{"location":"agent-configuration/#configuration-structure","title":"Configuration Structure","text":""},{"location":"agent-configuration/#basic-settings","title":"Basic Settings","text":"<pre><code>name: flexo\nhistory_limit: 4\ntimeouts:\n  model_response_timeout: 60\nmax_streaming_iterations: 2\ndetection_mode: vendor\nuse_vendor_chat_completions: true\n</code></pre>"},{"location":"agent-configuration/#key-configurations","title":"Key Configurations:","text":"<ul> <li><code>detection_mode</code>: Supports <code>'vendor'</code> or <code>'manual'</code> tool call detection (corresponds to <code>VendorToolCallDetectionStrategy</code> or <code>ManualToolCallDetectionStrategy</code>).</li> <li><code>use_vendor_chat_completions</code>: Enables the use of the chat completions API instead of the text generation endpoint.</li> <li><code>history_limit</code>: Controls the conversation context window by limiting the number of previous messages retained.</li> <li><code>max_streaming_iterations</code>: Limits the number of times the streaming state can be entered in a single session to prevent looping. Must be set to 2+ when using tools.</li> </ul>"},{"location":"agent-configuration/#system-prompt","title":"System Prompt","text":"<pre><code>system_prompt: |\n  I am a helpful AI assistant focused on clear, accurate, and direct communication. \n  I solve problems systematically and explain my reasoning when needed. \n  I maintain a professional yet approachable tone.\n\n  CORE BEHAVIORS:\n  - Communicate clearly and concisely\n  - Break down complex problems step-by-step\n  - Ask clarifying questions when truly needed\n  - Acknowledge limitations and uncertainties honestly\n  - Validate assumptions before proceeding\n\n  TOOL USAGE:\n  - Call tool(s) immediately as needed \n  - Do not reference tools or their formatting in your response\n  - Use tools only when they genuinely enhance the response\n  - Handle errors gracefully with clear explanations\n  - Show key results and interpret them in context\n  - Suggest alternatives if the primary approach fails\n\n  I adapt my style to user needs while staying focused on providing valuable, \n  actionable responses.\n</code></pre>"},{"location":"agent-configuration/#models-configuration","title":"Models Configuration","text":"<pre><code>models_config:\n  main_chat_model:\n    vendor: watsonx-llama  # Supported: anthropic, openai, watsonx (llama, mistral, granite), mistral-ai\n    model_id: meta-llama/llama-3-405b-instruct\n    decoding_method: greedy\n    max_new_tokens: 4000\n</code></pre>"},{"location":"agent-configuration/#tool-configuration","title":"Tool Configuration","text":"<p>Tools are configured as a list under <code>tools_config</code>. The name specified in each tool configuration is used to search for the corresponding implementation.</p> <pre><code>tools_config:\n  # Weather API Integration\n  - name: \"weather\"\n    endpoint_url: \"https://api.openweathermap.org/data/2.5/weather\"\n    api_key_env: \"OWM_API_KEY\"\n\n  # Wikipedia Summary Tool\n  - name: \"wikipedia\"\n    endpoint_url: \"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{encoded_query}\"\n\n  # DuckDuckGo Search Tool\n  - name: \"duckduckgo_search\"\n</code></pre>"},{"location":"agent-configuration/#rag-tool-example-elasticsearch","title":"RAG Tool Example (Elasticsearch)","text":"<pre><code>tools_config:\n  - name: \"medicare_search\"\n    connector_config:\n      connector_type: elasticsearch\n      index_name: my_index\n      api_key_env: ES_API_KEY\n      endpoint_env: ES_ENDPOINT\n      top_k: 5\n      timeout: 30\n      max_retries: 3\n      query_body:\n        _source: false\n        fields: [\"text\"]\n        query:\n          bool:\n            must:\n              - match:\n                  text:\n                    query: \"$USER_INPUT\"\n                    boost: 3.5\n        knn:\n          field: vector\n          query_vector_builder:\n            text_embedding:\n              model_id: thenlper__gte-base\n              model_text: \"$USER_INPUT\"\n          k: 100\n          num_candidates: 150\n        rank:\n          rrf:\n            rank_window_size: 40\n</code></pre>"},{"location":"agent-configuration/#implementation-details","title":"Implementation Details","text":""},{"location":"agent-configuration/#tool-detection-strategies","title":"Tool Detection Strategies","text":"<p>Based on the codebase's <code>tool_detection</code> module:</p> <ol> <li> <p>Vendor Detection (<code>detection_mode: vendor</code>):</p> <ul> <li>Uses vendor's native tool calling capabilities</li> <li>Handled by <code>VendorToolCallDetectionStrategy</code></li> </ul> </li> <li> <p>Manual Detection (<code>detection_mode: manual</code>):</p> <ul> <li>Custom pattern-matching for tool calls</li> <li>Handled by <code>ManualToolCallDetectionStrategy</code></li> </ul> </li> </ol>"},{"location":"agent-configuration/#prompt-builders","title":"Prompt Builders","text":"<p>The system automatically selects the appropriate prompt builder based on the vendor:</p> <ul> <li><code>AnthropicPromptBuilder</code></li> <li><code>OpenAIPromptBuilder</code></li> <li><code>MistralAIPromptBuilder</code></li> <li><code>GranitePromptBuilder</code></li> <li><code>LlamaPromptBuilder</code></li> <li><code>MistralPromptBuilder</code></li> </ul>"},{"location":"agent-configuration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Tool Configuration</p> <ul> <li>Use environment variables for sensitive credentials</li> <li>Configure appropriate timeouts and retry logic</li> <li>Set meaningful tool descriptions</li> <li>Use the appropriate tool name to link to implementation</li> </ul> </li> <li> <p>System Prompt</p> <ul> <li>Keep instructions clear and specific</li> <li>Include explicit tool usage guidelines</li> <li>Define error handling preferences</li> </ul> </li> </ol>"},{"location":"agent-configuration/#related-documentation","title":"Related Documentation","text":"<ul> <li>See <code>tools/</code> documentation for detailed tool configuration</li> <li>Check <code>llm/adapters/</code> for vendor-specific options</li> <li>Review <code>prompt_builders/</code> for prompt customization</li> </ul>"},{"location":"api/","title":"API Documentation","text":"<p>The generative AI agent exposes a FastAPI-based RESTful API for interacting with the model. This document provides an overview of the API, key endpoints, and how to access the full API documentation.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The API allows you to interact with the AI agent via HTTP requests. It supports:</p> <ul> <li>Sending messages to the AI agent.</li> <li>Streaming conversational responses.</li> <li>Providing contextual data for more tailored responses.</li> </ul>"},{"location":"api/#base-url","title":"Base URL","text":"<p>The API server runs on the following base URL when deployed locally: <pre><code>http://127.0.0.1:8000\n</code></pre></p> <p>For production deployments, replace the base URL with your server's address.</p>"},{"location":"api/#full-api-documentation","title":"Full API Documentation","text":"<p>The full API documentation is available in OpenAPI format. When the app is running, you can view it interactively using:</p> <ul> <li>Swagger UI: http://127.0.0.1:8000/docs</li> <li>ReDoc: http://127.0.0.1:8000/redoc</li> </ul> <p>To access the OpenAPI JSON specification directly: - Download openapi.json</p>"},{"location":"api/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/#1-post-v1chatcompletions","title":"1. POST <code>/v1/chat/completions</code>","text":"<p>Streams a conversational response from the AI agent.</p>"},{"location":"api/#request","title":"Request","text":"<ul> <li>Headers:</li> <li><code>Content-Type: application/json</code></li> <li>Body:   <pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        { \"text\": \"Hello, AI!\" }\n      ]\n    }\n  ],\n  \"context\": {\n    \"values\": [\n      { \"key\": \"session_id\", \"value\": \"abc123\" }\n    ]\n  },\n  \"include_status_updates\": false,\n  \"stream\": true\n}\n</code></pre></li> </ul>"},{"location":"api/#response","title":"Response","text":"<ul> <li>Success:   <pre><code>{\n  \"id\": \"abc123\",\n  \"object\": \"thread.message.delta\",\n  \"thread_id\": \"None\",\n  \"choices\": [\n    {\n      \"delta\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I assist you?\"\n      }\n    }\n  ]\n}\n</code></pre></li> <li>Error:   <pre><code>{\n  \"error\": \"Invalid input format\",\n  \"choices\": [\n    {\n      \"delta\": {\n        \"role\": \"assistant\",\n        \"content\": \"There seems to be an issue with the provided request format. Please check your input and try again.\"\n      }\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"api/#2-get-docs","title":"2. GET <code>/docs</code>","text":"<p>Provides interactive Swagger documentation for exploring the API.</p>"},{"location":"api/#3-get-redoc","title":"3. GET <code>/redoc</code>","text":"<p>Provides a ReDoc-based view of the API schema.</p>"},{"location":"api/#using-the-openapi-schema","title":"Using the OpenAPI Schema","text":"<p>The OpenAPI JSON file can be integrated with tools like:</p> <ul> <li>Postman: Import the schema to explore and test endpoints.</li> <li>Swagger Editor: Visualize and validate the schema.</li> </ul> <p>For instructions, see Swagger Editor or Postman.</p>"},{"location":"api/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you encounter issues accessing the API or endpoints:</li> <li>Ensure the server is running (<code>uvicorn main:app</code> or via Docker/Podman).</li> <li>Verify the base URL and port are correct.</li> <li>Check logs for errors in the server console.</li> </ul> <p>For further details, consult the Getting Started Guide or the Configuration Guide.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you set up and run the generative AI agent project.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+: Install Python</li> <li>Git: Install Git</li> <li>Docker (optional): Install Docker</li> <li>Access to flexo</li> </ul>"},{"location":"getting-started/#quick-setup","title":"Quick Setup","text":"<ol> <li> <p>Fork and clone the repository:    <pre><code># First, fork the repository on GitHub by clicking the 'Fork' button\n# Then clone your fork:\ngit clone https://github.com/YOUR_USERNAME/flexo.git\ncd flexo\n</code></pre></p> </li> <li> <p>Create Python environment:    <pre><code># Mac/Linux\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Windows\npython -m venv venv\n.\\venv\\Scripts\\activate\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Configure the project:</p> </li> <li> <p>Copy <code>.env.example</code> to <code>.env</code> and update with your settings</p> </li> <li> <p>Review <code>src/configs/agent.yaml</code> for agent configuration</p> </li> <li> <p>Start the API server:    <pre><code>uvicorn src.main:app --reload --host 127.0.0.1 --port 8000\n</code></pre></p> </li> <li> <p>Test the API using one of these methods:</p> </li> </ol> <p>A. Using cURL:    <pre><code>curl -X POST http://127.0.0.1:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n          \"messages\": [\n            {\n              \"role\": \"user\",\n              \"content\": [\n                { \"text\": \"Hello, AI!\" }\n              ]\n            }\n          ],\n          \"context\": {\n            \"values\": [\n              { \"key\": \"session_id\", \"value\": \"abc123\" }\n            ]\n          },\n          \"include_status_updates\": false,\n          \"stream\": true\n        }'\n</code></pre></p> <p>Note: Include an <code>-H \"X-API-Key: your-flexo-api-key\"</code> header when <code>ENABLE_API_KEY=true</code></p> <p>B. Using the Streamlit testing interface:    <pre><code># Navigate to the tests directory\ncd tests\n\n# Run the Streamlit app\nstreamlit run test_streamlit_app.py\n\n# This will open a browser window at http://localhost:8501 where you can interact with the agent\n</code></pre></p>"},{"location":"getting-started/#docker-deployment-optional","title":"Docker Deployment (Optional)","text":"<pre><code># Build the container\ndocker build -t flexo-agent .\n\n# Run the container\ndocker run -p 8000:8000 --env-file .env flexo-agent\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore configuration options in the Agent Configuration Guide</li> <li>Learn about deployment in the Deployment Guide</li> <li>Customize the agent's behavior and extend its functionality</li> </ul>"},{"location":"getting-started/#testing-tips","title":"Testing Tips","text":"<ul> <li>The Streamlit interface provides a more user-friendly way to test the agent's capabilities</li> <li>The Streamlit app is looking for your local agent instance running on port 8000</li> <li>For debugging, check both the API server logs and the Streamlit app logs</li> </ul>"},{"location":"model-configuration/","title":"Model Configuration Guide","text":"<p>This guide provides detailed information on configuring language models for use with the Flexo Framework. Model configurations are specified in the <code>models_config</code> section of <code>src/configs/agent.yaml</code>.</p>"},{"location":"model-configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Configuration Structure</li> <li>Supported Model Providers</li> <li>Cloud Providers</li> <li>Self-Hosted Options</li> <li>Configuration Parameters</li> <li>Common Parameters</li> <li>Provider-Specific Parameters</li> <li>Environment Variables</li> <li>Model Selection in Agent Configuration</li> <li>Advanced Configurations</li> <li>Examples</li> </ul>"},{"location":"model-configuration/#configuration-structure","title":"Configuration Structure","text":"<p>The model configuration in <code>models.yaml</code> follows this basic structure:</p> <pre><code>models:\n  model_name:\n    vendor: provider_name\n    model_id: model_identifier\n    # Additional parameters...\n</code></pre> <p>Each model entry consists of:</p> <ul> <li>A unique <code>model_name</code> which will be referenced in your agent configuration</li> <li>A <code>vendor</code> identifier that determines which adapter to use</li> <li>A <code>model_id</code> that specifies the actual model to use from the vendor</li> <li>Additional parameters specific to the provider or model</li> </ul>"},{"location":"model-configuration/#supported-model-providers","title":"Supported Model Providers","text":""},{"location":"model-configuration/#cloud-providers","title":"Cloud Providers","text":"Provider Vendor Key API Endpoints Environment Variables OpenAI <code>openai</code> <code>/chat/completions</code> <code>OPENAI_API_KEY</code> Anthropic <code>anthropic</code> <code>/messages</code> <code>ANTHROPIC_API_KEY</code> xAI <code>xai</code> <code>/chat/completions</code> <code>XAI_API_KEY</code> Mistral AI <code>mistral-ai</code> <code>/chat/completions</code> <code>MISTRAL_API_KEY</code> IBM WatsonX <code>watsonx-llama</code>, <code>watsonx-granite</code>, <code>watsonx-mistral</code> <code>/text/chat_stream</code>, <code>/text/generation_stream</code> <code>WATSONX_API_KEY</code>, <code>WATSONX_PROJECT_ID</code>"},{"location":"model-configuration/#self-hosted-options","title":"Self-Hosted Options","text":"<p>The <code>openai-compat</code> adapter types support any API that implements OpenAI-compatible endpoints:</p> Implementation Vendor Key Default Base URL vLLM <code>openai-compat</code> <code>http://localhost:8000/v1</code> Ollama <code>openai-compat</code> <code>http://localhost:11434/v1</code> LLaMA.cpp <code>openai-compat</code> <code>http://localhost:8080/v1</code> LM Studio <code>openai-compat</code> <code>http://localhost:1234/v1</code> LocalAI <code>openai-compat</code> <code>http://localhost:8080/v1</code> Text Generation WebUI <code>openai-compat</code> <code>http://localhost:5000/v1</code>"},{"location":"model-configuration/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"model-configuration/#common-parameters","title":"Common Parameters","text":"Parameter Type Description Default <code>model_id</code> string Identifier for the specific model to use Required <code>vendor</code> string Provider identifier (see tables above) Required <code>temperature</code> float Controls randomness in generation (0.0-1.0) 0.7 <code>max_tokens</code> integer Maximum tokens to generate 1024 <code>top_p</code> float Alternative to temperature for nucleus sampling (0.0-1.0) 1.0 <code>stop</code> array Sequences that will stop generation when produced <code>[]</code>"},{"location":"model-configuration/#provider-specific-parameters-examples","title":"Provider-Specific Parameters Examples","text":""},{"location":"model-configuration/#openai","title":"OpenAI","text":"<pre><code>gpt-4o:\n  vendor: openai\n  model_id: gpt-4-turbo\n  temperature: 0.7\n  max_tokens: 4096\n  top_p: 1.0\n  presence_penalty: 0.0  # OpenAI-specific\n  frequency_penalty: 0.0  # OpenAI-specific\n</code></pre> Parameter Type Description Default <code>presence_penalty</code> float Penalty for token presence (-2.0 to 2.0) 0.0 <code>frequency_penalty</code> float Penalty for token frequency (-2.0 to 2.0) 0.0"},{"location":"model-configuration/#anthropic","title":"Anthropic","text":"<pre><code>claude-35:\n  vendor: anthropic\n  model_id: claude-3-opus-20240229\n  temperature: 0.5\n  max_tokens: 4096\n  top_p: 0.9\n  top_k: 50  # Anthropic-specific\n</code></pre> Parameter Type Description Default <code>top_k</code> integer Limits token selection to top K options 50"},{"location":"model-configuration/#xai","title":"xAI","text":"<pre><code>grok-2:\n  vendor: xai\n  model_id: grok-2-latest\n  temperature: 0.7\n  max_tokens: 4096\n  top_p: 0.95\n  base_url: https://api.x.ai/v1  # Optional override\n</code></pre> Parameter Type Description Default <code>base_url</code> string API endpoint URL <code>https://api.x.ai/v1</code>"},{"location":"model-configuration/#ibm-watsonx","title":"IBM watsonX","text":"<pre><code>granite-8b:\n  vendor: watsonx-granite\n  model_id: ibm/granite-3-8b-instruct\n  temperature: 0.5\n  max_tokens: 1024\n  time_limit: 60  # watsonX-specific\n</code></pre> Parameter Type Description Default <code>time_limit</code> integer Maximum generation time in seconds 60"},{"location":"model-configuration/#mistral-ai","title":"Mistral AI","text":"<pre><code>mistral-large:\n  vendor: mistral-ai\n  model_id: mistral-large-latest\n  temperature: 0.7\n  max_tokens: 4096\n  safe_prompt: true  # Mistral-specific\n</code></pre> Parameter Type Description Default <code>safe_prompt</code> boolean Enable content filtering <code>true</code>"},{"location":"model-configuration/#openai-compatible","title":"OpenAI-Compatible","text":"<pre><code>local-model:\n  vendor: openai-compat  # or openai-compat-granite, openai-compat-llama, etc.\n  model_id: your-model-name\n  base_url: http://localhost:8000/v1\n  api_key: dummy-key\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre> Parameter Type Description Default <code>base_url</code> string The API endpoint URL Required <code>api_key</code> string API key (depending on implementation) Required"},{"location":"model-configuration/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables are used for API authentication:</p> Provider Environment Variable Required OpenAI <code>OPENAI_API_KEY</code> Yes Anthropic <code>ANTHROPIC_API_KEY</code> Yes xAI <code>XAI_API_KEY</code> Yes Mistral AI <code>MISTRAL_API_KEY</code> Yes IBM WatsonX <code>WATSONX_API_KEY</code>, <code>WATSONX_PROJECT_ID</code> Yes OpenAI-Compatible Varies (can be set in config) Depends"},{"location":"model-configuration/#model-selection-in-agent-configuration","title":"Model Selection in Agent Configuration","text":"<p>In your <code>agent.yaml</code> file, you specify which model to use:</p> <pre><code>models_config:\n  main_chat_model:\n    vendor: openai\n    model_id: gpt-4o\n    # Parameters...\n</code></pre> <p>You can also configure the agent to use different models for different purposes:</p> <pre><code>models_config:\n  main_chat_model:\n    vendor: anthropic\n    model_id: claude-3-5-sonnet-latest\n    # Primary model parameters...\n\n  watsonx_granite:\n    vendor: watsonx-granite\n    model_id: ibm/granite-3-8b-instruct\n    # Additional model instance parameters\n</code></pre>"},{"location":"model-configuration/#advanced-configurations","title":"Advanced Configurations","text":""},{"location":"model-configuration/#vendor-specific-prompt-builders","title":"Vendor-Specific Prompt Builders","text":"<p>Flexo automatically selects the appropriate prompt builder based on the vendor:</p> Vendor Prompt Builder Link <code>openai</code> <code>OpenAIPromptBuilder</code> <code>anthropic</code> <code>AnthropicPromptBuilder</code> <code>mistral-ai</code> <code>MistralAIPromptBuilder</code> <code>watsonx-granite</code> <code>WatsonXGranitePromptBuilder</code> <code>watsonx-llama</code> <code>WatsonXLlamaPromptBuilder</code> <code>watsonx-mistral</code> <code>WatsonXMistralPromptBuilder</code> <code>openai-compat-granite</code> <code>OpenAICompatGranitePromptBuilder</code> <code>openai-compat-llama</code> <code>OpenAICompatLlamaPromptBuilder</code> <code>xai</code> <code>XAIPromptBuilder</code>"},{"location":"model-configuration/#tool-detection-modes","title":"Tool Detection Modes","text":"<p>The <code>detection_mode</code> in your agent configuration affects how tool calls are detected:</p> <ul> <li><code>vendor</code>: Uses the provider's native tool-calling capabilities (recommended for cloud providers)</li> <li><code>manual</code>: Uses Flexo's pattern-matching for tool calls (useful for local models)</li> </ul> <pre><code>detection_mode: vendor  # or 'manual'\nuse_vendor_chat_completions: true  # or false\n</code></pre>"},{"location":"model-configuration/#examples","title":"Examples","text":""},{"location":"model-configuration/#openai-gpt-4","title":"OpenAI GPT-4","text":"<pre><code>gpt-4:\n  vendor: openai\n  model_id: gpt-4o\n  temperature: 0.7\n  max_tokens: 4096\n  presence_penalty: 0.0\n  frequency_penalty: 0.0\n</code></pre>"},{"location":"model-configuration/#anthropic-claude","title":"Anthropic Claude","text":"<pre><code>claude:\n  vendor: anthropic\n  model_id: claude-3-5-sonnet-latest\n  temperature: 0.7\n  max_tokens: 4096\n  top_p: 0.9\n</code></pre>"},{"location":"model-configuration/#xai_1","title":"xAI","text":"<pre><code>grok:\n  vendor: xai\n  model_id: grok-2-latest\n  temperature: 0.7\n  max_tokens: 4096\n</code></pre>"},{"location":"model-configuration/#mistral-ai_1","title":"Mistral AI","text":"<pre><code>mistral:\n  vendor: mistral-ai\n  model_id: mistral-large-latest\n  temperature: 0.7\n  max_tokens: 4096\n</code></pre>"},{"location":"model-configuration/#ibm-watsonx-llama","title":"IBM WatsonX (Llama)","text":"<pre><code>watsonx-llama:\n  vendor: watsonx-llama\n  model_id: meta-llama/llama-3-405b-instruct\n  decoding_method: greedy\n  max_tokens: 4000\n  temperature: 0.7\n</code></pre>"},{"location":"model-configuration/#ibm-watsonx-granite","title":"IBM WatsonX (Granite)","text":"<pre><code>watsonx-granite:\n  vendor: watsonx-granite\n  model_id: ibm/granite-3-8b-instruct\n  decoding_method: greedy\n  max_tokens: 4000\n  temperature: 0.7\n</code></pre>"},{"location":"model-configuration/#vllm-local-deployment","title":"vLLM (Local Deployment)","text":"<pre><code>vllm-local:\n  vendor: openai-compat-llama\n  model_id: meta-llama/Llama-3.2-8B-Instruct\n  base_url: http://localhost:8000/v1\n  api_key: dummy-key\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"model-configuration/#ollama-local-deployment","title":"Ollama (Local Deployment)","text":"<pre><code>ollama-local:\n  vendor: openai-compat-granite\n  model_id: granite31  # or any model name in Ollama\n  base_url: http://localhost:11434/v1\n  api_key: ollama\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"deployment/building-image/","title":"Building the Flexo Image","text":"<p>This guide walks through the process of building a Docker image for Flexo.</p>"},{"location":"deployment/building-image/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed (20.10.0 or higher)</li> <li>Git clone of Flexo repository</li> <li>(Optional) Podman 3.0+ as Docker alternative</li> </ul>"},{"location":"deployment/building-image/#building-the-image","title":"Building the Image","text":""},{"location":"deployment/building-image/#basic-build","title":"Basic Build","text":"<pre><code># From the repository root\ndocker build -t flexo:latest .\n</code></pre>"},{"location":"deployment/building-image/#platform-specific-build","title":"Platform-Specific Build","text":"<p>For cloud deployments, build for AMD64 architecture: <pre><code>docker buildx build --platform linux/amd64 \\\n    -t flexo:latest \\\n    --load .\n</code></pre></p>"},{"location":"deployment/building-image/#build-arguments","title":"Build Arguments","text":"<p>Customize the build with arguments: <pre><code>docker build \\\n    --build-arg PYTHON_VERSION=3.9 \\\n    -t flexo:latest .\n</code></pre></p>"},{"location":"deployment/building-image/#build-options","title":"Build Options","text":""},{"location":"deployment/building-image/#1-build-with-configuration","title":"1. Build with Configuration","text":"<p>Include agent configuration during build: <pre><code># Copy configuration files\ncp agent.yaml build/\ncp -r tools/ build/\n\n# Build image\ndocker build -t flexo:configured .\n</code></pre></p>"},{"location":"deployment/building-image/#2-build-base-image","title":"2. Build Base Image","text":"<p>Build without configuration for runtime mounting: <pre><code>docker build -t flexo:base -f Dockerfile.base .\n</code></pre></p>"},{"location":"deployment/building-image/#testing-the-build","title":"Testing the Build","text":"<p>Verify your build: <pre><code># Run container\ndocker run -it --rm \\\n    -p 8000:8000 \\\n    --env-file .env \\\n    flexo:latest\n\n# Test endpoint\ncurl http://localhost:8000/health\n</code></pre></p>"},{"location":"deployment/building-image/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ul> <li>Build fails: Check Docker daemon status and Dockerfile syntax</li> <li>Platform errors: Verify buildx setup for your target platform</li> <li>Size issues: Use .dockerignore to exclude unnecessary files</li> </ul>"},{"location":"deployment/building-image/#next-steps","title":"Next Steps","text":"<ul> <li>Push to Registry</li> <li>Configure Environment</li> </ul>"},{"location":"deployment/overview/","title":"Deployment Overview","text":"<p>Flexo can be deployed using two main approaches, each suited for different needs and scenarios.</p>"},{"location":"deployment/overview/#deployment-approaches","title":"Deployment Approaches","text":""},{"location":"deployment/overview/#approach-1-build-time-configuration","title":"Approach 1: Build-time Configuration","text":"<p>With this approach, you bake your agent configuration into the container image during build time. Environment variables are still injected at runtime for security.</p> <pre><code>graph LR\n    config1[agent.yaml + tools] --&gt; build[Build Image]\n    build --&gt; reg1[Push to Registry]\n    reg1 --&gt; deploy1[Deploy Container]\n    env1[.env variables] -.-&gt; deploy1\n\n    classDef config stroke:#0288d1\n    classDef env stroke:#388e3c\n    classDef action stroke:#ffa000\n\n    class config1 config\n    class env1 env\n    class build,reg1,deploy1 action</code></pre> <p>Best for:</p> <ul> <li>Stable configurations</li> <li>Version-controlled agent behavior</li> <li>Immutable deployments</li> <li>Faster container startup</li> </ul>"},{"location":"deployment/overview/#approach-2-runtime-configuration","title":"Approach 2: Runtime Configuration","text":"<p>With this approach, you use a base image and inject both configuration and environment variables at runtime. This allows for more flexible configuration management.</p> <pre><code>graph LR\n    base[Base Image] --&gt; reg2[Push to Registry]\n    reg2 --&gt; deploy2[Deploy Container]\n    config2[agent.yaml + tools] -.-&gt; deploy2\n    env2[.env variables] -.-&gt; deploy2\n\n    classDef config stroke:#6C8EBF\n    classDef env stroke:#63825A\n    classDef action stroke:#DD7E6B\n\n    class config2 config\n    class env2 env\n    class base,reg2,deploy2 action</code></pre> <p>Best for:</p> <ul> <li>Dynamic configuration updates</li> <li>Environment-specific settings</li> <li>Testing different configurations</li> <li>Separate configuration management</li> </ul>"},{"location":"deployment/overview/#deployment-process","title":"Deployment Process","text":"<p>The general deployment process follows these steps:</p> <ol> <li>Building the Image</li> <li>Pushing to a Registry</li> <li>Deploying to a Platform</li> </ol>"},{"location":"deployment/platforms/code-engine/","title":"Deploying Flexo to IBM Code Engine","text":"<p>This guide walks through deploying Flexo to IBM Code Engine, a fully managed serverless platform.</p>"},{"location":"deployment/platforms/code-engine/#prerequisites","title":"Prerequisites","text":"<ul> <li>IBM Cloud account with Code Engine permissions</li> <li>IBM Cloud CLI with Code Engine plugin</li> <li>Container image in a registry (ICR or Docker Hub)</li> </ul>"},{"location":"deployment/platforms/code-engine/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/platforms/code-engine/#1-create-project","title":"1. Create Project","text":"<pre><code># Login to IBM Cloud\nibmcloud login --sso\n\n# Create new project\nibmcloud ce project create --name flexo-project\n# Or select existing\nibmcloud ce project select --name flexo-project\n</code></pre>"},{"location":"deployment/platforms/code-engine/#2-create-environment-variables","title":"2. Create Environment Variables","text":"<p>Create two environment files to manage your configuration:</p> <p>secrets.env (for sensitive values): <pre><code># For watsonx.ai client\nWXAI_APIKEY=your-api-key\n\n# For example weather tool\nOWM_API_KEY=owm-api-key\n\n# For example RAG tool (using elastic)\nES_ES_API_KEY=elastic-api-key\n\n# API authentication\nFLEXO_API_KEY=flexo-api-key\n</code></pre></p> <p>config.env (for non-sensitive configuration): <pre><code># For watsonx.ai client\nWXAI_URL=https://us-south.ml.cloud.ibm.com\nWXAI_PROJECT_ID=your-project-id\n\n# For example RAG tool\nES_INDEX_NAME=my_index\nES_ES_ENDPOINT=elastic-endpoint\n\n# API Configurations\nENABLE_API_KEY=true\n</code></pre></p> <p>Create the configurations in Code Engine: <pre><code># Create secret from file\nibmcloud ce secret create --name flexo-secrets \\\n    --from-env-file secrets.env\n\n# Create configmap from file\nibmcloud ce configmap create --name flexo-config \\\n    --from-env-file config.env\n</code></pre></p>"},{"location":"deployment/platforms/code-engine/#3-deploy-application","title":"3. Deploy Application","text":"<pre><code># Basic deployment\nibmcloud ce application create \\\n    --name flexo \\\n    --image us.icr.io/namespace/flexo:latest \\\n    --port 8000 \\\n    --cpu 2 \\\n    --memory 8G \\\n    --env-from-secret flexo-secrets \\\n    --env-from-configmap flexo-config\n</code></pre>"},{"location":"deployment/platforms/code-engine/#optional-mount-configuration","title":"Optional: Mount Configuration","text":"<p>If you want to mount your agent configuration instead of building it into the image:</p> <pre><code># Create configmap from agent.yaml\nibmcloud ce configmap create --name agent-config \\\n    --from-file agent.yaml\n\n# Deploy with mount\nibmcloud ce application create \\\n    --name flexo \\\n    --image us.icr.io/namespace/flexo:latest \\\n    --env-from-secret flexo-secrets \\\n    --env-from-configmap flexo-config \\\n    --mount-configmap /app/config=agent-config\n</code></pre>"},{"location":"deployment/platforms/code-engine/#configuration-options","title":"Configuration Options","text":""},{"location":"deployment/platforms/code-engine/#scaling","title":"Scaling","text":"<pre><code>ibmcloud ce application update \\\n    --name flexo \\\n    --min-scale 1 \\\n    --max-scale 5\n</code></pre>"},{"location":"deployment/platforms/code-engine/#resource-allocation","title":"Resource Allocation","text":"<pre><code>ibmcloud ce application update \\\n    --name flexo \\\n    --cpu 2 \\\n    --memory 8G\n</code></pre>"},{"location":"deployment/platforms/code-engine/#monitoring","title":"Monitoring","text":"<pre><code># View logs\nibmcloud ce application logs --name flexo\n\n# Check status\nibmcloud ce application get --name flexo\n</code></pre>"},{"location":"deployment/platforms/code-engine/#environment-variable-access","title":"Environment Variable Access","text":"<ul> <li>Values from both secrets and configmaps are loaded as environment variables</li> <li>Access in your code using <code>os.getenv()</code>:   <pre><code>import os\n\napi_key = os.getenv('WXAI_APIKEY')\nlog_level = os.getenv('LOG_LEVEL')\n</code></pre></li> <li>Mounted files (like agent.yaml) are available as files at the specified path</li> </ul>"},{"location":"deployment/platforms/fargate/","title":"Deploying Flexo to AWS Fargate","text":"<p>This guide walks through deploying Flexo to AWS Fargate, a serverless container platform that integrates with Amazon ECS (Elastic Container Service).</p>"},{"location":"deployment/platforms/fargate/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with appropriate permissions</li> <li>AWS CLI installed and configured</li> <li>Container image in Amazon ECR or other accessible registry</li> <li>Task execution role with required permissions</li> </ul>"},{"location":"deployment/platforms/fargate/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/platforms/fargate/#1-create-ecr-repository-and-push-image","title":"1. Create ECR Repository and Push Image","text":"<pre><code># Create repository\naws ecr create-repository --repository-name flexo\n\n# Login to ECR\naws ecr get-login-password --region us-east-1 | \\\n    docker login --username AWS --password-stdin \\\n    ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com\n\n# Build and push\ndocker buildx build --platform linux/amd64 \\\n    -t ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/flexo:latest \\\n    --push .\n</code></pre>"},{"location":"deployment/platforms/fargate/#2-create-environment-variables","title":"2. Create Environment Variables","text":"<p>Store your configuration in AWS Systems Manager Parameter Store:</p> <p>For secrets (using SecureString): <pre><code># Store sensitive values\naws ssm put-parameter \\\n    --name \"/flexo/prod/WXAI_APIKEY\" \\\n    --value \"your-api-key\" \\\n    --type \"SecureString\"\n\naws ssm put-parameter \\\n    --name \"/flexo/prod/OWM_API_KEY\" \\\n    --value \"owm-api-key\" \\\n    --type \"SecureString\"\n\naws ssm put-parameter \\\n    --name \"/flexo/prod/ES_ES_API_KEY\" \\\n    --value \"elastic-api-key\" \\\n    --type \"SecureString\"\n\naws ssm put-parameter \\\n    --name \"/flexo/prod/FLEXO_API_KEY\" \\\n    --value \"flexo-api-key\" \\\n    --type \"SecureString\"\n</code></pre></p> <p>For configuration (using String): <pre><code># Store non-sensitive values\naws ssm put-parameter \\\n    --name \"/flexo/prod/WXAI_URL\" \\\n    --value \"https://us-south.ml.cloud.ibm.com\" \\\n    --type \"String\"\n\naws ssm put-parameter \\\n    --name \"/flexo/prod/WXAI_PROJECT_ID\" \\\n    --value \"your-project-id\" \\\n    --type \"String\"\n\naws ssm put-parameter \\\n    --name \"/flexo/prod/ES_INDEX_NAME\" \\\n    --value \"my_index\" \\\n    --type \"String\"\n\naws ssm put-parameter \\\n    --name \"/flexo/prod/ES_ES_ENDPOINT\" \\\n    --value \"elastic-endpoint\" \\\n    --type \"String\"\n</code></pre></p>"},{"location":"deployment/platforms/fargate/#3-create-ecs-task-definition","title":"3. Create ECS Task Definition","text":"<pre><code>{\n  \"family\": \"flexo\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"1024\",\n  \"memory\": \"2048\",\n  \"containerDefinitions\": [{\n    \"name\": \"flexo\",\n    \"image\": \"${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/flexo:latest\",\n    \"portMappings\": [{\n      \"containerPort\": 8000,\n      \"protocol\": \"tcp\"\n    }],\n    \"secrets\": [\n      {\n        \"name\": \"WXAI_APIKEY\",\n        \"valueFrom\": \"arn:aws:ssm:us-east-1:${AWS_ACCOUNT_ID}:parameter/flexo/prod/WXAI_APIKEY\"\n      },\n      {\n        \"name\": \"OWM_API_KEY\",\n        \"valueFrom\": \"arn:aws:ssm:us-east-1:${AWS_ACCOUNT_ID}:parameter/flexo/prod/OWM_API_KEY\"\n      }\n    ],\n    \"environment\": [\n      {\n        \"name\": \"WXAI_URL\",\n        \"valueFrom\": \"arn:aws:ssm:us-east-1:${AWS_ACCOUNT_ID}:parameter/flexo/prod/WXAI_URL\"\n      }\n    ],\n    \"logConfiguration\": {\n      \"logDriver\": \"awslogs\",\n      \"options\": {\n        \"awslogs-group\": \"/ecs/flexo\",\n        \"awslogs-region\": \"us-east-1\",\n        \"awslogs-stream-prefix\": \"ecs\"\n      }\n    }\n  }]\n}\n</code></pre>"},{"location":"deployment/platforms/fargate/#4-create-ecs-service","title":"4. Create ECS Service","text":"<pre><code># Create cluster if needed\naws ecs create-cluster --cluster-name flexo-cluster\n\n# Create service\naws ecs create-service \\\n    --cluster flexo-cluster \\\n    --service-name flexo \\\n    --task-definition flexo:1 \\\n    --desired-count 1 \\\n    --launch-type FARGATE \\\n    --network-configuration \"awsvpcConfiguration={subnets=[subnet-xxxxx],securityGroups=[sg-xxxxx],assignPublicIp=ENABLED}\"\n</code></pre>"},{"location":"deployment/platforms/fargate/#configuration-options","title":"Configuration Options","text":""},{"location":"deployment/platforms/fargate/#scaling","title":"Scaling","text":"<p>Use Application Auto Scaling to set up automatic scaling: <pre><code># Register scalable target\naws application-autoscaling register-scalable-target \\\n    --service-namespace ecs \\\n    --scalable-dimension ecs:service:DesiredCount \\\n    --resource-id service/flexo-cluster/flexo \\\n    --min-capacity 1 \\\n    --max-capacity 5\n\n# Configure scaling policy\naws application-autoscaling put-scaling-policy \\\n    --service-namespace ecs \\\n    --scalable-dimension ecs:service:DesiredCount \\\n    --resource-id service/flexo-cluster/flexo \\\n    --policy-name cpu-scaling \\\n    --policy-type TargetTrackingScaling \\\n    --target-tracking-scaling-policy-configuration file://scaling-policy.json\n</code></pre></p>"},{"location":"deployment/platforms/fargate/#monitoring","title":"Monitoring","text":"<pre><code># View logs\naws logs get-log-events \\\n    --log-group-name /ecs/flexo \\\n    --log-stream-name your-log-stream\n\n# Check service status\naws ecs describe-services \\\n    --cluster flexo-cluster \\\n    --services flexo\n</code></pre>"},{"location":"deployment/platforms/fargate/#possible-next-steps","title":"Possible Next Steps","text":"<ul> <li>Set up Application Load Balancer</li> <li>Configure CloudWatch alarms</li> <li>Set up CI/CD pipeline with AWS CodePipeline</li> </ul>"},{"location":"deployment/platforms/kubernetes/","title":"Deploying Flexo to Kubernetes","text":"<p>This guide covers deploying Flexo to a generic Kubernetes cluster.</p>"},{"location":"deployment/platforms/kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Kubernetes cluster</li> <li>kubectl CLI installed and configured</li> <li>Container image in accessible registry</li> <li>Kubernetes 1.19+ recommended</li> </ul>"},{"location":"deployment/platforms/kubernetes/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/platforms/kubernetes/#1-create-namespace","title":"1. Create Namespace","text":"<pre><code>kubectl create namespace flexo\nkubectl config set-context --current --namespace=flexo\n</code></pre>"},{"location":"deployment/platforms/kubernetes/#2-create-configuration","title":"2. Create Configuration","text":"<pre><code># configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: flexo-config\ndata:\n  agent.yaml: |\n    # Your agent configuration here\n  LOGGING_LEVEL: \"INFO\"\n\n---\n# secrets.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: flexo-secrets\ntype: Opaque\nstringData:\n  WATSON_API_KEY: &lt;your-key&gt;\n</code></pre>"},{"location":"deployment/platforms/kubernetes/#3-deploy-application","title":"3. Deploy Application","text":"<pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flexo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flexo\n  template:\n    metadata:\n      labels:\n        app: flexo\n    spec:\n      containers:\n      - name: flexo\n        image: your-registry/flexo:latest\n        ports:\n        - containerPort: 8000\n        envFrom:\n        - secretRef:\n            name: flexo-secrets\n        - configMapRef:\n            name: flexo-config\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2\"\n</code></pre>"},{"location":"deployment/platforms/kubernetes/#4-create-service","title":"4. Create Service","text":"<pre><code># service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: flexo\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8000\n  selector:\n    app: flexo\n\n---\n# ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: flexo\nspec:\n  rules:\n  - host: flexo.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: flexo\n            port:\n              number: 8000\n</code></pre>"},{"location":"deployment/platforms/kubernetes/#resource-management","title":"Resource Management","text":""},{"location":"deployment/platforms/kubernetes/#horizontal-pod-autoscaling","title":"Horizontal Pod Autoscaling","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: flexo\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: flexo\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 80\n</code></pre>"},{"location":"deployment/platforms/kubernetes/#monitoring","title":"Monitoring","text":""},{"location":"deployment/platforms/kubernetes/#health-checks","title":"Health Checks","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8000\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /health\n    port: 8000\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"deployment/platforms/openshift/","title":"Deploying Flexo to OpenShift","text":"<p>This guide covers deploying Flexo on OpenShift, providing enterprise-grade orchestration and scaling.</p>"},{"location":"deployment/platforms/openshift/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to OpenShift cluster</li> <li>OpenShift CLI (oc) installed</li> <li>Container image in accessible registry</li> <li>Cluster admin permissions (for some operations)</li> </ul>"},{"location":"deployment/platforms/openshift/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/platforms/openshift/#1-create-project","title":"1. Create Project","text":"<pre><code># Create new project\noc new-project flexo\n\n# Or use existing\noc project flexo\n</code></pre>"},{"location":"deployment/platforms/openshift/#2-create-secrets-and-configmaps","title":"2. Create Secrets and ConfigMaps","text":"<pre><code># secrets.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: flexo-secrets\ntype: Opaque\nstringData:\n  WATSON_API_KEY: &lt;your-key&gt;\n  OTHER_SECRET: &lt;value&gt;\n</code></pre> <pre><code># Apply configuration\noc apply -f secrets.yaml\noc create configmap flexo-config \\\n    --from-literal=LOGGING_LEVEL=INFO\n</code></pre>"},{"location":"deployment/platforms/openshift/#3-deploy-application","title":"3. Deploy Application","text":"<pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flexo\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: flexo\n        image: us.icr.io/namespace/flexo:latest\n        ports:\n        - containerPort: 8000\n        envFrom:\n        - secretRef:\n            name: flexo-secrets\n        - configMapRef:\n            name: flexo-config\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2\"\n</code></pre>"},{"location":"deployment/platforms/openshift/#4-create-service-and-route","title":"4. Create Service and Route","text":"<pre><code># service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: flexo\nspec:\n  ports:\n  - port: 8000\n  selector:\n    app: flexo\n\n---\n# route.yaml\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: flexo\nspec:\n  to:\n    kind: Service\n    name: flexo\n</code></pre>"},{"location":"deployment/platforms/openshift/#security-considerations","title":"Security Considerations","text":"<ul> <li>Set up network policies</li> <li>Configure service accounts</li> <li>Use OpenShift secrets management</li> <li>Enable TLS for routes</li> </ul>"},{"location":"deployment/platforms/openshift/#monitoring","title":"Monitoring","text":"<pre><code># Check deployment status\noc get deployment flexo\n\n# View logs\noc logs deployment/flexo\n\n# Monitor resources\noc adm top pods\n</code></pre>"},{"location":"deployment/platforms/openshift/#scaling","title":"Scaling","text":"<pre><code># Manual scaling\noc scale deployment/flexo --replicas=3\n\n# Automatic scaling\noc autoscale deployment/flexo --min=2 --max=5 --cpu-percent=80\n</code></pre>"},{"location":"deployment/platforms/overview/","title":"Deployment Platforms Overview","text":"<p>Flexo can be deployed to various cloud platforms and container orchestration services. This guide helps with a few options but it's up to you on determining the right platform for your needs.</p>"},{"location":"deployment/platforms/overview/#platforms","title":"Platforms","text":""},{"location":"deployment/platforms/overview/#links-to-docs","title":"Links to docs","text":"<ul> <li>Deploy to Code Engine</li> <li>Deploy to AWS Fargate</li> <li>Deploy to OpenShift</li> <li>Deploy to Kubernetes</li> </ul>"},{"location":"deployment/platforms/overview/#platform-comparison","title":"Platform Comparison","text":"Feature Code Engine AWS Fargate OpenShift Kubernetes Setup Complexity Low Medium High Medium Scaling Automatic Automatic Manual/Auto Manual/Auto Cost Model Pay-per-use Pay-per-use Cluster-based Cluster-based Management Managed Managed Self/Managed Self-managed Config Management Built-in Parameter Store ConfigMaps ConfigMaps"},{"location":"deployment/platforms/overview/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"deployment/platforms/overview/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Recommended Minimum Memory: 6GB</li> <li>Recommended Minimum CPU: 2 vCPU</li> <li>Storage: Based on usage</li> </ul> <p>Note: Number of uvicorn workers can be set using <code>UVICORN_WORKERS</code>. The default is calculated dynamically and set as <code>2 \u00d7 num_vCPUs + 1</code>.</p>"},{"location":"deployment/platforms/overview/#configuration-management","title":"Configuration Management","text":"<p>Each platform provides different methods for:</p> <ul> <li>Environment variables</li> <li>Secret management</li> <li>Volume mounts</li> <li>Network policies</li> </ul>"},{"location":"deployment/platforms/overview/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Consider platform-specific solutions for:</p> <ul> <li>Health monitoring</li> <li>Log aggregation</li> <li>Performance metrics</li> <li>Alert management</li> </ul>"},{"location":"deployment/registries/dockerhub/","title":"Docker Hub Guide","text":"<p>This guide covers using Docker Hub for storing and distributing your Flexo container images.</p>"},{"location":"deployment/registries/dockerhub/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Hub account</li> <li>Docker installed locally</li> <li>(Optional) Docker Hub Team/Pro subscription for private repositories</li> </ul>"},{"location":"deployment/registries/dockerhub/#setup-process","title":"Setup Process","text":""},{"location":"deployment/registries/dockerhub/#1-create-repository","title":"1. Create Repository","text":"<ol> <li>Log in to Docker Hub</li> <li>Click \"Create Repository\"</li> <li>Name it \"flexo\" and set visibility (public/private)</li> </ol>"},{"location":"deployment/registries/dockerhub/#2-authentication","title":"2. Authentication","text":"<pre><code># Login to Docker Hub\ndocker login\n\n# For automation, create access token:\n# 1. Docker Hub &gt; Account Settings &gt; Security\n# 2. New Access Token\n# 3. Save token securely\n</code></pre>"},{"location":"deployment/registries/dockerhub/#working-with-images","title":"Working with Images","text":""},{"location":"deployment/registries/dockerhub/#tag-your-image","title":"Tag Your Image","text":"<pre><code>docker tag flexo:latest &lt;username&gt;/flexo:latest\n</code></pre>"},{"location":"deployment/registries/dockerhub/#push-to-registry","title":"Push to Registry","text":"<pre><code>docker push &lt;username&gt;/flexo:latest\n</code></pre>"},{"location":"deployment/registries/dockerhub/#pull-image","title":"Pull Image","text":"<pre><code>docker pull &lt;username&gt;/flexo:latest\n</code></pre>"},{"location":"deployment/registries/dockerhub/#best-practices","title":"Best Practices","text":"<ul> <li>Use specific tags for versioning</li> <li>Set up automated builds</li> <li>Implement security scanning</li> <li>Use access tokens instead of password</li> <li>Consider rate limiting implications</li> </ul>"},{"location":"deployment/registries/dockerhub/#cost-considerations","title":"Cost Considerations","text":"<ul> <li>Free tier limitations</li> <li>Private repository pricing</li> <li>Pull rate limits</li> </ul>"},{"location":"deployment/registries/dockerhub/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Rate Limits: Check pull limits and consider authentication</li> <li>Push Errors: Verify repository permissions</li> <li>Authentication: Check token expiration and scope</li> </ul>"},{"location":"deployment/registries/dockerhub/#next-steps","title":"Next Steps","text":"<ul> <li>Deploy to Kubernetes</li> <li>Mount Configurations</li> </ul>"},{"location":"deployment/registries/ibm-registry/","title":"IBM Container Registry Guide","text":"<p>This guide details how to use IBM Container Registry (ICR) for storing and managing your Flexo container images.</p>"},{"location":"deployment/registries/ibm-registry/#prerequisites","title":"Prerequisites","text":"<ul> <li>IBM Cloud account</li> <li>IBM Cloud CLI installed</li> <li>Container Registry plugin installed (<code>ibmcloud plugin install container-registry</code>)</li> <li>Docker installed locally</li> </ul>"},{"location":"deployment/registries/ibm-registry/#setup-process","title":"Setup Process","text":""},{"location":"deployment/registries/ibm-registry/#1-install-and-configure-ibm-cloud-cli","title":"1. Install and Configure IBM Cloud CLI","text":"<pre><code># Login to IBM Cloud\nibmcloud login --sso\n\n# Target a resource group\nibmcloud target -g &lt;resource-group&gt;\n\n# Login to Container Registry\nibmcloud cr login\n</code></pre>"},{"location":"deployment/registries/ibm-registry/#2-create-a-namespace","title":"2. Create a Namespace","text":"<pre><code># Create a namespace for your images\nibmcloud cr namespace-add &lt;your-namespace&gt;\n</code></pre>"},{"location":"deployment/registries/ibm-registry/#3-configure-access","title":"3. Configure Access","text":"<pre><code># Generate API key for automation\nibmcloud iam api-key-create flexo-key -d \"Key for Flexo deployments\"\n\n# Optional: Create service ID\nibmcloud iam service-id-create flexo-service --description \"Service ID for Flexo\"\n</code></pre>"},{"location":"deployment/registries/ibm-registry/#pushing-images","title":"Pushing Images","text":""},{"location":"deployment/registries/ibm-registry/#tag-your-image","title":"Tag Your Image","text":"<pre><code>docker tag flexo:latest us.icr.io/&lt;your-namespace&gt;/flexo:latest\n</code></pre>"},{"location":"deployment/registries/ibm-registry/#push-to-registry","title":"Push to Registry","text":"<pre><code>docker push us.icr.io/&lt;your-namespace&gt;/flexo:latest\n</code></pre>"},{"location":"deployment/registries/ibm-registry/#best-practices","title":"Best Practices","text":"<ul> <li>Use meaningful image tags (e.g., version numbers, git hashes)</li> <li>Enable vulnerability scanning</li> <li>Regularly clean up unused images</li> <li>Set up retention policies</li> </ul>"},{"location":"deployment/registries/ibm-registry/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Login Issues: Check IBM Cloud CLI version and authentication</li> <li>Push Failures: Verify namespace permissions</li> <li>Pull Issues: Check network connectivity and credentials</li> </ul>"},{"location":"deployment/registries/ibm-registry/#next-steps","title":"Next Steps","text":"<ul> <li>Deploy to Code Engine</li> <li>Configure Environment Variables</li> </ul>"},{"location":"deployment/registries/overview/","title":"Container Registry Guide","text":"<p>After building your Flexo image, you'll need to push it to a container registry. This guide covers the main registry options and their setup.</p>"},{"location":"deployment/registries/overview/#example-registries","title":"Example Registries:","text":""},{"location":"deployment/registries/overview/#ibm-container-registry","title":"IBM Container Registry","text":"<p>Best for IBM Cloud deployments:</p> <ul> <li>Native integration with IBM Cloud</li> <li>Built-in vulnerability scanning</li> <li>Regional endpoints for faster pulls</li> </ul> <p>Detailed IBM Registry Guide</p>"},{"location":"deployment/registries/overview/#docker-hub","title":"Docker Hub","text":"<p>Universal option for all platforms:</p> <ul> <li>Widely supported</li> <li>Free public repositories</li> <li>Simple authentication</li> </ul> <p>Detailed Docker Hub Guide</p>"},{"location":"deployment/registries/overview/#general-process","title":"General Process","text":"<ol> <li> <p>Tag your image:    <pre><code># Format: registry/namespace/repository:tag\ndocker tag flexo:latest &lt;registry-url&gt;/&lt;namespace&gt;/flexo:latest\n</code></pre></p> </li> <li> <p>Authenticate:    <pre><code># For IBM Container Registry\nibmcloud cr login\n\n# For Docker Hub\ndocker login\n</code></pre></p> </li> <li> <p>Push image:    <pre><code>docker push &lt;registry-url&gt;/&lt;namespace&gt;/flexo:latest\n</code></pre></p> </li> </ol>"},{"location":"deployment/registries/overview/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Use unique credentials for automation</li> <li>Regularly rotate registry credentials</li> <li>Scan images for vulnerabilities</li> <li>Use specific tags instead of 'latest'</li> </ul>"},{"location":"deployment/registries/overview/#next-steps","title":"Next Steps","text":"<ul> <li>IBM Registry Setup</li> <li>Docker Hub Setup</li> <li>Platform Deployment</li> </ul>"},{"location":"other-topics/document-processing/elastic/","title":"Installing and Configuring Elasticsearch with Custom Models","text":""},{"location":"other-topics/document-processing/elastic/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+ to 3.11</li> <li>pip package manager</li> <li>Access to an Elasticsearch instance</li> <li>Administrative privileges (for installation)</li> </ul>"},{"location":"other-topics/document-processing/elastic/#installation-steps","title":"Installation Steps","text":""},{"location":"other-topics/document-processing/elastic/#1-installing-elasticsearch-client","title":"1. Installing Elasticsearch Client","text":"<p>First, install the official Elasticsearch Python client:</p> <pre><code>pip install elasticsearch==8.15.0\n</code></pre>"},{"location":"other-topics/document-processing/elastic/#2-installing-eland-and-dependencies","title":"2. Installing Eland and Dependencies","text":"<p>Eland is Elasticsearch's machine learning client that helps manage ML models:</p> <pre><code>pip install 'eland[pytorch]==8.15.0'\n</code></pre> <p>Note: If you encounter any dependency conflicts, you may need to change the version of your local <code>elasticsearch</code> and <code>eland</code> to match the version installed on your hosted Elastic instance. For example, in my case I needed to install version <code>8.15.0</code> locally in order to match what was installed at my Elastic endpoint where the huggingface model is being deployed.</p>"},{"location":"other-topics/document-processing/elastic/#uploading-a-custom-model","title":"Uploading a Custom Model","text":""},{"location":"other-topics/document-processing/elastic/#1-prepare-your-environment","title":"1. Prepare Your Environment","text":"<p>Ensure you have the following information ready: - Elasticsearch endpoint URL - API key with appropriate permissions - The Hugging Face model ID you want to import</p>"},{"location":"other-topics/document-processing/elastic/#2-model-import-command","title":"2. Model Import Command","text":"<p>Use the following command structure to import your model:</p> <pre><code>eland_import_hub_model --url \"ES_ENDPOINT\" --es-api-key \"ES_API_KEY\" --hub-model-id \"HUGGINGFACE_MODEL\" --task-type text_embedding\n</code></pre> <p>Note: It's not ideal, but you can add the <code>--insecure</code> tag if needed to get past SSL cert issues. </p> <p>Replace the placeholder values: - <code>ES_ENDPOINT</code>: Your Elasticsearch instance URL (e.g., \"https://localhost:9200\") - <code>ES_API_KEY</code>: Your Elasticsearch API key - <code>HUGGINGFACE_MODEL</code>: The Hugging Face model ID (e.g., \"sentence-transformers/all-MiniLM-L6-v2\")</p>"},{"location":"other-topics/document-processing/elastic/#3-command-options-explained","title":"3. Command Options Explained","text":"<ul> <li><code>--url</code>: Specifies the Elasticsearch endpoint</li> <li><code>--es-api-key</code>: Authentication key for Elasticsearch</li> <li><code>--hub-model-id</code>: The specific model to import from Hugging Face</li> <li><code>--task-type</code>: Specifies the model's purpose (text_embedding in this case)</li> <li><code>--insecure</code>: Allows connections to Elasticsearch without SSL verification</li> </ul>"},{"location":"other-topics/document-processing/elastic/#4-verifying-model-installation-and-deployment","title":"4. Verifying Model Installation and Deployment","text":"<p>After running the import command, you can verify the model installation and deployment status using several commands:</p> <ol> <li> <p>List all installed models: <pre><code>curl -X GET \"ES_ENDPOINT/_ml/trained_models\" -H \"Authorization: ApiKey ES_API_KEY\"\n</code></pre></p> </li> <li> <p>Check specific model details: <pre><code>curl -X GET \"ES_ENDPOINT/_ml/trained_models/MODEL_ID\" -H \"Authorization: ApiKey ES_API_KEY\"\n</code></pre></p> </li> <li> <p>Verify model deployment status: <pre><code>curl -X GET \"ES_ENDPOINT/_ml/trained_models/MODEL_ID/deployment/_stats\" -H \"Authorization: ApiKey ES_API_KEY\"\n</code></pre></p> </li> <li> <p>Deploy model if not already deployed: <pre><code>curl -X POST \"ES_ENDPOINT/_ml/trained_models/MODEL_ID/deployment/_start\" -H \"Authorization: ApiKey ES_API_KEY\"\n</code></pre></p> </li> </ol> <p>The deployment status response will include: - <code>state</code>: Current deployment state (starting, started, stopping, failed) - <code>allocation_status</code>: Resource allocation status - <code>nodes</code>: List of nodes where the model is deployed - <code>inference_count</code>: Number of inferences performed - <code>error</code>: Any error messages if deployment failed</p> <p>You can also check the Elasticsearch logs for any deployment issues: <pre><code>curl -X GET \"ES_ENDPOINT/_cat/indices/.logs-ml*?v\" -H \"Authorization: ApiKey ES_API_KEY\"\n</code></pre></p>"},{"location":"other-topics/document-processing/elastic/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Connection Errors</li> <li>Verify your Elasticsearch instance is running</li> <li>Check if the endpoint URL is correct</li> <li> <p>Ensure your API key has sufficient permissions</p> </li> <li> <p>Import Failures</p> </li> <li>Confirm you have enough disk space</li> <li>Verify the model ID is correct</li> <li> <p>Check Elasticsearch logs for detailed error messages</p> </li> <li> <p>SSL/TLS Issues</p> </li> <li>If using self-signed certificates, ensure proper configuration</li> <li>Consider using <code>--insecure</code> flag only in development environments</li> </ol>"},{"location":"other-topics/document-processing/elastic/#security-considerations","title":"Security Considerations","text":"<ul> <li>Always use API keys instead of passwords</li> <li>Avoid using <code>--insecure</code> flag in production environments</li> </ul>"},{"location":"other-topics/document-processing/elastic/#additional-resources","title":"Additional Resources","text":"<ul> <li>Elasticsearch Documentation</li> <li>Eland GitHub Repository</li> <li>Hugging Face Model Hub</li> </ul>"},{"location":"reference/","title":"Code Reference Documentation","text":""},{"location":"reference/#architecture-overview","title":"Architecture Overview","text":"<p>The codebase implements a flexible framework for integrating and orchestrating Large Language Models (LLMs) with a focus on:</p> <ul> <li>Vendor-agnostic LLM integration</li> <li>Extensible tool framework</li> <li>Streaming capabilities</li> <li>Pattern detection</li> <li>Database integration for RAG applications</li> </ul>"},{"location":"reference/#core-components","title":"Core Components","text":""},{"location":"reference/#agent-layer","title":"\ud83e\udd16 Agent Layer","text":"<p>The agent layer manages conversations and orchestrates interactions between LLMs and tools. It handles:</p> <ul> <li>Stream management for real-time responses</li> <li>Tool execution coordination</li> <li>Context management</li> <li>Error handling</li> </ul>"},{"location":"reference/#llm-integration","title":"\ud83d\udd2e LLM Integration","text":"<p>The LLM integration layer provides a unified interface to multiple LLM providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>MistralAI</li> <li>WatsonX (with support for multiple models)</li> </ul>"},{"location":"reference/#tools-framework","title":"\ud83d\udee0\ufe0f Tools Framework","text":"<p>The extensible tools framework enables:</p> <ul> <li>Custom tool implementation</li> <li>JSON and non-JSON response parsing</li> <li>REST-based tool integration</li> <li>Built-in tools for common use cases (RAG, Weather)</li> </ul>"},{"location":"reference/#database-integration","title":"\ud83d\udcbe  Database Integration","text":"<p>Database support for vector storage and retrieval:</p> <ul> <li>Elasticsearch integration</li> <li>Milvus support</li> <li>Query building</li> <li>Adapter pattern for database abstraction</li> </ul>"},{"location":"reference/#component-relationships","title":"Component Relationships","text":"<pre><code>graph TD\n    Client[Client] --&gt; Agent[Streaming Agent]\n    Agent --&gt; LLM[LLM Layer]\n    Agent --&gt; Tools[Tools Layer]\n    Tools --&gt; DB[Databases]\n\n    subgraph LLM Providers\n        LLM --&gt; OpenAI\n        LLM --&gt; Anthropic\n        LLM --&gt; MistralAI\n        LLM --&gt; WatsonX\n    end\n\n    subgraph Tools Framework\n        Tools --&gt; BaseTool[Base Tool]\n        BaseTool --&gt; RestTool[REST Tool]\n        BaseTool --&gt; RAGTool[RAG Tool]\n        BaseTool --&gt; WeatherTool[Weather Tool]\n    end\n\n    subgraph Databases\n        DB --&gt; ES[Elasticsearch]\n        DB --&gt; Milvus\n    end\n\n    style Agent stroke:#4169e1,stroke-width:3px\n    style LLM stroke:#228b22,stroke-width:3px\n    style Tools stroke:#cd853f,stroke-width:3px\n    style DB stroke:#4682b4,stroke-width:3px\n    style Client stroke:#333,stroke-width:2px</code></pre>"},{"location":"reference/#key-features","title":"Key Features","text":""},{"location":"reference/#llm-provider-support","title":"LLM Provider Support","text":"<ul> <li>Vendor-agnostic interface</li> <li>Provider-specific prompt builders</li> <li>Auth Token management</li> <li>Error handling and retry logic</li> </ul>"},{"location":"reference/#tool-framework","title":"Tool Framework","text":"<ul> <li>Base classes for rapid tool development</li> <li>REST integration support</li> <li>Parser framework for response handling</li> <li>Registry for tool management</li> </ul>"},{"location":"reference/#pattern-detection","title":"Pattern Detection","text":"<ul> <li>Real-time text pattern matching</li> <li>Aho-Corasick algorithm implementation</li> <li>Buffered processing for streaming</li> <li>Detection strategy framework</li> </ul>"},{"location":"reference/#database-integration_1","title":"Database Integration","text":"<ul> <li>Vector storage support</li> <li>Connection pooling</li> <li>Error handling and retries</li> </ul>"},{"location":"reference/#getting-started","title":"Getting Started","text":""},{"location":"reference/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Understanding the Architecture</p> <ul> <li>Review the Agent documentation</li> <li>Understand LLM Integration patterns</li> <li>Review Data Models</li> </ul> </li> <li> <p>Implementing New Features</p> <ul> <li>Adding a new tool? Start with Tools</li> <li>New LLM provider? See LLM Adapters</li> <li>Custom prompt handling? Check Prompt Builders</li> </ul> </li> <li> <p>Database Integration</p> <ul> <li>Review Database documentation</li> </ul> </li> </ol>"},{"location":"reference/#common-use-cases","title":"Common Use Cases","text":"<ol> <li> <p>Adding a New LLM Provider <pre><code># 1. Create adapter\n# 2. Add prompt builder\n# 3. Update factory\n# See LLM documentation for details\n</code></pre></p> </li> <li> <p>Implementing a Custom Tool <pre><code># 1. Extend BaseTool or BaseRESTTool\n# 2. Register in ToolRegistry\n# 3. Add to configuration\n# See Tools documentation for details\n</code></pre></p> </li> <li> <p>Database Integration <pre><code># 1. Choose adapter\n# 2. Configure connection\n# 3. Build queries\n# See Database documentation for details\n</code></pre></p> </li> </ol>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#common-components","title":"Common Components","text":"<ul> <li>Agent - Core orchestration</li> <li>LLM - Language model integration</li> <li>Tools - Tool framework</li> <li>Database - Storage integration</li> <li>API - REST endpoints</li> </ul>"},{"location":"reference/#utility-components","title":"Utility Components","text":"<ul> <li>Data Models - Shared data structures</li> <li>Utils - Common utilities</li> <li>Prompt Builders - LLM prompting</li> </ul>"},{"location":"reference/agent/","title":"Agent Documentation","text":""},{"location":"reference/agent/#overview","title":"Overview","text":"<p>The Agent module is the core orchestration component of the system, responsible for managing conversational interactions and tool execution. It implements a state-based streaming architecture that handles real-time chat completions, tool detection, and execution.</p>"},{"location":"reference/agent/#key-components","title":"Key Components","text":""},{"location":"reference/agent/#streamingchatagent","title":"StreamingChatAgent","text":"<p>The primary class that manages the conversation flow and tool execution pipeline. It implements a state machine with the following states:</p> <ul> <li>STREAMING: Generates response content from the LLM</li> <li>TOOL_DETECTION: Identifies tool calls in the response</li> <li>EXECUTING_TOOLS: Runs detected tools concurrently</li> <li>INTERMEDIATE: Handles continuation logic and depth limits</li> <li>COMPLETING: Finalizes the response stream</li> </ul>"},{"location":"reference/agent/#state-machine-flow","title":"State Machine Flow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; STREAMING\n    STREAMING --&gt; TOOL_DETECTION: Tool Detected\n    TOOL_DETECTION --&gt; EXECUTING_TOOLS: Process Tools\n    EXECUTING_TOOLS --&gt; INTERMEDIATE: Tools Complete\n    INTERMEDIATE --&gt; STREAMING: Continue\n    INTERMEDIATE --&gt; COMPLETING: Max Depth\n    STREAMING --&gt; COMPLETING: No Tools\n    COMPLETING --&gt; [*]</code></pre>"},{"location":"reference/agent/#features","title":"Features","text":""},{"location":"reference/agent/#tool-detection-strategies","title":"Tool Detection Strategies","text":"<p>The agent supports two detection modes:</p> <ol> <li> <p>Vendor Detection (Default)</p> <ul> <li>Uses LLM provider's native function calling</li> <li>Only works with vendor-supported models</li> <li>Automatic format handling</li> </ul> </li> <li> <p>Manual Detection</p> <ul> <li>Custom parser-based detection</li> <li>More flexible for custom formats</li> <li>Configurable parsing rules</li> </ul> </li> </ol>"},{"location":"reference/agent/#concurrent-tool-execution","title":"Concurrent Tool Execution","text":"<ul> <li>Asynchronous execution of multiple tools</li> <li>Error handling and result aggregation</li> <li>Configurable maximum iteration depth</li> </ul>"},{"location":"reference/agent/#streaming-support","title":"Streaming Support","text":"<ul> <li>Server-Sent Events (SSE) based streaming</li> <li>Real-time token-by-token updates</li> <li>Status updates for tool execution</li> </ul>"},{"location":"reference/agent/#configuration","title":"Configuration","text":"<p>The agent is configured through a YAML file. Please see Agent Configuration for details.</p>"},{"location":"reference/agent/#usage","title":"Usage","text":""},{"location":"reference/agent/#basic-chat-completion","title":"Basic Chat Completion","text":"<pre><code>agent = StreamingChatAgent(config=config)\nasync for response in agent.stream_step(conversation_history):\n    # Process streaming response\n</code></pre>"},{"location":"reference/agent/#with-tool-execution","title":"With Tool Execution","text":"<pre><code>context = {\"api_key\": \"...\"} # Optional context for tools\nasync for response in agent.stream_step(\n    conversation_history=messages,\n    api_passed_context=context\n):\n    # Handle response chunks and tool execution updates\n</code></pre>"},{"location":"reference/agent/#integration-points","title":"Integration Points","text":"<p>The Agent module integrates with several other system components:</p> <ul> <li>LLM Factory: For model-specific interactions</li> <li>Tool Registry: Managing available tools</li> <li>Prompt Builders: Constructing model inputs</li> <li>Data Models: Structured message handling</li> </ul>"},{"location":"reference/agent/#error-handling","title":"Error Handling","text":"<p>The module implements comprehensive error handling through:</p> <ul> <li>Decorator-based error catching</li> <li>Graceful error responses</li> <li>Detailed logging</li> <li>State recovery mechanisms</li> </ul>"},{"location":"reference/agent/#see-also","title":"See Also","text":"<ul> <li>Streaming Chat Agent Documentation</li> <li>Chat Completions API</li> <li>Tool Registry</li> </ul>"},{"location":"reference/agent/chat_agent_streaming/","title":"Streaming Chat Agent","text":""},{"location":"reference/agent/chat_agent_streaming/#src.agent.chat_agent_streaming.StreamingChatAgent","title":"<code>src.agent.chat_agent_streaming.StreamingChatAgent</code>","text":"<p>An agent that handles streaming chat interactions with support for tool execution.</p> <p>This agent processes conversations in a streaming fashion, handling message generation, tool detection, and tool execution. It supports both vendor-specific and manual tool detection strategies.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict</code> <p>Configuration dictionary containing:</p> <ul> <li><code>history_limit</code> (int): Maximum number of historical messages to consider</li> <li><code>system_prompt</code> (str): System prompt to prepend to conversations</li> <li><code>detection_mode</code> (str): Mode for tool detection ('vendor' or 'manual')</li> <li><code>use_vendor_chat_completions</code> (bool): Whether to use vendor chat completions</li> <li><code>models_config</code> (Dict): Configuration for language models</li> <li><code>tools_config</code> (Dict): Configuration for available tools</li> <li><code>logging_level</code> (str): Logging level (default: 'INFO')</li> <li><code>max_streaming_iterations</code> (int): Maximum number of streaming iterations</li> </ul> required <p>Attributes:</p> Name Type Description <code>response_model_name</code> <code>str</code> <p>Name of the main chat model</p> <code>history_limit</code> <code>int</code> <p>Maximum number of historical messages to consider</p> <code>system_prompt</code> <code>str</code> <p>System prompt prepended to conversations</p> <code>logger</code> <code>Logger</code> <p>Logger instance for the agent</p> <code>detection_mode</code> <code>str</code> <p>Current tool detection mode</p> <code>use_vendor_chat_completions</code> <code>bool</code> <p>Whether vendor chat completions are enabled</p> Source code in <code>src/agent/chat_agent_streaming.py</code> <pre><code>class StreamingChatAgent:\n    \"\"\"An agent that handles streaming chat interactions with support for tool execution.\n\n    This agent processes conversations in a streaming fashion, handling message generation,\n    tool detection, and tool execution. It supports both vendor-specific and manual tool\n    detection strategies.\n\n    Args:\n        config (Dict): Configuration dictionary containing:\n\n            - `history_limit` (int): Maximum number of historical messages to consider\n            - `system_prompt` (str): System prompt to prepend to conversations\n            - `detection_mode` (str): Mode for tool detection ('vendor' or 'manual')\n            - `use_vendor_chat_completions` (bool): Whether to use vendor chat completions\n            - `models_config` (Dict): Configuration for language models\n            - `tools_config` (Dict): Configuration for available tools\n            - `logging_level` (str): Logging level (default: 'INFO')\n            - `max_streaming_iterations` (int): Maximum number of streaming iterations\n\n    Attributes:\n        response_model_name (str): Name of the main chat model\n        history_limit (int): Maximum number of historical messages to consider\n        system_prompt (str): System prompt prepended to conversations\n        logger (logging.Logger): Logger instance for the agent\n        detection_mode (str): Current tool detection mode\n        use_vendor_chat_completions (bool): Whether vendor chat completions are enabled\n    \"\"\"\n    def __init__(self, config: Dict) -&gt; None:\n        self.config = config\n        self.response_model_name = \"main_chat_model\"\n        self.history_limit = self.config.get('history_limit', 3)\n        self.system_prompt = self.config.get('system_prompt')\n\n        # Initialize logger\n        self.logger = logging.getLogger(self.__class__.__name__)\n        logging_level = os.getenv(\"LOG_LEVEL\", \"INFO\")\n        logging.basicConfig(level=getattr(logging, logging_level.upper(), None))\n        self.logger.info(f'Logger set to {logging_level}')\n\n        # Initialize vendor-specific components\n        self.main_chat_model_config = self.config.get('models_config').get('main_chat_model')\n        self.llm_factory = LLMFactory(config=self.config.get('models_config'))\n\n        # Determine detection strategy first\n        self.detection_mode = self.config.get(\"detection_mode\", \"vendor\")\n        self.use_vendor_chat_completions = self.config.get(\"use_vendor_chat_completions\", True)\n\n        # Load parser config if needed for manual detection\n        self.logger.info(\"\\n\\n\" + \"=\" * 60 + \"\\n\" + \"Agent Configuration Summary\" + \"\\n\" + \"=\" * 60 + \"\\n\" +\n                         f\"Main Chat Model Config:\\n{json5.dumps(self.main_chat_model_config, indent=4)}\\n\" +\n                         f\"Tool Detection Mode: {self.detection_mode}\\n\" +\n                         f\"Vendor Chat API Mode: {self.use_vendor_chat_completions}\\n\" +\n                         \"\\n\" + \"=\" * 60 + \"\\n\")\n        if self.detection_mode == \"manual\":\n            with open(\"src/configs/parsing.yaml\", \"r\") as f:\n                parser_config = yaml.safe_load(f)\n            self.tool_call_parser = ToolCallParserFactory.get_parser(\n                FormatType.JSON,\n                parser_config\n            )\n            self.detection_strategy = ManualToolCallDetectionStrategy(\n                parser=self.tool_call_parser\n            )\n        else:\n            self.detection_strategy = VendorToolCallDetectionStrategy()\n            self.tool_call_parser = None\n\n        # Initialize prompt builder with inject_tools flag based on detection mode\n        self.prompt_builder: BasePromptBuilder = PromptBuilderFactory.get_prompt_builder(\n            vendor=self.main_chat_model_config.get('vendor')\n        )\n\n        # Initialize tool registry\n        self.tool_registry = ToolRegistry(\n            tools_config=self.config.get(\"tools_config\"),\n            mcp_config=self.config.get(\"mcp_config\")\n        )\n        asyncio.create_task(self.tool_registry.initialize_all_tools())\n\n    @handle_streaming_errors\n    async def stream_step(\n            self,\n            conversation_history: List[TextChatMessage],\n            api_passed_context: Optional[Dict[str, Any]] = None\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Process a conversation step in a streaming fashion.\n\n        Handles the main conversation flow, including message generation, tool detection,\n        and tool execution. The process flows through different states until completion.\n\n        Args:\n            conversation_history (List[TextChatMessage]): List of previous conversation messages\n            api_passed_context (Optional[Dict[str, Any]]): Additional context passed from the API\n\n        Yields:\n            SSEChunk: Server-Sent Events chunks containing response content or status updates\n\n        States:\n            - STREAMING: Generating response content\n            - TOOL_DETECTION: Detecting tool calls in the response\n            - EXECUTING_TOOLS: Running detected tools\n            - INTERMEDIATE: Handling intermediate steps\n            - COMPLETING: Finalizing the response\n        \"\"\"\n        self.logger.debug(\"Starting streaming agent processing\")\n\n        context = await self._initialize_context(conversation_history, api_passed_context)\n\n        while context.current_state != StreamState.COMPLETED:\n            match context.current_state:\n\n                case StreamState.STREAMING:\n                    self.logger.info(f\"--- Entering Streaming State ---\")\n                    async for item in self._handle_streaming(context):\n                        yield item\n\n                case StreamState.TOOL_DETECTION:\n                    self.logger.info(f\"--- Entering Tool Detection State ---\")\n                    async for item in self._handle_tool_detection(context):\n                        yield item\n\n                case StreamState.EXECUTING_TOOLS:\n                    self.logger.info(f\"--- Entering Executing Tools State ---\")\n                    async for item in self._handle_tool_execution(context):\n                        yield item\n\n                case StreamState.INTERMEDIATE:\n                    self.logger.info(f\"--- Entering Intermediate State ---\")\n                    async for item in self._handle_intermediate(context):\n                        yield item\n\n                case StreamState.COMPLETING:\n                    self.logger.info(f\"--- Entering Completing State ---\")\n                    async for item in self._handle_completing(context):\n                        yield item\n                    context.current_state = StreamState.COMPLETED\n\n    # ----------------------------------------------------------------\n    #  STATE HANDLERS\n    # ----------------------------------------------------------------\n\n    @handle_streaming_errors\n    async def _handle_streaming(self, context: StreamContext) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the streaming state of the conversation.\n\n        Processes the main content generation phase, monitoring for tool calls and\n        managing the response stream.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Content chunks and status updates\n\n        Raises:\n            Exception: If maximum streaming iterations are exceeded\n        \"\"\"\n        self.detection_strategy.reset()\n        context.streaming_entry_count += 1\n        if context.streaming_entry_count &gt; context.max_streaming_iterations:\n            self.logger.error(\"Maximum streaming iterations reached. Aborting further streaming.\")\n            yield await SSEChunk.make_stop_chunk(\n                content=\"Maximum streaming depth reached. Please try your request again.\"\n            )\n            context.current_state = StreamState.COMPLETING\n            return\n\n        prompt_payload = PromptPayload(\n            conversation_history=context.conversation_history,\n            tool_definitions=context.tool_definitions if self.detection_mode == \"manual\" else None\n        )\n\n        prompt_output: PromptBuilderOutput = (\n            await self.prompt_builder.build_chat(prompt_payload) if self.use_vendor_chat_completions\n            else await self.prompt_builder.build_text(prompt_payload)\n        )\n        llm_input = prompt_output.get_output()\n\n        stream_kwargs = {\n            'prompt': llm_input if isinstance(llm_input, str) else None,\n            'messages': llm_input if isinstance(llm_input, list) else None,\n            'tools': context.tool_definitions if self.detection_mode != \"manual\" else None\n        }\n        stream_kwargs = {k: v for k, v in stream_kwargs.items() if v is not None}\n\n        self.logger.debug(f\"stream_kwargs: {stream_kwargs}\")\n        llm_adapter = context.llm_factory.get_adapter(self.response_model_name)\n        stream_gen = llm_adapter.gen_sse_stream if isinstance(llm_input, str) else llm_adapter.gen_chat_sse_stream\n\n        accumulated_content = []\n        async for sse_chunk in stream_gen(**stream_kwargs):\n            detection_result = await self.detection_strategy.detect_chunk(sse_chunk, context)\n            self.logger.debug(f\"Detection result: {detection_result}\")\n\n            if detection_result.state in [DetectionState.NO_MATCH, DetectionState.PARTIAL_MATCH]:\n                if detection_result.content:\n                    accumulated_content.append(detection_result.content)\n                    yield SSEChunk.make_text_chunk(detection_result.content)\n\n            elif detection_result.state == DetectionState.COMPLETE_MATCH:\n                async for chunk in self._handle_complete_match(context, detection_result, accumulated_content):\n                    yield chunk\n                return\n\n        final_result = await self.detection_strategy.finalize_detection(context)\n        self.logger.debug(f\"Final detection result: {final_result}\")\n\n        if final_result.state == DetectionState.COMPLETE_MATCH:\n            async for chunk in self._handle_complete_match(context, final_result, accumulated_content):\n                yield chunk\n        else:\n            if final_result.content:\n                accumulated_content.append(final_result.content)\n                yield SSEChunk.make_text_chunk(final_result.content)\n\n            if accumulated_content:\n                context.conversation_history.append(\n                    AssistantMessage(content=\"\".join(accumulated_content))\n                )\n\n            yield await SSEChunk.make_stop_chunk()\n            context.current_state = StreamState.COMPLETING\n\n    @handle_streaming_errors\n    async def _handle_tool_detection(self, context: StreamContext) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the tool detection state.\n\n        Processes detected tool calls and transitions to tool execution.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Tool detection status updates\n        \"\"\"\n        self.logger.debug(\"Tool calls detected, transitioning to EXECUTING_TOOLS\")\n        context.current_state = StreamState.EXECUTING_TOOLS\n        yield await SSEChunk.make_status_chunk(\n            AgentStatus.TOOL_DETECTED,\n            {\"tools\": [tc.format_tool_calls() for tc in context.current_tool_call]}\n        )\n\n    @handle_streaming_errors\n    async def _handle_tool_execution(\n            self,\n            context: StreamContext\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Execute detected tools and process their results.\n\n        Runs the detected tools concurrently and handles their results, including\n        error cases.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Tool execution status updates\n        \"\"\"\n        if context.message_buffer.strip():\n            context.conversation_history.append(\n                AssistantMessage(content=context.message_buffer)\n            )\n            context.message_buffer = \"\"\n\n        results = await self._execute_tools_concurrently(context)\n\n        tool_results = []\n        for call, result in zip(context.current_tool_call, results):\n            context.conversation_history.append(\n                AssistantMessage(tool_calls=[call])\n            )\n            if isinstance(result, Exception):\n                context.conversation_history.append(\n                    AssistantMessage(\n                        content=f\"Error executing tool {call.function.name}: {str(result)}\"\n                    )\n                )\n            else:\n                tool_results.append(result)\n                context.conversation_history.append(\n                    ToolMessage(\n                        content=result[\"result\"],\n                        tool_call_id=call.id\n                    )\n                )\n\n        self.logger.debug(\"Tool execution results: %s\", tool_results)\n        context.current_state = StreamState.INTERMEDIATE\n        yield await SSEChunk.make_status_chunk(AgentStatus.TOOLS_EXECUTED)\n\n    @handle_streaming_errors\n    async def _handle_intermediate(self, context: StreamContext) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the intermediate state between tool executions.\n\n        Resets the message buffer and detection strategy for the next iteration.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Status updates for continuation\n        \"\"\"\n        context.message_buffer = \"\"\n        self.detection_strategy.reset()\n        context.current_state = StreamState.STREAMING\n        yield await SSEChunk.make_status_chunk(AgentStatus.CONTINUING)\n\n    @handle_streaming_errors\n    async def _handle_completing(self, context: StreamContext) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the completion state of the conversation.\n\n        Finalizes the conversation and sends the stop signal.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Final stop chunk\n        \"\"\"\n        self.logger.info(f\"--- Entering COMPLETING State ---\")\n\n        yield await SSEChunk.make_stop_chunk()\n        self.logger.info(f\"Streaming process completed.\")\n\n    # ----------------------------------------------------------------\n    #  HELPER METHODS\n    # ----------------------------------------------------------------\n\n    async def _initialize_context(\n            self,\n            conversation_history: List[TextChatMessage],\n            api_passed_context: Optional[Dict]\n    ) -&gt; StreamContext:\n        \"\"\"Initialize the streaming context for a new conversation step.\n\n        Sets up the conversation history with system prompt and creates the\n        streaming context with necessary configurations.\n\n        Args:\n            conversation_history (List[TextChatMessage]): Previous conversation messages\n            api_passed_context (Optional[Dict]): Additional context from the API\n\n        Returns:\n            StreamContext: Initialized streaming context\n        \"\"\"\n        selected_history = (\n            conversation_history[-self.history_limit:]\n            if self.history_limit &gt; 0\n            else conversation_history\n        )\n\n        if self.system_prompt:\n            system_message = SystemMessage(content=self.system_prompt)\n            selected_history.insert(0, system_message)\n\n        return StreamContext(\n            conversation_history=selected_history,\n            tool_definitions=await self.tool_registry.get_tool_definitions(),\n            context=api_passed_context,\n            llm_factory=self.llm_factory,\n            current_state=StreamState.STREAMING,\n            max_streaming_iterations=self.config.get(\"max_streaming_iterations\", 1),\n            streaming_entry_count=0\n        )\n\n    async def _handle_complete_match(\n            self,\n            context: StreamContext,\n            result: DetectionResult,\n            accumulated_content: List[str]\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle a complete tool call match detection.\n\n        Process detected tool calls and update the conversation history.\n\n        Args:\n            context (StreamContext): Current streaming context\n            result (DetectionResult): Tool detection result\n            accumulated_content (List[str]): Accumulated response content\n\n        Yields:\n            SSEChunk: Content chunks and updates\n        \"\"\"\n        if result.content:\n            accumulated_content.append(result.content)\n            yield SSEChunk.make_text_chunk(result.content)\n\n        context.current_tool_call = result.tool_calls or []\n        if accumulated_content:\n            context.conversation_history.append(\n                AssistantMessage(content=\"\".join(accumulated_content))\n            )\n        context.current_state = StreamState.TOOL_DETECTION\n\n    async def _execute_tools_concurrently(self, context: StreamContext) -&gt; List[Any]:\n        \"\"\"Execute multiple tools concurrently.\n\n        Run detected tools in parallel and handle their results or errors.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Returns:\n            List[Any]: List of tool execution results or exceptions\n\n        Raises:\n            RuntimeError: If a requested tool is not found\n        \"\"\"\n        async def run_tool(tool_call: ToolCall):\n            try:\n                tool = await self.tool_registry.get_tool(tool_call.function.name)\n                if not tool:\n                    raise RuntimeError(f\"Tool {tool_call.function.name} not found\")\n                tool_args = json5.loads(tool_call.function.arguments)\n                self.logger.info(f\"Running tool {tool_call.function.name} with arguments: {tool_args}\")\n                result = await tool.execute(\n                    context=context,\n                    **tool_args\n                )\n                return {\"tool_name\": tool_call.function.name, \"result\": result.result}\n\n            except Exception as e:\n                self.logger.error(f\"Error executing tool {tool_call.function.name}\", exc_info=True)\n                return e\n\n        tasks = [run_tool(tc) for tc in context.current_tool_call]\n        return await asyncio.gather(*tasks, return_exceptions=True)\n</code></pre>"},{"location":"reference/agent/chat_agent_streaming/#src.agent.chat_agent_streaming.StreamingChatAgent.stream_step","title":"<code>stream_step(conversation_history, api_passed_context=None)</code>  <code>async</code>","text":"<p>Process a conversation step in a streaming fashion.</p> <p>Handles the main conversation flow, including message generation, tool detection, and tool execution. The process flows through different states until completion.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_history</code> <code>List[TextChatMessage]</code> <p>List of previous conversation messages</p> required <code>api_passed_context</code> <code>Optional[Dict[str, Any]]</code> <p>Additional context passed from the API</p> <code>None</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Server-Sent Events chunks containing response content or status updates</p> States <ul> <li>STREAMING: Generating response content</li> <li>TOOL_DETECTION: Detecting tool calls in the response</li> <li>EXECUTING_TOOLS: Running detected tools</li> <li>INTERMEDIATE: Handling intermediate steps</li> <li>COMPLETING: Finalizing the response</li> </ul> Source code in <code>src/agent/chat_agent_streaming.py</code> <pre><code>@handle_streaming_errors\nasync def stream_step(\n        self,\n        conversation_history: List[TextChatMessage],\n        api_passed_context: Optional[Dict[str, Any]] = None\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Process a conversation step in a streaming fashion.\n\n    Handles the main conversation flow, including message generation, tool detection,\n    and tool execution. The process flows through different states until completion.\n\n    Args:\n        conversation_history (List[TextChatMessage]): List of previous conversation messages\n        api_passed_context (Optional[Dict[str, Any]]): Additional context passed from the API\n\n    Yields:\n        SSEChunk: Server-Sent Events chunks containing response content or status updates\n\n    States:\n        - STREAMING: Generating response content\n        - TOOL_DETECTION: Detecting tool calls in the response\n        - EXECUTING_TOOLS: Running detected tools\n        - INTERMEDIATE: Handling intermediate steps\n        - COMPLETING: Finalizing the response\n    \"\"\"\n    self.logger.debug(\"Starting streaming agent processing\")\n\n    context = await self._initialize_context(conversation_history, api_passed_context)\n\n    while context.current_state != StreamState.COMPLETED:\n        match context.current_state:\n\n            case StreamState.STREAMING:\n                self.logger.info(f\"--- Entering Streaming State ---\")\n                async for item in self._handle_streaming(context):\n                    yield item\n\n            case StreamState.TOOL_DETECTION:\n                self.logger.info(f\"--- Entering Tool Detection State ---\")\n                async for item in self._handle_tool_detection(context):\n                    yield item\n\n            case StreamState.EXECUTING_TOOLS:\n                self.logger.info(f\"--- Entering Executing Tools State ---\")\n                async for item in self._handle_tool_execution(context):\n                    yield item\n\n            case StreamState.INTERMEDIATE:\n                self.logger.info(f\"--- Entering Intermediate State ---\")\n                async for item in self._handle_intermediate(context):\n                    yield item\n\n            case StreamState.COMPLETING:\n                self.logger.info(f\"--- Entering Completing State ---\")\n                async for item in self._handle_completing(context):\n                    yield item\n                context.current_state = StreamState.COMPLETED\n</code></pre>"},{"location":"reference/api/","title":"API Documentation","text":""},{"location":"reference/api/#overview","title":"Overview","text":"<p>The API module provides HTTP endpoints for interacting with the LLM agent system. It consists of two main components:</p> <ol> <li>Chat Completions API - REST endpoints for chat interactions</li> <li>Server-Sent Events (SSE) Models - Data models for streaming responses</li> </ol>"},{"location":"reference/api/#components","title":"Components","text":""},{"location":"reference/api/#chat-completions-api","title":"Chat Completions API","text":"<p>The Chat Completions API provides REST endpoints that follow patterns similar to OpenAI's Chat API, making it familiar for developers. Key features include:</p> <ul> <li>Support for multiple message types (user, system, assistant)</li> <li>Function/tool calling capabilities</li> <li>Streaming and non-streaming responses</li> <li>Temperature and other LLM parameter controls</li> </ul> <p>Example endpoint: <code>/v1/chat/completions</code></p>"},{"location":"reference/api/#sse-models","title":"SSE Models","text":"<p>The SSE (Server-Sent Events) models handle streaming responses from the system. These models structure the data for:</p> <ul> <li>Incremental token/text-chunk streaming</li> <li>Function/tool call events</li> <li>Status updates</li> <li>Error messages</li> </ul>"},{"location":"reference/api/#api-reference","title":"API Reference","text":"<p>For detailed information about specific components, see:</p> <ul> <li>API Request Models - Details about the chat endpoint models</li> <li>SSE Models - Information about streaming response models</li> </ul>"},{"location":"reference/api/#see-also","title":"See Also","text":"<ul> <li>Agent Configuration - Configure the underlying agent</li> <li>Tool Registry - Available tools and functions</li> <li>LLM Factory - LLM provider configuration</li> </ul>"},{"location":"reference/api/chat_completions_api/","title":"Chat completions api","text":""},{"location":"reference/api/chat_completions_api/#src.api.routes.chat_completions_api.ChatCompletionRequest","title":"<code>src.api.routes.chat_completions_api.ChatCompletionRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for chat completion endpoints.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Optional[str]</code> <p>ID of the model to use for completion.</p> <code>messages</code> <code>List[dict]</code> <p>Array of message objects with role and content.</p> <code>context</code> <code>Optional[ContextModel]</code> <p>Additional context for API tools.</p> Source code in <code>src/api/request_models.py</code> <pre><code>class ChatCompletionRequest(BaseModel):\n    \"\"\"Request model for chat completion endpoints.\n\n    Attributes:\n        model (Optional[str]): ID of the model to use for completion.\n        messages (List[dict]): Array of message objects with role and content.\n        context (Optional[ContextModel]): Additional context for API tools.\n    \"\"\"\n    model: Optional[str] = Field(None, description=\"ID of the model to use\")\n    messages: List[TextChatMessage] = Field(..., description=\"Array of messages (role/content)\")\n    context: Optional[ContextModel] = Field(None, description=\"Additional context values (e.g. for API tools)\")\n</code></pre>"},{"location":"reference/api/request_models/","title":"Request Models","text":""},{"location":"reference/api/request_models/#request-models","title":"Request Models","text":""},{"location":"reference/api/request_models/#src.api.request_models.ChatCompletionRequest","title":"<code>src.api.request_models.ChatCompletionRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for chat completion endpoints.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Optional[str]</code> <p>ID of the model to use for completion.</p> <code>messages</code> <code>List[dict]</code> <p>Array of message objects with role and content.</p> <code>context</code> <code>Optional[ContextModel]</code> <p>Additional context for API tools.</p> Source code in <code>src/api/request_models.py</code> <pre><code>class ChatCompletionRequest(BaseModel):\n    \"\"\"Request model for chat completion endpoints.\n\n    Attributes:\n        model (Optional[str]): ID of the model to use for completion.\n        messages (List[dict]): Array of message objects with role and content.\n        context (Optional[ContextModel]): Additional context for API tools.\n    \"\"\"\n    model: Optional[str] = Field(None, description=\"ID of the model to use\")\n    messages: List[TextChatMessage] = Field(..., description=\"Array of messages (role/content)\")\n    context: Optional[ContextModel] = Field(None, description=\"Additional context values (e.g. for API tools)\")\n</code></pre>"},{"location":"reference/api/request_models/#src.api.request_models.ContextValue","title":"<code>src.api.request_models.ContextValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a key-value pair for contextual information.</p> <p>This model is used to store individual pieces of context data that can be passed to tools or used in conversation.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The identifier for this piece of context</p> <code>value</code> <code>str</code> <p>The actual context value</p> Source code in <code>src/api/request_models.py</code> <pre><code>class ContextValue(BaseModel):\n    \"\"\"Represents a key-value pair for contextual information.\n\n    This model is used to store individual pieces of context data\n    that can be passed to tools or used in conversation.\n\n    Attributes:\n        key (str): The identifier for this piece of context\n        value (str): The actual context value\n    \"\"\"\n    key: str\n    value: str\n</code></pre>"},{"location":"reference/api/request_models/#src.api.request_models.ContextModel","title":"<code>src.api.request_models.ContextModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for multiple context values.</p> <p>This model serves as a collection of context values that can be passed around the system to provide additional information to tools and conversations.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>List[ContextValue]</code> <p>List of context key-value pairs</p> <p>Note: This model is used to define a structure for exposing in the API for allowing additional context to be passed through the system. The keys and values are used to create a dictionary which is passed to the streaming agent and anything downstream.</p> Source code in <code>src/api/request_models.py</code> <pre><code>class ContextModel(BaseModel):\n    \"\"\"Container for multiple context values.\n\n    This model serves as a collection of context values that can be\n    passed around the system to provide additional information to\n    tools and conversations.\n\n    Attributes:\n        values (List[ContextValue]): List of context key-value pairs\n\n    Note: This model is used to define a structure for exposing in the API for allowing additional context to be passed through the system. The keys and values are used to create a dictionary which is passed to the streaming agent and anything downstream.\n    \"\"\"\n    values: Optional[List[ContextValue]] = Field(None, description=\"Additional context values (e.g. for API tools)\")\n</code></pre>"},{"location":"reference/api/sse_models/","title":"Sse Models","text":""},{"location":"reference/api/sse_models/#src.api.sse_models.AgentStatus","title":"<code>src.api.sse_models.AgentStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>src/api/sse_models.py</code> <pre><code>class AgentStatus(str, Enum):\n    STARTING = \"starting_generation\"\n    TOOL_DETECTED = \"tool_call_detected\"\n    TOOLS_EXECUTED = \"tools_executed\"\n    MAX_DEPTH = \"max_depth_reached\"\n    CONTINUING = \"continuing_generation\"\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEStatus","title":"<code>src.api.sse_models.SSEStatus</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/api/sse_models.py</code> <pre><code>class SSEStatus(BaseModel):\n    status: AgentStatus\n    details: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEFunction","title":"<code>src.api.sse_models.SSEFunction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for function calls in SSE responses, with support for streaming chunks.</p> Source code in <code>src/api/sse_models.py</code> <pre><code>class SSEFunction(BaseModel):\n    \"\"\"Model for function calls in SSE responses, with support for streaming chunks.\"\"\"\n    name: str = \"\"\n    arguments: str = \"\"\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEToolCall","title":"<code>src.api.sse_models.SSEToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for tool calls in SSE responses.</p> Source code in <code>src/api/sse_models.py</code> <pre><code>class SSEToolCall(BaseModel):\n    \"\"\"Model for tool calls in SSE responses.\"\"\"\n    index: int = 0\n    id: Optional[str] = None\n    type: str = \"function\"\n    function: Optional[SSEFunction] = None\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEDelta","title":"<code>src.api.sse_models.SSEDelta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for delta content in SSE responses.</p> Source code in <code>src/api/sse_models.py</code> <pre><code>class SSEDelta(BaseModel):\n    \"\"\"Model for delta content in SSE responses.\"\"\"\n    role: Optional[str] = None\n    content: Optional[str] = None\n    tool_calls: Optional[List[SSEToolCall]] = None\n    refusal: Optional[str] = None\n    status: Optional[str] = None\n    metadata: Optional[dict] = None\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEChoice","title":"<code>src.api.sse_models.SSEChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for choices in SSE responses.</p> Source code in <code>src/api/sse_models.py</code> <pre><code>class SSEChoice(BaseModel):\n    \"\"\"Model for choices in SSE responses.\"\"\"\n    index: int\n    delta: SSEDelta\n    logprobs: Optional[dict] = None\n    finish_reason: Optional[str] = None\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEChunk","title":"<code>src.api.sse_models.SSEChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for SSE chunks.</p> Source code in <code>src/api/sse_models.py</code> <pre><code>class SSEChunk(BaseModel):\n    \"\"\"Model for SSE chunks.\"\"\"\n    id: str\n    object: str\n    created: int\n    model: str\n    service_tier: Optional[str] = None\n    system_fingerprint: Optional[str] = None\n    choices: List[SSEChoice]\n    thread_id: Optional[str] = None  # this is an IBM wxO thing\n\n    @staticmethod\n    def make_text_chunk(text: str) -&gt; 'SSEChunk':\n        \"\"\"\n        Utility to create a minimal SSEChunk that only has user-visible 'content'.\n        This ensures we never leak partial function-call details back to the user.\n        \"\"\"\n        return SSEChunk(\n            id=f\"chatcmpl-{time.time()}\",\n            object=\"chat.completion.chunk\",\n            created=int(time.time()),\n            model=\"agent-01\",\n            choices=[\n                SSEChoice(\n                    index=0,\n                    delta=SSEDelta(role=\"assistant\", content=text),\n                    finish_reason=None\n                )\n            ]\n        )\n\n    @staticmethod\n    async def make_status_chunk(status: str, extra_info: Optional[Dict] = None) -&gt; 'SSEChunk':\n        metadata = {\"status\": status}\n        if extra_info:\n            metadata.update(extra_info)\n\n        return SSEChunk(\n            id=f\"status_{time.time()}\",\n            object=\"chat.completion.chunk\",\n            created=int(time.time()),\n            model=\"agent-01\",\n            choices=[\n                SSEChoice(\n                    index=0,\n                    delta=SSEDelta(\n                        role=\"system\",\n                        metadata=metadata\n                    ),\n                    finish_reason=None\n                )\n            ]\n        )\n\n    @staticmethod\n    async def make_stop_chunk(content=None, refusal=None) -&gt; 'SSEChunk':\n        return SSEChunk(\n            id=f\"chatcmpl-{time.time()}\",\n            object=\"chat.completion.chunk\",\n            created=int(time.time()),\n            model=\"agent-01\",\n            choices=[\n                SSEChoice(\n                    index=0,\n                    delta=SSEDelta(role=\"assistant\", content=content, refusal=refusal),\n                    finish_reason=\"stop\"\n                )\n            ]\n        )\n</code></pre>"},{"location":"reference/api/sse_models/#src.api.sse_models.SSEChunk.make_text_chunk","title":"<code>make_text_chunk(text)</code>  <code>staticmethod</code>","text":"<p>Utility to create a minimal SSEChunk that only has user-visible 'content'. This ensures we never leak partial function-call details back to the user.</p> Source code in <code>src/api/sse_models.py</code> <pre><code>@staticmethod\ndef make_text_chunk(text: str) -&gt; 'SSEChunk':\n    \"\"\"\n    Utility to create a minimal SSEChunk that only has user-visible 'content'.\n    This ensures we never leak partial function-call details back to the user.\n    \"\"\"\n    return SSEChunk(\n        id=f\"chatcmpl-{time.time()}\",\n        object=\"chat.completion.chunk\",\n        created=int(time.time()),\n        model=\"agent-01\",\n        choices=[\n            SSEChoice(\n                index=0,\n                delta=SSEDelta(role=\"assistant\", content=text),\n                finish_reason=None\n            )\n        ]\n    )\n</code></pre>"},{"location":"reference/data_models/","title":"Data Models","text":""},{"location":"reference/data_models/#overview","title":"Overview","text":"<p>The data models package provides the core data structures used throughout the application. These models define the shape of data for chat completions, streaming operations, tools/functions, and agent configurations. Each model is designed to be immutable and serializable, ensuring type safety and consistent data handling across the application.</p>"},{"location":"reference/data_models/#core-components","title":"Core Components","text":""},{"location":"reference/data_models/#agent-models","title":"Agent Models","text":"<p>The agent models (<code>StreamState</code>, <code>StreamResult</code>, <code>StreamContext</code>, <code>Config</code>) handle the state and configuration of chat agents:</p> <pre><code>graph TD\n    A[StreamContext] --&gt; B[StreamState]\n    A --&gt; C[StreamResult]\n    A --&gt; D[Config]</code></pre>"},{"location":"reference/data_models/#chat-completion-models","title":"Chat Completion Models","text":"<p>These models represent the structure of chat messages and conversations:</p> <pre><code>graph TD\n    A[MessageBase] --&gt; B[UserMessage]\n    A --&gt; C[AssistantMessage]\n    A --&gt; D[SystemMessage]\n    A --&gt; E[ToolMessage]\n    B --&gt; F[UserTextContent]\n    B --&gt; G[UserImageURLContent]</code></pre>"},{"location":"reference/data_models/#tool-models","title":"Tool Models","text":"<p>The tool models (<code>Tool</code>, <code>Function</code>, <code>ToolsList</code>) define the structure for function calling and tool execution:</p> <pre><code>graph TD\n    A[Tool] --&gt; B[Function]\n    B --&gt; C[FunctionParameters]\n    D[ToolsList] --&gt; A</code></pre>"},{"location":"reference/data_models/#streaming-models","title":"Streaming Models","text":"<p>Models for handling streaming operations and pattern matching:</p> <ul> <li><code>PatternMatchResult</code>: Represents pattern matching results during streaming</li> <li><code>StreamConfig</code>: Configuration for streaming operations</li> </ul>"},{"location":"reference/data_models/#wx-assistant-models","title":"WX Assistant Models","text":"<p>Models specific to Watson Assistant integration:</p> <ul> <li><code>WxAssistantMessage</code>: Message format for Watson Assistant</li> <li><code>WxAssistantConversationInput</code>: Input structure for Watson Assistant conversations</li> </ul>"},{"location":"reference/data_models/#module-structure","title":"Module Structure","text":"<pre><code>data_models/\n\u251c\u2500\u2500 agent.py          # Agent-related models\n\u251c\u2500\u2500 chat_completions.py  # Chat completion models\n\u251c\u2500\u2500 streaming.py      # Streaming operation models\n\u251c\u2500\u2500 tools.py          # Tool and function models\n\u2514\u2500\u2500 wx_assistant.py   # Watson Assistant models\n</code></pre>"},{"location":"reference/data_models/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/data_models/#message-chain-building","title":"Message Chain Building","text":"<pre><code>messages = [\n    SystemMessage(content=\"You are a helpful assistant\"),\n    UserMessage(content=UserTextContent(text=\"Hello\")),\n    AssistantMessage(content=\"Hi! How can I help you today?\")\n]\n</code></pre>"},{"location":"reference/data_models/#see-also","title":"See Also","text":"<ul> <li>Agent Documentation - Understanding how data models are used in the agent</li> <li>API Documentation - How data models are serialized in API responses</li> <li>Tool Documentation - Details on tool-specific data models</li> </ul>"},{"location":"reference/data_models/agent/","title":"Agent","text":""},{"location":"reference/data_models/agent/#src.data_models.agent.StreamState","title":"<code>src.data_models.agent.StreamState</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Defines the possible states of the streaming agent during processing.</p> <p>The StreamState enum represents different operational states that the streaming agent can be in at any given time during message processing and tool execution.</p> <p>Attributes:</p> Name Type Description <code>STREAMING</code> <p>Currently streaming response content</p> <code>TOOL_DETECTION</code> <p>Analyzing stream for potential tool calls</p> <code>EXECUTING_TOOLS</code> <p>Currently executing detected tools</p> <code>INTERMEDIATE</code> <p>Temporary state between major operations</p> Example <pre><code>state = StreamState.IDLE\nif processing_started:\n    state = StreamState.STREAMING\n</code></pre> Source code in <code>src/data_models/agent.py</code> <pre><code>class StreamState(Enum):\n    \"\"\"Defines the possible states of the streaming agent during processing.\n\n    The StreamState enum represents different operational states that the streaming agent\n    can be in at any given time during message processing and tool execution.\n\n    Attributes:\n        STREAMING: Currently streaming response content\n        TOOL_DETECTION: Analyzing stream for potential tool calls\n        EXECUTING_TOOLS: Currently executing detected tools\n        INTERMEDIATE: Temporary state between major operations\n\n    Example:\n        ```python\n        state = StreamState.IDLE\n        if processing_started:\n            state = StreamState.STREAMING\n        ```\n    \"\"\"\n    STREAMING = \"streaming\"\n    TOOL_DETECTION = \"detection\"\n    EXECUTING_TOOLS = \"executing\"\n    INTERMEDIATE = \"intermediate\"\n    COMPLETING = \"completing\"\n    COMPLETED = \"completed\"\n</code></pre>"},{"location":"reference/data_models/agent/#src.data_models.agent.StreamResult","title":"<code>src.data_models.agent.StreamResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the result of processing an individual stream chunk.</p> <p>This class encapsulates the various pieces of information that can be produced when processing a single chunk of a stream, including content, errors, and status updates.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Optional[str]</code> <p>The actual content from the stream chunk. May be None if chunk contained no content (e.g., only status updates).</p> <code>error</code> <code>Optional[str]</code> <p>Error message if any issues occurred during processing. None if processing was successful.</p> <code>status</code> <code>Optional[str]</code> <p>Status message indicating state changes or completion. Used to communicate processing progress.</p> <code>should_continue</code> <code>bool</code> <p>Flag indicating if streaming should continue. Defaults to True, set to False for terminating streaming.</p> Example <pre><code>result = StreamResult(\n    content=\"Generated text response\",\n    status=\"Processing complete\",\n    should_continue=True\n)\n</code></pre> Source code in <code>src/data_models/agent.py</code> <pre><code>class StreamResult(BaseModel):\n    \"\"\"Represents the result of processing an individual stream chunk.\n\n    This class encapsulates the various pieces of information that can be produced\n    when processing a single chunk of a stream, including content, errors, and status updates.\n\n    Attributes:\n        content (Optional[str]): The actual content from the stream chunk. May be None if\n            chunk contained no content (e.g., only status updates).\n        error (Optional[str]): Error message if any issues occurred during processing.\n            None if processing was successful.\n        status (Optional[str]): Status message indicating state changes or completion.\n            Used to communicate processing progress.\n        should_continue (bool): Flag indicating if streaming should continue.\n            Defaults to True, set to False for terminating streaming.\n\n    Example:\n        ```python\n        result = StreamResult(\n            content=\"Generated text response\",\n            status=\"Processing complete\",\n            should_continue=True\n        )\n        ```\n    \"\"\"\n    content: Optional[str] = Field(\n        default=None,\n        description=\"The content of the stream chunk\"\n    )\n    error: Optional[str] = Field(\n        default=None,\n        description=\"Error message if processing failed\"\n    )\n    status: Optional[str] = Field(\n        default=None,\n        description=\"Status message indicating state changes or completion\"\n    )\n    should_continue: bool = Field(\n        default=True,\n        description=\"Flag indicating if streaming should continue\"\n    )\n</code></pre>"},{"location":"reference/data_models/agent/#src.data_models.agent.StreamContext","title":"<code>src.data_models.agent.StreamContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Context and state for a streaming conversation session.</p> <p>This class stores all necessary information for managing a streaming conversation, including conversation history, available tool definitions, state buffers, and session metadata. It also tracks the number of times the streaming state has been initiated.</p> <p>Attributes:</p> Name Type Description <code>conversation_history</code> <code>List[TextChatMessage]</code> <p>The full conversation history, including a system message at the start if available.</p> <code>tool_definitions</code> <code>List[Tool]</code> <p>Definitions of available tools for execution.</p> <code>message_buffer</code> <code>str</code> <p>Buffer for accumulating generated response text.</p> <code>tool_call_buffer</code> <code>str</code> <p>Buffer for accumulating potential tool call text until parsing.</p> <code>current_tool_call</code> <code>Optional[List[ToolCall]]</code> <p>The currently processing tool calls, if any.</p> <code>current_state</code> <code>StreamState</code> <p>The current state of the stream processing.</p> <code>streaming_entry_count</code> <code>int</code> <p>Counter tracking the number of times the streaming state has been entered.</p> <code>max_streaming_iterations</code> <code>int</code> <p>The maximum allowed number of times the streaming state can be initiated.</p> <code>context</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata associated with the streaming session.</p> <code>llm_factory</code> <code>Optional[LLMFactory]</code> <p>LLM factory associated with the streaming agent.</p> Source code in <code>src/data_models/agent.py</code> <pre><code>class StreamContext(BaseModel):\n    \"\"\"Context and state for a streaming conversation session.\n\n    This class stores all necessary information for managing a streaming conversation,\n    including conversation history, available tool definitions, state buffers, and\n    session metadata. It also tracks the number of times the streaming state has been\n    initiated.\n\n    Attributes:\n        conversation_history (List[TextChatMessage]): The full conversation history,\n            including a system message at the start if available.\n        tool_definitions (List[Tool]): Definitions of available tools for execution.\n        message_buffer (str): Buffer for accumulating generated response text.\n        tool_call_buffer (str): Buffer for accumulating potential tool call text until parsing.\n        current_tool_call (Optional[List[ToolCall]]): The currently processing tool calls, if any.\n        current_state (StreamState): The current state of the stream processing.\n        streaming_entry_count (int): Counter tracking the number of times the streaming state has been entered.\n        max_streaming_iterations (int): The maximum allowed number of times the streaming state can be initiated.\n        context (Optional[Dict[str, Any]]): Additional metadata associated with the streaming session.\n        llm_factory (Optional[LLMFactory]): LLM factory associated with the streaming agent.\n    \"\"\"\n\n    conversation_history: List[TextChatMessage] = Field(\n        default_factory=list,\n        description=\"Full conversation history with system message at the start.\"\n    )\n    tool_definitions: List[Tool] = Field(\n        default_factory=list,\n        description=\"Definitions of available tools.\"\n    )\n    message_buffer: str = Field(\n        default=\"\",\n        description=\"Buffer for accumulating generated response text.\"\n    )\n    tool_call_buffer: str = Field(\n        default=\"\",\n        description=\"Buffer for accumulating tool call text until parsing.\"\n    )\n    current_tool_call: Optional[List[ToolCall]] = Field(\n        default=None,\n        description=\"Currently processing tool calls.\"\n    )\n    current_state: StreamState = Field(\n        default=None,\n        description=\"Current state of the stream processing.\"\n    )\n    streaming_entry_count: int = Field(\n        default=0,\n        description=\"Tracks how many times the streaming state has been entered.\"\n    )\n    max_streaming_iterations: int = Field(\n        default=3,\n        description=\"The maximum allowed number of times the streaming state can be initiated.\"\n    )\n    context: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"Optional metadata associated with the streaming session.\"\n    )\n    llm_factory: Optional[LLMFactory] = Field(\n        default=None,\n        description=\"LLM Model factory for retrieving LLM adapters.\"\n    )\n\n    class Config:\n        \"\"\"Pydantic model configuration.\"\"\"\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/data_models/agent/#src.data_models.agent.StreamContext.Config","title":"<code>Config</code>","text":"<p>Pydantic model configuration.</p> Source code in <code>src/data_models/agent.py</code> <pre><code>class Config:\n    \"\"\"Pydantic model configuration.\"\"\"\n    arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/data_models/chat_completions/","title":"Chat Completions","text":""},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.MessageBase","title":"<code>src.data_models.chat_completions.MessageBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all message types in the chat completion system.</p> <p>This class serves as the foundation for all message types, enforcing a common structure and validation rules. It uses Pydantic's strict mode to forbid extra attributes.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message sender. Must be implemented by child classes.</p> Note <p>This class should not be used directly but rather inherited by specific message types.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class MessageBase(BaseModel):\n    \"\"\"Base class for all message types in the chat completion system.\n\n    This class serves as the foundation for all message types, enforcing a common structure\n    and validation rules. It uses Pydantic's strict mode to forbid extra attributes.\n\n    Attributes:\n        role (str): The role of the message sender. Must be implemented by child classes.\n\n    Note:\n        This class should not be used directly but rather inherited by specific message types.\n    \"\"\"\n    role: str\n\n    model_config = ConfigDict(extra='forbid')\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.UserTextContent","title":"<code>src.data_models.chat_completions.UserTextContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the text content structure for user messages.</p> <p>This model defines the format for text-based content in user messages, ensuring proper typing and validation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['text']</code> <p>Content type identifier, always set to \"text\".</p> <code>text</code> <code>str</code> <p>The actual text content of the user message.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class UserTextContent(BaseModel):\n    \"\"\"Represents the text content structure for user messages.\n\n    This model defines the format for text-based content in user messages, ensuring\n    proper typing and validation.\n\n    Attributes:\n        type (Literal[\"text\"]): Content type identifier, always set to \"text\".\n        text (str): The actual text content of the user message.\n    \"\"\"\n    type: Literal[\"text\"] = Field(default=\"text\", description=\"Content type, fixed to 'text' for user messages\")\n    text: str = Field(..., description=\"The text content of the user message\")\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.UserImageURLContent","title":"<code>src.data_models.chat_completions.UserImageURLContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the image URL content structure for user messages.</p> <p>This model defines the format for image-based content in user messages, supporting base64 encoded images with configurable processing detail levels.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['image_url']</code> <p>Content type identifier, always set to \"image_url\".</p> <code>image_url</code> <code>dict</code> <p>Dictionary containing a 'url' field with a base64 encoded image string.</p> <code>detail</code> <code>Optional[Literal['low', 'high', 'auto']]</code> <p>Processing detail level for the image. Defaults to \"auto\".</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class UserImageURLContent(BaseModel):\n    \"\"\"Represents the image URL content structure for user messages.\n\n    This model defines the format for image-based content in user messages, supporting\n    base64 encoded images with configurable processing detail levels.\n\n    Attributes:\n        type (Literal[\"image_url\"]): Content type identifier, always set to \"image_url\".\n        image_url (dict): Dictionary containing a 'url' field with a base64 encoded image string.\n        detail (Optional[Literal[\"low\", \"high\", \"auto\"]]): Processing detail level for the image.\n            Defaults to \"auto\".\n    \"\"\"\n    type: Literal[\"image_url\"] = Field(\n        default=\"image_url\",\n        description=\"Content type, fixed to 'image_url' for user messages\"\n    )\n    image_url: dict = Field(\n        ...,\n        description=\"The image URL as a dictionary containing a 'url' field with a base64 encoded string\"\n    )\n    detail: Optional[Literal[\"low\", \"high\", \"auto\"]] = Field(\n        default=\"auto\",\n        description=\"Detail level for image processing\"\n    )\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.UserMessage","title":"<code>src.data_models.chat_completions.UserMessage</code>","text":"<p>               Bases: <code>MessageBase</code></p> <p>Represents a message from the user in the chat completion system.</p> <p>This model handles both simple text messages and complex content types including images. It supports single string content or a list of mixed content types.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['user']</code> <p>Role identifier, always set to \"user\".</p> <code>content</code> <code>Union[str, List[Union[UserTextContent, UserImageURLContent]]]</code> <p>The message content, either as a simple string or a list of content objects.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class UserMessage(MessageBase):\n    \"\"\"Represents a message from the user in the chat completion system.\n\n    This model handles both simple text messages and complex content types including\n    images. It supports single string content or a list of mixed content types.\n\n    Attributes:\n        role (Literal[\"user\"]): Role identifier, always set to \"user\".\n        content (Union[str, List[Union[UserTextContent, UserImageURLContent]]]):\n            The message content, either as a simple string or a list of content objects.\n    \"\"\"\n    role: Literal[\"user\"] = Field(default=\"user\", description=\"Role is fixed to 'user' for user messages\")\n    content: Union[str, List[Union[UserTextContent, UserImageURLContent]]] = Field(\n        ..., description=\"String or detailed content of the user message\"\n    )\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.FunctionDetail","title":"<code>src.data_models.chat_completions.FunctionDetail</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the structure for function call details in tool calls.</p> <p>This model contains the essential information needed to execute a function through the tool calling system.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function to be called.</p> <code>arguments</code> <code>Dict</code> <p>Dictionary of arguments to be passed to the function.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class FunctionDetail(BaseModel):\n    \"\"\"Defines the structure for function call details in tool calls.\n\n    This model contains the essential information needed to execute a function\n    through the tool calling system.\n\n    Attributes:\n        name (str): The name of the function to be called.\n        arguments (Dict): Dictionary of arguments to be passed to the function.\n    \"\"\"\n    name: str = Field(..., description=\"Name of the function\")\n    arguments: str = Field(..., description=\"Arguments for the function\")\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.ToolCall","title":"<code>src.data_models.chat_completions.ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool call made by the assistant.</p> <p>This model handles the structure and formatting of tool calls, including custom serialization of function arguments to JSON.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the tool call.</p> <code>type</code> <code>Literal['function']</code> <p>Type of tool call, currently only supports \"function\".</p> <code>function</code> <code>FunctionDetail</code> <p>Detailed information about the function to be called.</p> <p>Methods:</p> Name Description <code>model_dump</code> <p>Custom serialization method that converts function arguments to JSON string.</p> <code>format_tool_calls</code> <p>Formats the tool call as a JSON array string for API compatibility.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"Represents a tool call made by the assistant.\n\n    This model handles the structure and formatting of tool calls, including\n    custom serialization of function arguments to JSON.\n\n    Attributes:\n        id (str): Unique identifier for the tool call.\n        type (Literal[\"function\"]): Type of tool call, currently only supports \"function\".\n        function (FunctionDetail): Detailed information about the function to be called.\n\n    Methods:\n        model_dump(*args, **kwargs) -&gt; dict:\n            Custom serialization method that converts function arguments to JSON string.\n\n        format_tool_calls() -&gt; str:\n            Formats the tool call as a JSON array string for API compatibility.\n    \"\"\"\n    id: str = Field(..., description=\"ID of the tool call\")\n    type: Literal[\"function\"] = Field(default=\"function\", description=\"Tool type, currently only 'function' is allowed\")\n    function: FunctionDetail = Field(..., description=\"Details of the function call, including name and arguments\")\n\n    def model_dump(self, *args, **kwargs) -&gt; dict:\n        \"\"\"Custom model_dump to convert 'arguments' in function to a JSON string.\"\"\"\n        # Call the original model_dump\n        data = super().model_dump(*args, **kwargs)\n\n        # Convert 'arguments' to a JSON string within 'function'\n        if \"function\" in data:\n            data[\"function\"][\"arguments\"] = json.dumps(data[\"function\"][\"arguments\"])\n\n        return data\n\n    def format_tool_calls(self) -&gt; str:\n        \"\"\"Format tool call as a JSON array string.\"\"\"\n        formatted_call = {\n            \"name\": self.function.name,\n            \"arguments\": self.function.arguments\n        }\n        return json.dumps(formatted_call)\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.ToolCall.format_tool_calls","title":"<code>format_tool_calls()</code>","text":"<p>Format tool call as a JSON array string.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>def format_tool_calls(self) -&gt; str:\n    \"\"\"Format tool call as a JSON array string.\"\"\"\n    formatted_call = {\n        \"name\": self.function.name,\n        \"arguments\": self.function.arguments\n    }\n    return json.dumps(formatted_call)\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.ToolCall.model_dump","title":"<code>model_dump(*args, **kwargs)</code>","text":"<p>Custom model_dump to convert 'arguments' in function to a JSON string.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>def model_dump(self, *args, **kwargs) -&gt; dict:\n    \"\"\"Custom model_dump to convert 'arguments' in function to a JSON string.\"\"\"\n    # Call the original model_dump\n    data = super().model_dump(*args, **kwargs)\n\n    # Convert 'arguments' to a JSON string within 'function'\n    if \"function\" in data:\n        data[\"function\"][\"arguments\"] = json.dumps(data[\"function\"][\"arguments\"])\n\n    return data\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.AssistantMessage","title":"<code>src.data_models.chat_completions.AssistantMessage</code>","text":"<p>               Bases: <code>MessageBase</code></p> <p>Represents a message from the assistant in the chat completion system.</p> <p>This model handles various types of assistant responses, including regular messages, tool calls, and refusals. It includes custom serialization logic to handle None values.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['assistant']</code> <p>Role identifier, always set to \"assistant\".</p> <code>content</code> <code>Optional[Union[str, List[dict]]]</code> <p>The content of the assistant's message.</p> <code>refusal</code> <code>Optional[str]</code> <p>Optional refusal message if the assistant declines to respond.</p> <code>tool_calls</code> <code>Optional[List[ToolCall]]</code> <p>List of tool calls made by the assistant.</p> <p>Methods:</p> Name Description <code>model_dump</code> <p>Custom serialization method that excludes None values and properly formats tool calls.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class AssistantMessage(MessageBase):\n    \"\"\"Represents a message from the assistant in the chat completion system.\n\n    This model handles various types of assistant responses, including regular messages,\n    tool calls, and refusals. It includes custom serialization logic to handle None values.\n\n    Attributes:\n        role (Literal[\"assistant\"]): Role identifier, always set to \"assistant\".\n        content (Optional[Union[str, List[dict]]]): The content of the assistant's message.\n        refusal (Optional[str]): Optional refusal message if the assistant declines to respond.\n        tool_calls (Optional[List[ToolCall]]): List of tool calls made by the assistant.\n\n    Methods:\n        model_dump(*args, **kwargs) -&gt; dict:\n            Custom serialization method that excludes None values and properly formats tool calls.\n    \"\"\"\n    role: Literal[\"assistant\"] = Field(\n        default=\"assistant\", description=\"Role is fixed to 'assistant' for assistant messages\"\n    )\n    content: Optional[Union[str, List[dict]]] = Field(None, description=\"The content of the assistant message\")\n    refusal: Optional[str] = Field(None, description=\"The refusal message by the assistant\")\n    tool_calls: Optional[List[ToolCall]] = Field(None, description=\"List of tool calls made by the assistant\")\n\n    def model_dump(self, *args, **kwargs) -&gt; dict:\n        \"\"\"Custom model_dump that excludes fields with None values and calls model_dump on nested ToolCall models.\"\"\"\n        data = super().model_dump(*args, **kwargs)\n        if self.tool_calls:\n            data[\"tool_calls\"] = [call.model_dump() for call in self.tool_calls]\n        return {key: value for key, value in data.items() if value is not None}\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.AssistantMessage.model_dump","title":"<code>model_dump(*args, **kwargs)</code>","text":"<p>Custom model_dump that excludes fields with None values and calls model_dump on nested ToolCall models.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>def model_dump(self, *args, **kwargs) -&gt; dict:\n    \"\"\"Custom model_dump that excludes fields with None values and calls model_dump on nested ToolCall models.\"\"\"\n    data = super().model_dump(*args, **kwargs)\n    if self.tool_calls:\n        data[\"tool_calls\"] = [call.model_dump() for call in self.tool_calls]\n    return {key: value for key, value in data.items() if value is not None}\n</code></pre>"},{"location":"reference/data_models/chat_completions/#src.data_models.chat_completions.SystemMessage","title":"<code>src.data_models.chat_completions.SystemMessage</code>","text":"<p>               Bases: <code>MessageBase</code></p> <p>Represents a system message in the chat completion system.</p> <p>This model handles system-level instructions and context that guide the conversation behavior.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['system']</code> <p>Role identifier, always set to \"system\".</p> <code>content</code> <code>str</code> <p>The content of the system message.</p> Source code in <code>src/data_models/chat_completions.py</code> <pre><code>class SystemMessage(MessageBase):\n    \"\"\"Represents a system message in the chat completion system.\n\n    This model handles system-level instructions and context that guide the\n    conversation behavior.\n\n    Attributes:\n        role (Literal[\"system\"]): Role identifier, always set to \"system\".\n        content (str): The content of the system message.\n    \"\"\"\n    role: Literal[\"system\"] = Field(default=\"system\", description=\"Role is fixed to 'system' for system messages\")\n    content: str = Field(..., description=\"The content of the system message\")\n</code></pre>"},{"location":"reference/data_models/streaming/","title":"Streaming","text":""},{"location":"reference/data_models/streaming/#src.data_models.streaming.PatternMatchResult","title":"<code>src.data_models.streaming.PatternMatchResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of a pattern matching operation in stream processing.</p> <p>This class encapsulates all possible outcomes and data from a pattern matching operation, including matched patterns, processed output, and any errors that occurred.</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>Optional[str]</code> <p>The processed text after pattern matching. None if no processing occurred.</p> <code>pattern_name</code> <code>Optional[str]</code> <p>Name of the matched pattern. None if no pattern was matched.</p> <code>matched</code> <code>bool</code> <p>Whether a pattern was successfully matched. Defaults to False.</p> <code>text_with_tool_call</code> <code>Optional[str]</code> <p>Complete text containing the tool call if a match was found. None otherwise.</p> <code>tool_call_message</code> <code>Optional[str]</code> <p>Message associated with the tool call. None if no tool call was detected.</p> <code>error</code> <code>Optional[str]</code> <p>Error message if pattern matching failed. None if processing was successful.</p> Example <pre><code>result = PatternMatchResult(\n    output=\"Processed text\",\n    pattern_name=\"MistralPattern0\",\n    matched=True,\n    text_with_tool_call=\"Complete tool call text\",\n    tool_call_message=\"Tool call detected\"\n)\n</code></pre> Source code in <code>src/data_models/streaming.py</code> <pre><code>class PatternMatchResult(BaseModel):\n    \"\"\"Result of a pattern matching operation in stream processing.\n\n    This class encapsulates all possible outcomes and data from a pattern\n    matching operation, including matched patterns, processed output,\n    and any errors that occurred.\n\n    Attributes:\n        output (Optional[str]): The processed text after pattern matching.\n            None if no processing occurred.\n        pattern_name (Optional[str]): Name of the matched pattern.\n            None if no pattern was matched.\n        matched (bool): Whether a pattern was successfully matched.\n            Defaults to False.\n        text_with_tool_call (Optional[str]): Complete text containing the tool call\n            if a match was found. None otherwise.\n        tool_call_message (Optional[str]): Message associated with the tool call.\n            None if no tool call was detected.\n        error (Optional[str]): Error message if pattern matching failed.\n            None if processing was successful.\n\n    Example:\n        ```python\n        result = PatternMatchResult(\n            output=\"Processed text\",\n            pattern_name=\"MistralPattern0\",\n            matched=True,\n            text_with_tool_call=\"Complete tool call text\",\n            tool_call_message=\"Tool call detected\"\n        )\n        ```\n    \"\"\"\n    output: Optional[str] = Field(\n        default=None,\n        description=\"The processed output text after pattern matching\"\n    )\n    pattern_name: Optional[str] = Field(\n        default=None,\n        description=\"The name of the matched pattern\"\n    )\n    matched: bool = Field(\n        default=False,\n        description=\"Indicates whether a pattern was successfully matched\"\n    )\n    text_with_tool_call: Optional[str] = Field(\n        default=None,\n        description=\"The complete text containing the tool call if matched\"\n    )\n    tool_call_message: Optional[str] = Field(\n        default=None,\n        description=\"Any message associated with the tool call\"\n    )\n    error: Optional[str] = Field(\n        default=None,\n        description=\"Error message if pattern matching failed\"\n    )\n</code></pre>"},{"location":"reference/data_models/streaming/#src.data_models.streaming.StreamConfig","title":"<code>src.data_models.streaming.StreamConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration settings for stream processing operations.</p> <p>This class defines the configuration parameters that control how streaming data is processed, buffered, and chunked. It uses Pydantic for validation and provides several factory methods for common configurations.</p> <p>Attributes:</p> Name Type Description <code>buffer_size</code> <code>int</code> <p>Size of the buffer in characters. Zero disables buffering. Must be greater than or equal to 0.</p> <code>chunk_separator</code> <code>str</code> <p>String used to separate chunks when combining buffered content.</p> <code>strip_whitespace</code> <code>bool</code> <p>Whether to remove whitespace from chunks before processing.</p> <code>buffering_enabled</code> <code>bool</code> <p>Computed field indicating if buffering is active.</p> Example <pre><code># Create a default configuration\nconfig = StreamConfig.create_default()\n\n# Create a buffered configuration\nbuffered_config = StreamConfig.create_buffered(buffer_size=100)\n\n# Create configuration with custom separator\ncustom_config = StreamConfig.create_with_separator(\n    buffer_size=50,\n    separator=\"\\n\"\n)\n</code></pre> Source code in <code>src/data_models/streaming.py</code> <pre><code>class StreamConfig(BaseModel):\n    \"\"\"Configuration settings for stream processing operations.\n\n    This class defines the configuration parameters that control how streaming\n    data is processed, buffered, and chunked. It uses Pydantic for validation\n    and provides several factory methods for common configurations.\n\n    Attributes:\n        buffer_size (int): Size of the buffer in characters. Zero disables buffering.\n            Must be greater than or equal to 0.\n        chunk_separator (str): String used to separate chunks when combining buffered content.\n        strip_whitespace (bool): Whether to remove whitespace from chunks before processing.\n        buffering_enabled (bool): Computed field indicating if buffering is active.\n\n    Example:\n        ```python\n        # Create a default configuration\n        config = StreamConfig.create_default()\n\n        # Create a buffered configuration\n        buffered_config = StreamConfig.create_buffered(buffer_size=100)\n\n        # Create configuration with custom separator\n        custom_config = StreamConfig.create_with_separator(\n            buffer_size=50,\n            separator=\"\\\\n\"\n        )\n        ```\n    \"\"\"\n    buffer_size: int = Field(\n        default=0,\n        description=\"Size of the buffer in characters. Set to 0 to disable buffering.\",\n        ge=0\n    )\n    chunk_separator: str = Field(\n        default=\"\",\n        description=\"Separator to use between chunks when combining buffered content.\"\n    )\n    strip_whitespace: bool = Field(\n        default=False,\n        description=\"Whether to strip whitespace from chunks before processing.\"\n    )\n\n    @computed_field\n    @property\n    def buffering_enabled(self) -&gt; bool:\n        \"\"\"Indicates whether buffering is enabled based on buffer size.\"\"\"\n        return self.buffer_size &gt; 0\n\n    model_config = {\n        \"frozen\": True,\n        \"extra\": \"forbid\",\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"buffer_size\": 10,\n                    \"chunk_separator\": \"\\n\",\n                    \"strip_whitespace\": True\n                }\n            ]\n        }\n    }\n\n    @classmethod\n    def create_default(cls) -&gt; \"StreamConfig\":\n        \"\"\"Create a StreamConfig with default values.\"\"\"\n        return cls()\n\n    @classmethod\n    def create_buffered(cls, buffer_size: int) -&gt; \"StreamConfig\":\n        \"\"\"Create a StreamConfig with specified buffer size.\"\"\"\n        return cls(buffer_size=buffer_size)\n\n    @classmethod\n    def create_with_separator(\n            cls,\n            buffer_size: int = 0,\n            separator: str = \"\\n\"\n    ) -&gt; \"StreamConfig\":\n        \"\"\"Create a StreamConfig with specified buffer size and separator.\"\"\"\n        return cls(\n            buffer_size=buffer_size,\n            chunk_separator=separator\n        )\n</code></pre>"},{"location":"reference/data_models/streaming/#src.data_models.streaming.StreamConfig.buffering_enabled","title":"<code>buffering_enabled</code>  <code>property</code>","text":"<p>Indicates whether buffering is enabled based on buffer size.</p>"},{"location":"reference/data_models/streaming/#src.data_models.streaming.StreamConfig.create_buffered","title":"<code>create_buffered(buffer_size)</code>  <code>classmethod</code>","text":"<p>Create a StreamConfig with specified buffer size.</p> Source code in <code>src/data_models/streaming.py</code> <pre><code>@classmethod\ndef create_buffered(cls, buffer_size: int) -&gt; \"StreamConfig\":\n    \"\"\"Create a StreamConfig with specified buffer size.\"\"\"\n    return cls(buffer_size=buffer_size)\n</code></pre>"},{"location":"reference/data_models/streaming/#src.data_models.streaming.StreamConfig.create_default","title":"<code>create_default()</code>  <code>classmethod</code>","text":"<p>Create a StreamConfig with default values.</p> Source code in <code>src/data_models/streaming.py</code> <pre><code>@classmethod\ndef create_default(cls) -&gt; \"StreamConfig\":\n    \"\"\"Create a StreamConfig with default values.\"\"\"\n    return cls()\n</code></pre>"},{"location":"reference/data_models/streaming/#src.data_models.streaming.StreamConfig.create_with_separator","title":"<code>create_with_separator(buffer_size=0, separator='\\n')</code>  <code>classmethod</code>","text":"<p>Create a StreamConfig with specified buffer size and separator.</p> Source code in <code>src/data_models/streaming.py</code> <pre><code>@classmethod\ndef create_with_separator(\n        cls,\n        buffer_size: int = 0,\n        separator: str = \"\\n\"\n) -&gt; \"StreamConfig\":\n    \"\"\"Create a StreamConfig with specified buffer size and separator.\"\"\"\n    return cls(\n        buffer_size=buffer_size,\n        chunk_separator=separator\n    )\n</code></pre>"},{"location":"reference/data_models/tools/","title":"Tools","text":""},{"location":"reference/data_models/tools/#src.data_models.tools.FunctionParameters","title":"<code>src.data_models.tools.FunctionParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for function parameters following JSON Schema specification.</p> <p>This class defines the structure for function parameters using JSON Schema. It specifies the type, properties, required fields, and whether additional properties are allowed.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of the parameters object, always \"object\".</p> <code>properties</code> <code>Dict[str, Dict[str, Any]]</code> <p>Mapping of parameter names to their JSON Schema definitions.</p> <code>required</code> <code>Optional[List[str]]</code> <p>List of required parameter names.</p> <code>additionalProperties</code> <code>Optional[bool]</code> <p>Whether additional properties beyond those specified are allowed.</p> Example <pre><code>parameters = FunctionParameters(\n    type=\"object\",\n    properties={\n        \"location\": {\n            \"type\": \"string\",\n            \"description\": \"City name or coordinates\"\n        }\n    },\n    required=[\"location\"]\n)\n</code></pre> Source code in <code>src/data_models/tools.py</code> <pre><code>class FunctionParameters(BaseModel):\n    \"\"\"Schema for function parameters following JSON Schema specification.\n\n    This class defines the structure for function parameters using JSON Schema.\n    It specifies the type, properties, required fields, and whether additional\n    properties are allowed.\n\n    Attributes:\n        type (str): The type of the parameters object, always \"object\".\n        properties (Dict[str, Dict[str, Any]]): Mapping of parameter names to their\n            JSON Schema definitions.\n        required (Optional[List[str]]): List of required parameter names.\n        additionalProperties (Optional[bool]): Whether additional properties beyond\n            those specified are allowed.\n\n    Example:\n        ```python\n        parameters = FunctionParameters(\n            type=\"object\",\n            properties={\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City name or coordinates\"\n                }\n            },\n            required=[\"location\"]\n        )\n        ```\n    \"\"\"\n    type: str = \"object\"\n    properties: Dict[str, Dict[str, Any]]\n    required: Optional[List[str]] = None\n    additionalProperties: Optional[bool] = None\n</code></pre>"},{"location":"reference/data_models/tools/#src.data_models.tools.Function","title":"<code>src.data_models.tools.Function</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a function that can be called by the model.</p> <p>Defines the structure of a callable function, including its name, description, parameters, and validation settings.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Function identifier, must be 1-64 characters and contain only alphanumeric characters, underscores, and hyphens.</p> <code>description</code> <code>Optional[str]</code> <p>Human-readable description of what the function does.</p> <code>parameters</code> <code>Optional[FunctionParameters]</code> <p>Schema defining the function's parameters.</p> <code>strict</code> <code>Optional[bool]</code> <p>Whether to enforce strict parameter validation. Defaults to False.</p> Example <pre><code>function = Function(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    parameters=FunctionParameters(...),\n    strict=True\n)\n</code></pre> Source code in <code>src/data_models/tools.py</code> <pre><code>class Function(BaseModel):\n    \"\"\"Represents a function that can be called by the model.\n\n    Defines the structure of a callable function, including its name,\n    description, parameters, and validation settings.\n\n    Attributes:\n        name (str): Function identifier, must be 1-64 characters and contain only\n            alphanumeric characters, underscores, and hyphens.\n        description (Optional[str]): Human-readable description of what the\n            function does.\n        parameters (Optional[FunctionParameters]): Schema defining the function's\n            parameters.\n        strict (Optional[bool]): Whether to enforce strict parameter validation.\n            Defaults to False.\n\n    Example:\n        ```python\n        function = Function(\n            name=\"get_weather\",\n            description=\"Get current weather for a location\",\n            parameters=FunctionParameters(...),\n            strict=True\n        )\n        ```\n    \"\"\"\n    name: str = Field(..., max_length=64, pattern=\"^[a-zA-Z0-9_-]+$\")\n    description: Optional[str] = None\n    parameters: Optional[FunctionParameters] = None\n    strict: Optional[bool] = Field(default=False)\n</code></pre>"},{"location":"reference/data_models/tools/#src.data_models.tools.Tool","title":"<code>src.data_models.tools.Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool that the model can use.</p> <p>A tool is a wrapper around a function that can be called by the model. Currently, only function-type tools are supported.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['function']</code> <p>The type of tool, currently only \"function\" is supported.</p> <code>function</code> <code>Function</code> <p>The function definition for this tool.</p> Example <pre><code>tool = Tool(\n    type=\"function\",\n    function=Function(\n        name=\"get_weather\",\n        description=\"Get current weather\",\n        parameters=FunctionParameters(...),\n        strict=True\n    )\n)\n</code></pre> Source code in <code>src/data_models/tools.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"Represents a tool that the model can use.\n\n    A tool is a wrapper around a function that can be called by the model.\n    Currently, only function-type tools are supported.\n\n    Attributes:\n        type (Literal[\"function\"]): The type of tool, currently only \"function\"\n            is supported.\n        function (Function): The function definition for this tool.\n\n    Example:\n        ```python\n        tool = Tool(\n            type=\"function\",\n            function=Function(\n                name=\"get_weather\",\n                description=\"Get current weather\",\n                parameters=FunctionParameters(...),\n                strict=True\n            )\n        )\n        ```\n    \"\"\"\n    type: Literal[\"function\"] = \"function\"\n    function: Function\n</code></pre>"},{"location":"reference/data_models/tools/#src.data_models.tools.ToolsList","title":"<code>src.data_models.tools.ToolsList</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for a list of tools.</p> <p>Manages a collection of tools that can be provided to the model, with a maximum limit of 128 tools.</p> <p>Attributes:</p> Name Type Description <code>tools</code> <code>List[Tool]</code> <p>List of tool definitions, maximum length of 128.</p> Source code in <code>src/data_models/tools.py</code> <pre><code>class ToolsList(BaseModel):\n    \"\"\"Container for a list of tools.\n\n    Manages a collection of tools that can be provided to the model,\n    with a maximum limit of 128 tools.\n\n    Attributes:\n        tools (List[Tool]): List of tool definitions, maximum length of 128.\n    \"\"\"\n    tools: List[Tool] = Field(..., max_length=128)\n</code></pre>"},{"location":"reference/data_models/wx_assistant/","title":"Wx Assistant","text":""},{"location":"reference/data_models/wx_assistant/#src.data_models.wx_assistant.WxAssistantMessage","title":"<code>src.data_models.wx_assistant.WxAssistantMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>This is a class for WxAssistant messages.</p> <p>Attributes:</p> Name Type Description <code>u</code> <code>Optional[str]</code> <p>Represents the user input.</p> <code>a</code> <code>Optional[str]</code> <p>Represents the assistant response.</p> <code>n</code> <code>Optional[bool]</code> <p>An optional boolean for additional context.</p> Source code in <code>src/data_models/wx_assistant.py</code> <pre><code>class WxAssistantMessage(BaseModel):\n    \"\"\"\n    This is a class for WxAssistant messages.\n\n    Attributes:\n        u (Optional[str]): Represents the user input.\n        a (Optional[str]): Represents the assistant response.\n        n (Optional[bool]): An optional boolean for additional context.\n    \"\"\"\n    u: Optional[str] = None\n    a: Optional[str] = None\n    n: Optional[bool] = None\n\n    def to_dict(self):\n        return {\"u\": self.u, \"a\": self.a, \"n\": self.n}\n</code></pre>"},{"location":"reference/data_models/wx_assistant/#src.data_models.wx_assistant.WxAssistantConversationInput","title":"<code>src.data_models.wx_assistant.WxAssistantConversationInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>This is a class for WxAssistant conversation input.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[WxAssistantMessage]</code> <p>A list of WxAssistantMessage instances.</p> <code>context</code> <code>Optional[ContextModel]</code> <p>Optional metadata for additional context.</p> Source code in <code>src/data_models/wx_assistant.py</code> <pre><code>class WxAssistantConversationInput(BaseModel):\n    \"\"\"\n    This is a class for WxAssistant conversation input.\n\n    Attributes:\n        messages (List[WxAssistantMessage]): A list of WxAssistantMessage instances.\n        context (Optional[ContextModel]): Optional metadata for additional context.\n    \"\"\"\n    messages: List[WxAssistantMessage]\n    context: Optional[ContextModel] = None\n</code></pre>"},{"location":"reference/database/","title":"Database Components","text":""},{"location":"reference/database/#overview","title":"Overview","text":"<p>The database package provides a flexible and extensible database integration layer supporting both vector and document databases. It implements an adapter pattern through the <code>DatabaseAdapter</code> interface, allowing seamless integration of different database backends while maintaining a consistent API.</p>"},{"location":"reference/database/#architecture","title":"Architecture","text":"<p>The package consists of four main components:</p> <pre><code>graph TD\n    A[DatabaseAdapter] --&gt; B[MilvusClient]\n    A --&gt; C[ElasticsearchClient]\n    D[ElasticQueryBuilder] --&gt; C\n    E[Client Code] --&gt; A</code></pre>"},{"location":"reference/database/#components","title":"Components","text":""},{"location":"reference/database/#base-adapter","title":"Base Adapter","text":"<p>The <code>DatabaseAdapter</code> abstract base class defines the core interface that all database implementations must follow. It ensures consistency across different database backends by requiring implementation of three key methods:</p> <ul> <li><code>add()</code>: Add data to the database</li> <li><code>search()</code>: Search for data in the database</li> <li><code>reset()</code>: Reset or clear the database</li> </ul>"},{"location":"reference/database/#vector-database-milvus","title":"Vector Database (Milvus)","text":"<p>The <code>MilvusClient</code> implements vector database operations using Milvus. It provides:</p> <ul> <li>Vector similarity search</li> <li>Collection management</li> <li>Index creation and optimization</li> <li>Metadata handling</li> <li>Configurable schema definition</li> </ul>"},{"location":"reference/database/#document-database-elasticsearch","title":"Document Database (Elasticsearch)","text":"<p>The <code>ElasticsearchClient</code> provides asynchronous document database operations using Elasticsearch. Features include:</p> <ul> <li>Asynchronous document indexing and search</li> <li>Index management</li> <li>Configurable mappings and settings</li> <li>Environment-based configuration</li> </ul>"},{"location":"reference/database/#query-building","title":"Query Building","text":"<p>The <code>ElasticQueryBuilder</code> helps construct structured queries for Elasticsearch, offering:</p> <ul> <li>Template-based query construction</li> <li>Safe user input handling</li> <li>Configurable query parameters</li> <li>Error handling and validation</li> </ul>"},{"location":"reference/database/#configuration","title":"Configuration","text":""},{"location":"reference/database/#milvus-configuration","title":"Milvus Configuration","text":"<pre><code>host: localhost\nport: 19530\nuser: username   # optional\npassword: pass   # optional\n</code></pre>"},{"location":"reference/database/#elasticsearch-configuration","title":"Elasticsearch Configuration","text":"<p>Required environment variables:</p> <ul> <li><code>ES_ENDPOINT</code>: Elasticsearch server endpoint</li> <li><code>ES_API_KEY</code>: API key for authentication</li> </ul>"},{"location":"reference/database/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/database/#using-milvusclient","title":"Using MilvusClient","text":"<pre><code># Initialize Milvus client\nclient = MilvusClient(\n    config_file=\"config.yaml\",\n    collection_name=\"vectors\",\n    vector_dim=768,\n    additional_fields=[\n        {\"name\": \"text\", \"dtype\": DataType.VARCHAR, \"max_length\": 65535}\n    ]\n)\n\n# Add vector with metadata\nvector = [0.1, 0.2, ..., 0.768]\nmetadata = {\"text\": \"document content\"}\nclient.add(vector, metadata)\n\n# Search vectors\nresults = client.search(\n    vector=query_vector,\n    top_k=5,\n    output_fields=[\"text\"]\n)\n</code></pre>"},{"location":"reference/database/#using-elasticsearchclient","title":"Using ElasticsearchClient","text":"<pre><code># Initialize Elasticsearch client\nclient = ElasticsearchClient()\n\n# Create index with mappings\nmappings = {\n    \"properties\": {\n        \"title\": {\"type\": \"text\"},\n        \"content\": {\"type\": \"text\"}\n    }\n}\nawait client.create_index(\"documents\", mappings=mappings)\n\n# Add document\ndoc = {\"title\": \"Example\", \"content\": \"Content\"}\nawait client.add(doc, \"documents\")\n\n# Search documents\nquery = {\"query\": {\"match\": {\"content\": \"search text\"}}}\nresults = await client.search(query, \"documents\")\n</code></pre>"},{"location":"reference/database/#see-also","title":"See Also","text":"<ul> <li>MilvusClient Reference</li> <li>ElasticsearchClient Reference</li> </ul>"},{"location":"reference/database/base_adapter/","title":"Base Adapter","text":""},{"location":"reference/database/base_adapter/#src.database.base_adapter.DatabaseAdapter","title":"<code>src.database.base_adapter.DatabaseAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for database operations.</p> <p>This class serves as a template for implementing database adapters, ensuring consistent interface across different database backends. All database implementations must inherit from this class and implement its abstract methods.</p> <p>Methods:</p> Name Description <code>add</code> <p>Add data to the database</p> <code>search</code> <p>Search for data in the database</p> <code>reset</code> <p>Reset or clear the database</p> Example <pre><code>class MyDatabaseClient(DatabaseAdapter):\n    async def add(self, document, index_name):\n        # Implementation for adding documents\n        pass\n\n    async def search(self, query, index_name):\n        # Implementation for searching documents\n        pass\n\n    async def reset(self, index_name):\n        # Implementation for resetting the database\n        pass\n</code></pre> Source code in <code>src/database/base_adapter.py</code> <pre><code>class DatabaseAdapter(ABC):\n    \"\"\"Abstract base class defining the interface for database operations.\n\n    This class serves as a template for implementing database adapters,\n    ensuring consistent interface across different database backends.\n    All database implementations must inherit from this class and\n    implement its abstract methods.\n\n    Methods:\n        add: Add data to the database\n        search: Search for data in the database\n        reset: Reset or clear the database\n\n    Example:\n        ```python\n        class MyDatabaseClient(DatabaseAdapter):\n            async def add(self, document, index_name):\n                # Implementation for adding documents\n                pass\n\n            async def search(self, query, index_name):\n                # Implementation for searching documents\n                pass\n\n            async def reset(self, index_name):\n                # Implementation for resetting the database\n                pass\n        ```\n    \"\"\"\n\n    @abstractmethod\n    async def add(self, *args, **kwargs):\n        \"\"\"Add data to the database.\n\n        This abstract method must be implemented by concrete database adapters\n        to handle data insertion operations.\n\n        Args:\n            *args: Variable length argument list for flexibility across implementations\n            **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n        Raises:\n            NotImplementedError: If the concrete class doesn't implement this method\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def search(self, *args, **kwargs):\n        \"\"\"Search for data in the database.\n\n        This abstract method must be implemented by concrete database adapters\n        to handle search operations.\n\n        Args:\n            *args: Variable length argument list for flexibility across implementations\n            **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n        Raises:\n            NotImplementedError: If the concrete class doesn't implement this method\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def reset(self, *args, **kwargs):\n        \"\"\"Reset or clear data in the database.\n\n        This abstract method must be implemented by concrete database adapters\n        to handle database reset operations.\n\n        Args:\n            *args: Variable length argument list for flexibility across implementations\n            **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n        Raises:\n            NotImplementedError: If the concrete class doesn't implement this method\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/database/base_adapter/#src.database.base_adapter.DatabaseAdapter.add","title":"<code>add(*args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Add data to the database.</p> <p>This abstract method must be implemented by concrete database adapters to handle data insertion operations.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list for flexibility across implementations</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for flexibility across implementations</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the concrete class doesn't implement this method</p> Source code in <code>src/database/base_adapter.py</code> <pre><code>@abstractmethod\nasync def add(self, *args, **kwargs):\n    \"\"\"Add data to the database.\n\n    This abstract method must be implemented by concrete database adapters\n    to handle data insertion operations.\n\n    Args:\n        *args: Variable length argument list for flexibility across implementations\n        **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n    Raises:\n        NotImplementedError: If the concrete class doesn't implement this method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/database/base_adapter/#src.database.base_adapter.DatabaseAdapter.reset","title":"<code>reset(*args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Reset or clear data in the database.</p> <p>This abstract method must be implemented by concrete database adapters to handle database reset operations.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list for flexibility across implementations</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for flexibility across implementations</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the concrete class doesn't implement this method</p> Source code in <code>src/database/base_adapter.py</code> <pre><code>@abstractmethod\nasync def reset(self, *args, **kwargs):\n    \"\"\"Reset or clear data in the database.\n\n    This abstract method must be implemented by concrete database adapters\n    to handle database reset operations.\n\n    Args:\n        *args: Variable length argument list for flexibility across implementations\n        **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n    Raises:\n        NotImplementedError: If the concrete class doesn't implement this method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/database/base_adapter/#src.database.base_adapter.DatabaseAdapter.search","title":"<code>search(*args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Search for data in the database.</p> <p>This abstract method must be implemented by concrete database adapters to handle search operations.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list for flexibility across implementations</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for flexibility across implementations</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the concrete class doesn't implement this method</p> Source code in <code>src/database/base_adapter.py</code> <pre><code>@abstractmethod\nasync def search(self, *args, **kwargs):\n    \"\"\"Search for data in the database.\n\n    This abstract method must be implemented by concrete database adapters\n    to handle search operations.\n\n    Args:\n        *args: Variable length argument list for flexibility across implementations\n        **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n    Raises:\n        NotImplementedError: If the concrete class doesn't implement this method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/database/elastic_client/","title":"Elasticsearch Client","text":""},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient","title":"<code>src.database.elastic_client.ElasticsearchClient</code>","text":"<p>               Bases: <code>DatabaseAdapter</code></p> <p>Asynchronous Elasticsearch client adapter for database operations.</p> <p>This class implements the DatabaseAdapter interface to provide asynchronous interaction with Elasticsearch. It handles document indexing, searching, index management, and other essential Elasticsearch operations.</p> <p>The client requires Elasticsearch endpoint and API key to be set in environment variables ES_ENDPOINT and ES_API_KEY respectively.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>AsyncElasticsearch</code> <p>Async Elasticsearch client instance configured with the provided endpoint and API key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If either ES_ENDPOINT or ES_API_KEY environment variables are not set.</p> Example <pre><code># Initialize the client\nes_client = ElasticsearchClient()\n\n# Add a document\ndocument = {\"title\": \"Example\", \"content\": \"Sample text\"}\nawait es_client.add(document, \"my_index\")\n\n# Search for documents\nquery = {\"query\": {\"match\": {\"content\": \"sample\"}}}\nresults = await es_client.search(query, \"my_index\")\n</code></pre> Source code in <code>src/database/elastic_client.py</code> <pre><code>class ElasticsearchClient(DatabaseAdapter):\n    \"\"\"Asynchronous Elasticsearch client adapter for database operations.\n\n    This class implements the DatabaseAdapter interface to provide asynchronous\n    interaction with Elasticsearch. It handles document indexing, searching,\n    index management, and other essential Elasticsearch operations.\n\n    The client requires Elasticsearch endpoint and API key to be set in environment\n    variables ES_ENDPOINT and ES_API_KEY respectively.\n\n    Attributes:\n        client (AsyncElasticsearch): Async Elasticsearch client instance configured\n            with the provided endpoint and API key.\n\n    Raises:\n        ValueError: If either ES_ENDPOINT or ES_API_KEY environment variables are not set.\n\n    Example:\n        ```python\n        # Initialize the client\n        es_client = ElasticsearchClient()\n\n        # Add a document\n        document = {\"title\": \"Example\", \"content\": \"Sample text\"}\n        await es_client.add(document, \"my_index\")\n\n        # Search for documents\n        query = {\"query\": {\"match\": {\"content\": \"sample\"}}}\n        results = await es_client.search(query, \"my_index\")\n        ```\n    \"\"\"\n    def __init__(self, verify_certs: bool = True):\n        \"\"\"Initialize the Elasticsearch client with credentials from environment variables.\n\n        Establishes connection to Elasticsearch using endpoint and API key from\n        environment variables. Sets up the async client with specific configurations\n        for SSL verification and request timeout.\n\n        Raises:\n            ValueError: If required environment variables are not set.\n        \"\"\"\n        es_endpoint = os.getenv(\"ES_ENDPOINT\")\n        es_api_key = os.getenv(\"ES_API_KEY\")\n\n        if not es_endpoint or not es_api_key:\n            raise ValueError(\"Elasticsearch endpoint and API key must be provided.\")\n\n        self.client = AsyncElasticsearch(\n            es_endpoint,\n            api_key=es_api_key,\n            verify_certs=verify_certs,\n            request_timeout=60,\n        )\n\n    async def add(self, document, index_name):\n        \"\"\"Add a document to the specified Elasticsearch index.\n\n        Args:\n            document: Dictionary containing the document data to be indexed.\n            index_name (str): Name of the index to add the document to.\n\n        Returns:\n            dict: Elasticsearch response containing the indexing result.\n\n        Example:\n            ```python\n            response = await client.add(\n                {\"title\": \"Test\", \"content\": \"Content\"},\n                \"my_index\"\n            )\n            ```\n        \"\"\"\n        response = await self.client.index(index=index_name, body=document)\n        return response\n\n    async def search(self, query_body: dict, index_name: str, size: int = 5):\n        \"\"\"Search for documents in the specified index.\n\n        Args:\n            query_body (dict): Elasticsearch query DSL dictionary.\n            index_name (str): Name of the index to search in.\n            size (int, optional): Maximum number of results to return. Defaults to 5.\n\n        Returns:\n            dict: Elasticsearch response containing search results.\n\n        Example:\n            ```python\n            query = {\n                \"query\": {\n                    \"match\": {\n                        \"content\": \"search text\"\n                    }\n                }\n            }\n            results = await client.search(query, \"my_index\", size=10)\n            ```\n        \"\"\"\n        response = await self.client.search(\n            index=index_name,\n            body=query_body,\n            size=size,\n        )\n        return response\n\n    async def reset(self, index_name):\n        \"\"\"Delete all documents from the specified index.\n\n        Args:\n            index_name (str): Name of the index to reset.\n\n        Example:\n            ```python\n            await client.reset(\"my_index\")\n            ```\n        \"\"\"\n        await self.client.delete_by_query(\n            index=index_name,\n            body={\"query\": {\"match_all\": {}}},\n        )\n\n    async def create_index(self, index_name, settings=None, mappings=None):\n        \"\"\"Create a new Elasticsearch index if it doesn't exist.\n\n        Args:\n            index_name (str): Name of the index to create.\n            settings (dict, optional): Index settings configuration. Defaults to empty dict.\n            mappings (dict, optional): Index mappings configuration. Defaults to empty dict.\n\n        Example:\n            ```python\n            settings = {\"number_of_shards\": 1}\n            mappings = {\n                \"properties\": {\n                    \"title\": {\"type\": \"text\"},\n                    \"content\": {\"type\": \"text\"}\n                }\n            }\n            await client.create_index(\"my_index\", settings, mappings)\n            ```\n        \"\"\"\n        if not await self.client.indices.exists(index=index_name):\n            await self.client.indices.create(index=index_name, body={\n                \"settings\": settings if settings else {},\n                \"mappings\": mappings if mappings else {}\n            })\n\n    async def index_exists(self, index_name: str) -&gt; bool:\n        \"\"\"Check if an index exists.\n\n        Args:\n            index_name (str): Name of the index to check.\n\n        Returns:\n            bool: True if index exists, False otherwise.\n\n        Example:\n            ```python\n            exists = await client.index_exists(\"my_index\")\n            if not exists:\n                await client.create_index(\"my_index\")\n            ```\n        \"\"\"\n        return await self.client.indices.exists(index=index_name)\n</code></pre>"},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient.__init__","title":"<code>__init__(verify_certs=True)</code>","text":"<p>Initialize the Elasticsearch client with credentials from environment variables.</p> <p>Establishes connection to Elasticsearch using endpoint and API key from environment variables. Sets up the async client with specific configurations for SSL verification and request timeout.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required environment variables are not set.</p> Source code in <code>src/database/elastic_client.py</code> <pre><code>def __init__(self, verify_certs: bool = True):\n    \"\"\"Initialize the Elasticsearch client with credentials from environment variables.\n\n    Establishes connection to Elasticsearch using endpoint and API key from\n    environment variables. Sets up the async client with specific configurations\n    for SSL verification and request timeout.\n\n    Raises:\n        ValueError: If required environment variables are not set.\n    \"\"\"\n    es_endpoint = os.getenv(\"ES_ENDPOINT\")\n    es_api_key = os.getenv(\"ES_API_KEY\")\n\n    if not es_endpoint or not es_api_key:\n        raise ValueError(\"Elasticsearch endpoint and API key must be provided.\")\n\n    self.client = AsyncElasticsearch(\n        es_endpoint,\n        api_key=es_api_key,\n        verify_certs=verify_certs,\n        request_timeout=60,\n    )\n</code></pre>"},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient.add","title":"<code>add(document, index_name)</code>  <code>async</code>","text":"<p>Add a document to the specified Elasticsearch index.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <p>Dictionary containing the document data to be indexed.</p> required <code>index_name</code> <code>str</code> <p>Name of the index to add the document to.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Elasticsearch response containing the indexing result.</p> Example <pre><code>response = await client.add(\n    {\"title\": \"Test\", \"content\": \"Content\"},\n    \"my_index\"\n)\n</code></pre> Source code in <code>src/database/elastic_client.py</code> <pre><code>async def add(self, document, index_name):\n    \"\"\"Add a document to the specified Elasticsearch index.\n\n    Args:\n        document: Dictionary containing the document data to be indexed.\n        index_name (str): Name of the index to add the document to.\n\n    Returns:\n        dict: Elasticsearch response containing the indexing result.\n\n    Example:\n        ```python\n        response = await client.add(\n            {\"title\": \"Test\", \"content\": \"Content\"},\n            \"my_index\"\n        )\n        ```\n    \"\"\"\n    response = await self.client.index(index=index_name, body=document)\n    return response\n</code></pre>"},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient.create_index","title":"<code>create_index(index_name, settings=None, mappings=None)</code>  <code>async</code>","text":"<p>Create a new Elasticsearch index if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to create.</p> required <code>settings</code> <code>dict</code> <p>Index settings configuration. Defaults to empty dict.</p> <code>None</code> <code>mappings</code> <code>dict</code> <p>Index mappings configuration. Defaults to empty dict.</p> <code>None</code> Example <pre><code>settings = {\"number_of_shards\": 1}\nmappings = {\n    \"properties\": {\n        \"title\": {\"type\": \"text\"},\n        \"content\": {\"type\": \"text\"}\n    }\n}\nawait client.create_index(\"my_index\", settings, mappings)\n</code></pre> Source code in <code>src/database/elastic_client.py</code> <pre><code>async def create_index(self, index_name, settings=None, mappings=None):\n    \"\"\"Create a new Elasticsearch index if it doesn't exist.\n\n    Args:\n        index_name (str): Name of the index to create.\n        settings (dict, optional): Index settings configuration. Defaults to empty dict.\n        mappings (dict, optional): Index mappings configuration. Defaults to empty dict.\n\n    Example:\n        ```python\n        settings = {\"number_of_shards\": 1}\n        mappings = {\n            \"properties\": {\n                \"title\": {\"type\": \"text\"},\n                \"content\": {\"type\": \"text\"}\n            }\n        }\n        await client.create_index(\"my_index\", settings, mappings)\n        ```\n    \"\"\"\n    if not await self.client.indices.exists(index=index_name):\n        await self.client.indices.create(index=index_name, body={\n            \"settings\": settings if settings else {},\n            \"mappings\": mappings if mappings else {}\n        })\n</code></pre>"},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient.index_exists","title":"<code>index_exists(index_name)</code>  <code>async</code>","text":"<p>Check if an index exists.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if index exists, False otherwise.</p> Example <pre><code>exists = await client.index_exists(\"my_index\")\nif not exists:\n    await client.create_index(\"my_index\")\n</code></pre> Source code in <code>src/database/elastic_client.py</code> <pre><code>async def index_exists(self, index_name: str) -&gt; bool:\n    \"\"\"Check if an index exists.\n\n    Args:\n        index_name (str): Name of the index to check.\n\n    Returns:\n        bool: True if index exists, False otherwise.\n\n    Example:\n        ```python\n        exists = await client.index_exists(\"my_index\")\n        if not exists:\n            await client.create_index(\"my_index\")\n        ```\n    \"\"\"\n    return await self.client.indices.exists(index=index_name)\n</code></pre>"},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient.reset","title":"<code>reset(index_name)</code>  <code>async</code>","text":"<p>Delete all documents from the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to reset.</p> required Example <pre><code>await client.reset(\"my_index\")\n</code></pre> Source code in <code>src/database/elastic_client.py</code> <pre><code>async def reset(self, index_name):\n    \"\"\"Delete all documents from the specified index.\n\n    Args:\n        index_name (str): Name of the index to reset.\n\n    Example:\n        ```python\n        await client.reset(\"my_index\")\n        ```\n    \"\"\"\n    await self.client.delete_by_query(\n        index=index_name,\n        body={\"query\": {\"match_all\": {}}},\n    )\n</code></pre>"},{"location":"reference/database/elastic_client/#src.database.elastic_client.ElasticsearchClient.search","title":"<code>search(query_body, index_name, size=5)</code>  <code>async</code>","text":"<p>Search for documents in the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>query_body</code> <code>dict</code> <p>Elasticsearch query DSL dictionary.</p> required <code>index_name</code> <code>str</code> <p>Name of the index to search in.</p> required <code>size</code> <code>int</code> <p>Maximum number of results to return. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Elasticsearch response containing search results.</p> Example <pre><code>query = {\n    \"query\": {\n        \"match\": {\n            \"content\": \"search text\"\n        }\n    }\n}\nresults = await client.search(query, \"my_index\", size=10)\n</code></pre> Source code in <code>src/database/elastic_client.py</code> <pre><code>async def search(self, query_body: dict, index_name: str, size: int = 5):\n    \"\"\"Search for documents in the specified index.\n\n    Args:\n        query_body (dict): Elasticsearch query DSL dictionary.\n        index_name (str): Name of the index to search in.\n        size (int, optional): Maximum number of results to return. Defaults to 5.\n\n    Returns:\n        dict: Elasticsearch response containing search results.\n\n    Example:\n        ```python\n        query = {\n            \"query\": {\n                \"match\": {\n                    \"content\": \"search text\"\n                }\n            }\n        }\n        results = await client.search(query, \"my_index\", size=10)\n        ```\n    \"\"\"\n    response = await self.client.search(\n        index=index_name,\n        body=query_body,\n        size=size,\n    )\n    return response\n</code></pre>"},{"location":"reference/database/milvus_client/","title":"Milvus Client","text":""},{"location":"reference/database/milvus_client/#src.database.milvus_client.MilvusClient","title":"<code>src.database.milvus_client.MilvusClient</code>","text":"<p>               Bases: <code>DatabaseAdapter</code></p> Source code in <code>src/database/milvus_client.py</code> <pre><code>class MilvusClient(DatabaseAdapter):\n    def __init__(\n            self,\n            config_file,\n            collection_name,\n            vector_dim,\n            additional_fields=None,\n            index_params=None,\n    ):\n        \"\"\"Initializes a Milvus client for vector database operations.\n\n        Args:\n            config_file (str): Path to the YAML configuration file.\n            collection_name (str): Name of the Milvus collection to be used or created.\n            vector_dim (int): Dimension of the vectors stored in the collection.\n            additional_fields (list, optional): Additional fields for the collection schema.\n        \"\"\"\n        self._load_config(config_file)\n        self.collection_name = collection_name\n        self.vector_dim = vector_dim\n        self.additional_fields = additional_fields if additional_fields else []\n        self.index_params = (\n            index_params\n            if index_params\n            else {\n                \"index_type\": \"IVF_FLAT\",\n                \"metric_type\": \"L2\",\n                \"params\": {\"nlist\": 1024},\n            }\n        )\n        self._connect()\n        self._create_collection()\n\n    def add(self, vector, metadata, check_dup=False):\n        \"\"\"Adds a vector and its associated metadata to the collection.\n\n        Args:\n            vector (list[float]): The vector to be added.\n            metadata (dict): Metadata associated with the vector.\n            check_dup (bool): Flag to check for duplicates before insertion (currently not implemented).\n        \"\"\"\n        data = [vector] + [metadata[field] for field in metadata]\n        self.collection.insert(data)\n        self.collection.flush()  # Ensures data persistence\n\n    def search(\n            self,\n            vector,\n            top_k,\n            distance_range=None,\n            search_params=None,\n            output_fields=None,\n            filter_expr=None,\n    ):\n        \"\"\"Performs a vector search in the collection.\n\n        Args:\n            vector (list[float]): The query vector.\n            top_k (int): Number of top results to return.\n            distance_range (list[int, int], optional): Minimum and maximum distances for filtering results.\n            search_params (dict, optional): Parameters for the search.\n            output_fields (list, optional): Fields to include in the returned results.\n            filter_expr (str, optional): Filter expression for conditional search.\n\n        Returns:\n            SearchResult: Search results from Milvus.\n        \"\"\"\n        self.collection.load()\n        if search_params is None:\n            search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n        if distance_range:\n            search_params[\"params\"].update(\n                {\"range_filter\": distance_range[0], \"radius\": distance_range[1]}\n            )\n\n        return self.collection.search(\n            data=[vector],\n            anns_field=\"vector\",\n            param=search_params,\n            limit=top_k,\n            expr=filter_expr,\n            output_fields=output_fields,\n        )\n\n    def reset(self):\n        \"\"\"Drops the current collection and creates a new one.\"\"\"\n        self.reset_collection()\n\n    def reset_collection(self):\n        \"\"\"Drops the current collection and creates a new one.\"\"\"\n        if self.collection_name in list_collections():\n            drop_collection(self.collection_name)\n        self._create_collection()\n\n    def _load_config(self, config_file):\n        \"\"\"Loads configuration settings from a YAML file.\"\"\"\n        with open(config_file, \"r\") as file:\n            self.config = yaml.safe_load(file)\n\n    def _connect(self, secure: bool = True):\n        \"\"\"Connects to the Milvus server using loaded configuration.\"\"\"\n        # Connect using only the non-None configuration parameters\n        connections.connect(**{k: v for k, v in self.config.items() if v is not None}, )\n\n    def _create_collection(\n            self, **kwargs\n    ):  # add ability to enable enable_dynamic_field and other Collection params\n        \"\"\"Creates a new collection in Milvus or loads an existing one.\"\"\"\n        if self.collection_name not in list_collections():\n            fields = [\n                         FieldSchema(\n                             name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True\n                         ),\n                         FieldSchema(\n                             name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=self.vector_dim\n                         ),\n                     ] + [\n                         FieldSchema(\n                             name=field[\"name\"],\n                             dtype=field[\"dtype\"],\n                             max_length=field.get(\"max_length\"),\n                         )\n                         for field in self.additional_fields\n                     ]\n            schema = CollectionSchema(\n                fields, \"Vector collection with additional metadata\"\n            )\n            self.collection = Collection(name=self.collection_name, schema=schema)\n            self._create_index()\n        else:\n            self.collection = Collection(name=self.collection_name)\n\n    def _print_collection_schema(self):\n        \"\"\"Prints the schema of the current collection.\"\"\"\n        if self.collection_name in list_collections():\n            collection = Collection(name=self.collection_name)\n            print(f\"Schema for collection '{self.collection_name}':\")\n            for field in collection.schema.fields:\n                print(\n                    f\"Field Name: {field.name}, Data Type: {field.dtype}, Description: {field.description}\"\n                )\n        else:\n            print(f\"Collection '{self.collection_name}' does not exist.\")\n\n    def _create_index(self, index_params=None):\n        \"\"\"Creates an index for efficient search in the collection.\"\"\"\n        self.collection.create_index(\n            field_name=\"vector\", index_params=self.index_params\n        )\n</code></pre>"},{"location":"reference/database/milvus_client/#src.database.milvus_client.MilvusClient.__init__","title":"<code>__init__(config_file, collection_name, vector_dim, additional_fields=None, index_params=None)</code>","text":"<p>Initializes a Milvus client for vector database operations.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <code>collection_name</code> <code>str</code> <p>Name of the Milvus collection to be used or created.</p> required <code>vector_dim</code> <code>int</code> <p>Dimension of the vectors stored in the collection.</p> required <code>additional_fields</code> <code>list</code> <p>Additional fields for the collection schema.</p> <code>None</code> Source code in <code>src/database/milvus_client.py</code> <pre><code>def __init__(\n        self,\n        config_file,\n        collection_name,\n        vector_dim,\n        additional_fields=None,\n        index_params=None,\n):\n    \"\"\"Initializes a Milvus client for vector database operations.\n\n    Args:\n        config_file (str): Path to the YAML configuration file.\n        collection_name (str): Name of the Milvus collection to be used or created.\n        vector_dim (int): Dimension of the vectors stored in the collection.\n        additional_fields (list, optional): Additional fields for the collection schema.\n    \"\"\"\n    self._load_config(config_file)\n    self.collection_name = collection_name\n    self.vector_dim = vector_dim\n    self.additional_fields = additional_fields if additional_fields else []\n    self.index_params = (\n        index_params\n        if index_params\n        else {\n            \"index_type\": \"IVF_FLAT\",\n            \"metric_type\": \"L2\",\n            \"params\": {\"nlist\": 1024},\n        }\n    )\n    self._connect()\n    self._create_collection()\n</code></pre>"},{"location":"reference/database/milvus_client/#src.database.milvus_client.MilvusClient.add","title":"<code>add(vector, metadata, check_dup=False)</code>","text":"<p>Adds a vector and its associated metadata to the collection.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>list[float]</code> <p>The vector to be added.</p> required <code>metadata</code> <code>dict</code> <p>Metadata associated with the vector.</p> required <code>check_dup</code> <code>bool</code> <p>Flag to check for duplicates before insertion (currently not implemented).</p> <code>False</code> Source code in <code>src/database/milvus_client.py</code> <pre><code>def add(self, vector, metadata, check_dup=False):\n    \"\"\"Adds a vector and its associated metadata to the collection.\n\n    Args:\n        vector (list[float]): The vector to be added.\n        metadata (dict): Metadata associated with the vector.\n        check_dup (bool): Flag to check for duplicates before insertion (currently not implemented).\n    \"\"\"\n    data = [vector] + [metadata[field] for field in metadata]\n    self.collection.insert(data)\n    self.collection.flush()  # Ensures data persistence\n</code></pre>"},{"location":"reference/database/milvus_client/#src.database.milvus_client.MilvusClient.reset","title":"<code>reset()</code>","text":"<p>Drops the current collection and creates a new one.</p> Source code in <code>src/database/milvus_client.py</code> <pre><code>def reset(self):\n    \"\"\"Drops the current collection and creates a new one.\"\"\"\n    self.reset_collection()\n</code></pre>"},{"location":"reference/database/milvus_client/#src.database.milvus_client.MilvusClient.reset_collection","title":"<code>reset_collection()</code>","text":"<p>Drops the current collection and creates a new one.</p> Source code in <code>src/database/milvus_client.py</code> <pre><code>def reset_collection(self):\n    \"\"\"Drops the current collection and creates a new one.\"\"\"\n    if self.collection_name in list_collections():\n        drop_collection(self.collection_name)\n    self._create_collection()\n</code></pre>"},{"location":"reference/database/milvus_client/#src.database.milvus_client.MilvusClient.search","title":"<code>search(vector, top_k, distance_range=None, search_params=None, output_fields=None, filter_expr=None)</code>","text":"<p>Performs a vector search in the collection.</p> <p>Parameters:</p> Name Type Description Default <code>vector</code> <code>list[float]</code> <p>The query vector.</p> required <code>top_k</code> <code>int</code> <p>Number of top results to return.</p> required <code>distance_range</code> <code>list[int, int]</code> <p>Minimum and maximum distances for filtering results.</p> <code>None</code> <code>search_params</code> <code>dict</code> <p>Parameters for the search.</p> <code>None</code> <code>output_fields</code> <code>list</code> <p>Fields to include in the returned results.</p> <code>None</code> <code>filter_expr</code> <code>str</code> <p>Filter expression for conditional search.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SearchResult</code> <p>Search results from Milvus.</p> Source code in <code>src/database/milvus_client.py</code> <pre><code>def search(\n        self,\n        vector,\n        top_k,\n        distance_range=None,\n        search_params=None,\n        output_fields=None,\n        filter_expr=None,\n):\n    \"\"\"Performs a vector search in the collection.\n\n    Args:\n        vector (list[float]): The query vector.\n        top_k (int): Number of top results to return.\n        distance_range (list[int, int], optional): Minimum and maximum distances for filtering results.\n        search_params (dict, optional): Parameters for the search.\n        output_fields (list, optional): Fields to include in the returned results.\n        filter_expr (str, optional): Filter expression for conditional search.\n\n    Returns:\n        SearchResult: Search results from Milvus.\n    \"\"\"\n    self.collection.load()\n    if search_params is None:\n        search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n    if distance_range:\n        search_params[\"params\"].update(\n            {\"range_filter\": distance_range[0], \"radius\": distance_range[1]}\n        )\n\n    return self.collection.search(\n        data=[vector],\n        anns_field=\"vector\",\n        param=search_params,\n        limit=top_k,\n        expr=filter_expr,\n        output_fields=output_fields,\n    )\n</code></pre>"},{"location":"reference/database/query_builder/","title":"Query Builder","text":""},{"location":"reference/database/query_builder/#src.database.query_builder.ElasticQueryBuilder","title":"<code>src.database.query_builder.ElasticQueryBuilder</code>","text":"Source code in <code>src/database/query_builder.py</code> <pre><code>class ElasticQueryBuilder:\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        Initialize the QueryBuilder with the Elasticsearch-related config.\n\n        Args:\n            config (Dict[str, Any]): Elasticsearch config, including the query templates and other parameters.\n        \"\"\"\n        if not config or 'query_body' not in config:\n            raise ValueError(\"Elasticsearch config must include 'query_body'.\")\n\n        self.query_body: Dict[str, Any] = config.get('query_body')\n        self.timeout: int = config.get('timeout', 30)\n        self.max_retries: int = config.get('max_retries', 3)\n        self.overfetch_buffer: int = config.get('overfetch_buffer', 50)\n\n    def get_query(self, user_input: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve and process the query template with user input.\n\n        Args:\n            user_input (str): The user input to inject into the query.\n\n        Returns:\n            Dict[str, Any]: The processed query body.\n        \"\"\"\n        if not self.query_body:\n            raise ValueError(\"No query body found in the Elasticsearch configuration.\")\n\n        # Convert the query body to JSON string for template substitution\n        query_body_str = json.dumps(self.query_body)\n\n        # Escape user input properly using json.dumps\n        escaped_user_input = json.dumps(user_input)[1:-1]\n\n        # Replace placeholders using Python's Template\n        processed_query_str = Template(query_body_str).substitute(USER_INPUT=escaped_user_input)\n\n        return json.loads(processed_query_str)\n</code></pre>"},{"location":"reference/database/query_builder/#src.database.query_builder.ElasticQueryBuilder.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the QueryBuilder with the Elasticsearch-related config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Elasticsearch config, including the query templates and other parameters.</p> required Source code in <code>src/database/query_builder.py</code> <pre><code>def __init__(self, config: Dict[str, Any]):\n    \"\"\"\n    Initialize the QueryBuilder with the Elasticsearch-related config.\n\n    Args:\n        config (Dict[str, Any]): Elasticsearch config, including the query templates and other parameters.\n    \"\"\"\n    if not config or 'query_body' not in config:\n        raise ValueError(\"Elasticsearch config must include 'query_body'.\")\n\n    self.query_body: Dict[str, Any] = config.get('query_body')\n    self.timeout: int = config.get('timeout', 30)\n    self.max_retries: int = config.get('max_retries', 3)\n    self.overfetch_buffer: int = config.get('overfetch_buffer', 50)\n</code></pre>"},{"location":"reference/database/query_builder/#src.database.query_builder.ElasticQueryBuilder.get_query","title":"<code>get_query(user_input)</code>","text":"<p>Retrieve and process the query template with user input.</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>The user input to inject into the query.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The processed query body.</p> Source code in <code>src/database/query_builder.py</code> <pre><code>def get_query(self, user_input: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve and process the query template with user input.\n\n    Args:\n        user_input (str): The user input to inject into the query.\n\n    Returns:\n        Dict[str, Any]: The processed query body.\n    \"\"\"\n    if not self.query_body:\n        raise ValueError(\"No query body found in the Elasticsearch configuration.\")\n\n    # Convert the query body to JSON string for template substitution\n    query_body_str = json.dumps(self.query_body)\n\n    # Escape user input properly using json.dumps\n    escaped_user_input = json.dumps(user_input)[1:-1]\n\n    # Replace placeholders using Python's Template\n    processed_query_str = Template(query_body_str).substitute(USER_INPUT=escaped_user_input)\n\n    return json.loads(processed_query_str)\n</code></pre>"},{"location":"reference/llm/","title":"LLM Documentation","text":""},{"location":"reference/llm/#overview","title":"Overview","text":"<p>The LLM (Large Language Model) Module provides interfaces and implementations for interacting with various LLM providers. It includes factory methods for creating LLM instances, adapters for different vendors, and utilities for pattern detection and tool call handling.</p>"},{"location":"reference/llm/#components","title":"Components","text":""},{"location":"reference/llm/#llm-factory","title":"LLM Factory","text":"<ul> <li><code>LLMFactory</code> - A factory class for creating LLM instances based on the selected provider.</li> </ul>"},{"location":"reference/llm/#llm-adapters","title":"LLM Adapters","text":"<p>Adapters to support multiple LLM providers:</p> <ul> <li><code>BaseVendorAdapter</code> - Abstract base class for all LLM vendor adapters.</li> <li><code>AnthropicAdapter</code> - Adapter for Anthropic\u2019s Claude models.</li> <li><code>MistralAdapter</code> - Adapter for Mistral AI models.</li> <li><code>OpenAIAdapter</code> - Adapter for OpenAI\u2019s GPT models.</li> <li><code>WatsonXAdapter</code> - Adapter for IBM WatsonX models.</li> </ul>"},{"location":"reference/llm/#watsonx-components","title":"WatsonX Components","text":"<ul> <li><code>WatsonXConfig</code> - Configuration settings for WatsonX integration.</li> <li><code>IBMTokenManager</code> - Token manager for IBM WatsonX authentication.</li> </ul>"},{"location":"reference/llm/#pattern-detection","title":"Pattern Detection","text":"<ul> <li><code>AhoCorasickAutomaton</code> - An automaton for efficient pattern matching.</li> <li><code>AhoCorasickBufferedProcessor</code> - A buffered processor using the Aho-Corasick algorithm.</li> </ul>"},{"location":"reference/llm/#tool-call-detection","title":"Tool Call Detection","text":"<ul> <li><code>BaseToolCallDetectionStrategy</code> - Abstract class for tool call detection strategies.</li> <li><code>DetectionState</code> and <code>DetectionResult</code> - Data models representing the state and results of detection.</li> <li><code>ManualToolCallDetectionStrategy</code> - A manual approach to tool call detection.</li> <li><code>VendorToolCallDetectionStrategy</code> - Vendor-specific tool call detection strategies.</li> </ul>"},{"location":"reference/llm/#additional-resources","title":"Additional Resources","text":"<ul> <li>LLM Adapters</li> <li>Pattern Detection</li> <li>Tool Call Detection</li> </ul>"},{"location":"reference/llm/llm_factory/","title":"Factory","text":""},{"location":"reference/llm/llm_factory/#src.llm.llm_factory.LLMFactory","title":"<code>src.llm.llm_factory.LLMFactory</code>","text":"<p>Factory for creating and managing LLM adapters for different vendors.</p> <p>This class implements the Factory pattern to instantiate and manage different LLM adapters based on configuration. It maintains a singleton-like pattern for adapter instances and service-specific token managers.</p> <p>Attributes:</p> Name Type Description <code>_adapters</code> <code>Dict[str, BaseVendorAdapter]</code> <p>Class-level dictionary storing instantiated adapters.</p> <code>_token_manager</code> <code>IBMTokenManager</code> <p>Class-level token manager instance for WatsonX.</p> <code>_adapter_registry</code> <code>Dict[str, Type[BaseVendorAdapter]]</code> <p>Mapping of vendor names to adapter classes.</p> Source code in <code>src/llm/llm_factory.py</code> <pre><code>class LLMFactory:\n    \"\"\"Factory for creating and managing LLM adapters for different vendors.\n\n    This class implements the Factory pattern to instantiate and manage different\n    LLM adapters based on configuration. It maintains a singleton-like pattern\n    for adapter instances and service-specific token managers.\n\n    Attributes:\n        _adapters (Dict[str, BaseVendorAdapter]): Class-level dictionary storing instantiated adapters.\n        _token_manager (IBMTokenManager): Class-level token manager instance for WatsonX.\n        _adapter_registry (Dict[str, Type[BaseVendorAdapter]]): Mapping of vendor names to adapter classes.\n    \"\"\"\n\n    _adapters: Optional[Dict[str, BaseVendorAdapter]] = None\n    _token_manager: Optional[IBMTokenManager] = None\n\n    # Registry of standard adapter classes by vendor name\n    _adapter_registry: Dict[str, Type[BaseVendorAdapter]] = {\n        \"openai\": OpenAIAdapter,\n        \"anthropic\": AnthropicAdapter,\n        \"mistral-ai\": MistralAIAdapter,\n        \"xai\": XAIAdapter,\n        \"openai-compat\": OpenAICompatAdapter,\n    }\n\n    def __init__(self, config: Dict[str, Dict[str, Any]]):\n        \"\"\"Initialize the LLM Factory with configuration.\n\n        Args:\n            config (Dict[str, Dict[str, Any]]): Configuration dictionary containing model configurations.\n                Expected format:\n                {\n                    \"model_name\": {\n                        \"vendor\": str,\n                        \"model_id\": str,\n                        ...additional_config\n                    }\n                }\n\n        Raises:\n            ValueError: If the configuration format is invalid.\n        \"\"\"\n        if LLMFactory._adapters is None:\n            self._initialize_adapters(config)\n\n    @classmethod\n    def _initialize_adapters(cls, config: Dict[str, Dict[str, Any]]) -&gt; None:\n        \"\"\"Initialize adapters based on the provided configuration.\n\n        This method creates adapter instances for each model in the config.\n\n        Args:\n            config (Dict[str, Dict[str, Any]]): Configuration dictionary for all models.\n\n        Raises:\n            ValueError: If an unknown vendor is specified or if required configuration is missing.\n        \"\"\"\n        cls._adapters = {}\n        logger.debug(\"Initializing LLM adapters\")\n\n        # Initialize service-specific components once if needed\n        cls._initialize_service_components(config)\n\n        # Process each model in the configuration\n        for model_name, model_config in config.items():\n            try:\n                # Validate and extract configuration\n                validated_config = cls._validate_model_config(model_name, model_config)\n                vendor = validated_config[\"vendor\"]\n                model_id = validated_config[\"model_id\"]\n                adapter_params = validated_config[\"adapter_params\"]\n\n                # Create the adapter\n                adapter = cls._create_adapter(vendor, model_id, **adapter_params)\n                cls._adapters[model_name] = adapter\n                logger.debug(f\"Initialized {vendor} adapter for model: {model_name}\")\n\n            except Exception as e:\n                logger.error(f\"Failed to initialize adapter for {model_name}: {str(e)}\")\n                # Add context to the exception\n                raise ValueError(f\"Adapter initialization failed for {model_name}\") from e\n\n    @classmethod\n    def _initialize_service_components(cls, config: Dict[str, Dict[str, Any]]) -&gt; None:\n        \"\"\"Initialize service-specific components required by adapters.\n\n        Args:\n            config (Dict[str, Dict[str, Any]]): Configuration dictionary for all models.\n\n        Raises:\n            ValueError: If initialization of a service component fails.\n        \"\"\"\n        # Initialize WatsonX Token Manager if needed\n        if any(\"watsonx\" in model_config.get(\"vendor\", \"\") for model_config in config.values()):\n            try:\n                cls._token_manager = IBMTokenManager(api_key=WatsonXConfig.CREDS.get('apikey'))\n                logger.debug(\"Initialized WatsonX Token Manager\")\n            except Exception as e:\n                logger.error(f\"Failed to initialize WatsonX Token Manager: {str(e)}\")\n                raise ValueError(\"Failed to initialize WatsonX Token Manager\") from e\n\n    @classmethod\n    def _create_adapter(cls, vendor: str, model_id: str, **kwargs) -&gt; BaseVendorAdapter:\n        \"\"\"Create an adapter instance based on vendor and model ID.\n\n        Args:\n            vendor (str): The vendor identifier.\n            model_id (str): The model identifier.\n            **kwargs: Additional parameters for the adapter.\n\n        Returns:\n            BaseVendorAdapter: The created adapter instance.\n\n        Raises:\n            ValueError: If the vendor is unknown or if adapter creation fails.\n        \"\"\"\n        # Handle special case for WatsonX\n        if \"watsonx\" in vendor:\n            if cls._token_manager is None:\n                raise ValueError(\"IBMTokenManager was not initialized for WatsonX models.\")\n            return WatsonXAdapter(\n                model_name=model_id,\n                token_manager=cls._token_manager,\n                **kwargs\n            )\n\n        # Handle case for xAI\n        if vendor == \"xai\":\n            # Get API key from config or environment\n            api_key = kwargs.pop(\"api_key\", None) or os.getenv(\"XAI_API_KEY\")\n            if not api_key:\n                logger.warning(f\"No XAI API key found for model {model_id}. Set XAI_API_KEY environment variable.\")\n\n            # Use the standard X.AI base URL unless overridden\n            base_url = kwargs.pop(\"base_url\", \"https://api.x.ai/v1\")\n\n            return XAIAdapter(\n                model_name=model_id,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs\n            )\n\n        # Check for OpenAI compatibility vendors (partial match)\n        if \"openai-compat\" in vendor:\n            return OpenAICompatAdapter(model_name=model_id, **kwargs)\n\n        # Handle standard adapters from registry with exact match\n        adapter_class = cls._adapter_registry.get(vendor)\n        if adapter_class:\n            return adapter_class(model_name=model_id, **kwargs)\n\n        # If we get here, the vendor is unknown\n        raise ValueError(f\"Unknown vendor '{vendor}'\")\n\n    @staticmethod\n    def _validate_model_config(model_name: str, config: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate model configuration and extract adapter parameters.\n\n        Args:\n            model_name (str): The name of the model.\n            config (Dict[str, Any]): The model configuration.\n\n        Returns:\n            Dict[str, Any]: Validated configuration with extracted parameters.\n\n        Raises:\n            ValueError: If required fields are missing.\n        \"\"\"\n        # Check required fields\n        required_fields = [\"vendor\", \"model_id\"]\n        for field in required_fields:\n            if field not in config:\n                raise ValueError(f\"Missing required field '{field}' for model '{model_name}'\")\n\n        # Extract and return relevant configuration\n        return {\n            \"vendor\": config[\"vendor\"],\n            \"model_id\": config[\"model_id\"],\n            \"adapter_params\": {k: v for k, v in config.items() if k not in [\"vendor\", \"model_id\"]}\n        }\n\n    @classmethod\n    def get_adapter(cls, model_name: str, config: Optional[Dict[str, Dict[str, Any]]] = None) -&gt; BaseVendorAdapter:\n        \"\"\"Retrieve an adapter instance for a specific model with lazy initialization if needed.\n\n        Args:\n            model_name (str): Name of the model to retrieve the adapter for.\n            config (Optional[Dict[str, Dict[str, Any]]]): Configuration to use if factory is not initialized.\n\n        Returns:\n            BaseVendorAdapter: The adapter instance for the specified model.\n\n        Raises:\n            ValueError: If adapters haven't been initialized or if the\n                requested model adapter is not found.\n        \"\"\"\n        # Lazy initialization if needed\n        if cls._adapters is None:\n            if not config:\n                raise ValueError(\"Adapters have not been initialized. Initialize the factory with a config first.\")\n            cls._initialize_adapters(config)\n\n        adapter = cls._adapters.get(model_name)\n        if adapter:\n            logger.debug(f\"Retrieved adapter for model: {model_name}\")\n            return adapter\n        else:\n            raise ValueError(f\"Adapter for model '{model_name}' not found.\")\n\n    @classmethod\n    def has_adapter(cls, model_name: str) -&gt; bool:\n        \"\"\"Check if an adapter is available for a model without raising exceptions.\n\n        Args:\n            model_name (str): The name of the model to check.\n\n        Returns:\n            bool: True if the adapter exists, False otherwise.\n        \"\"\"\n        return cls._adapters is not None and model_name in cls._adapters\n\n    @classmethod\n    def list_available_models(cls) -&gt; list:\n        \"\"\"List all available model names that have initialized adapters.\n\n        Returns:\n            list: List of model names with initialized adapters.\n\n        Raises:\n            ValueError: If adapters haven't been initialized.\n        \"\"\"\n        if cls._adapters is None:\n            raise ValueError(\"Adapters have not been initialized.\")\n\n        return list(cls._adapters.keys())\n</code></pre>"},{"location":"reference/llm/llm_factory/#src.llm.llm_factory.LLMFactory.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the LLM Factory with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Dict[str, Any]]</code> <p>Configuration dictionary containing model configurations. Expected format: {     \"model_name\": {         \"vendor\": str,         \"model_id\": str,         ...additional_config     } }</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configuration format is invalid.</p> Source code in <code>src/llm/llm_factory.py</code> <pre><code>def __init__(self, config: Dict[str, Dict[str, Any]]):\n    \"\"\"Initialize the LLM Factory with configuration.\n\n    Args:\n        config (Dict[str, Dict[str, Any]]): Configuration dictionary containing model configurations.\n            Expected format:\n            {\n                \"model_name\": {\n                    \"vendor\": str,\n                    \"model_id\": str,\n                    ...additional_config\n                }\n            }\n\n    Raises:\n        ValueError: If the configuration format is invalid.\n    \"\"\"\n    if LLMFactory._adapters is None:\n        self._initialize_adapters(config)\n</code></pre>"},{"location":"reference/llm/llm_factory/#src.llm.llm_factory.LLMFactory.get_adapter","title":"<code>get_adapter(model_name, config=None)</code>  <code>classmethod</code>","text":"<p>Retrieve an adapter instance for a specific model with lazy initialization if needed.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to retrieve the adapter for.</p> required <code>config</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Configuration to use if factory is not initialized.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BaseVendorAdapter</code> <code>BaseVendorAdapter</code> <p>The adapter instance for the specified model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If adapters haven't been initialized or if the requested model adapter is not found.</p> Source code in <code>src/llm/llm_factory.py</code> <pre><code>@classmethod\ndef get_adapter(cls, model_name: str, config: Optional[Dict[str, Dict[str, Any]]] = None) -&gt; BaseVendorAdapter:\n    \"\"\"Retrieve an adapter instance for a specific model with lazy initialization if needed.\n\n    Args:\n        model_name (str): Name of the model to retrieve the adapter for.\n        config (Optional[Dict[str, Dict[str, Any]]]): Configuration to use if factory is not initialized.\n\n    Returns:\n        BaseVendorAdapter: The adapter instance for the specified model.\n\n    Raises:\n        ValueError: If adapters haven't been initialized or if the\n            requested model adapter is not found.\n    \"\"\"\n    # Lazy initialization if needed\n    if cls._adapters is None:\n        if not config:\n            raise ValueError(\"Adapters have not been initialized. Initialize the factory with a config first.\")\n        cls._initialize_adapters(config)\n\n    adapter = cls._adapters.get(model_name)\n    if adapter:\n        logger.debug(f\"Retrieved adapter for model: {model_name}\")\n        return adapter\n    else:\n        raise ValueError(f\"Adapter for model '{model_name}' not found.\")\n</code></pre>"},{"location":"reference/llm/llm_factory/#src.llm.llm_factory.LLMFactory.has_adapter","title":"<code>has_adapter(model_name)</code>  <code>classmethod</code>","text":"<p>Check if an adapter is available for a model without raising exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the adapter exists, False otherwise.</p> Source code in <code>src/llm/llm_factory.py</code> <pre><code>@classmethod\ndef has_adapter(cls, model_name: str) -&gt; bool:\n    \"\"\"Check if an adapter is available for a model without raising exceptions.\n\n    Args:\n        model_name (str): The name of the model to check.\n\n    Returns:\n        bool: True if the adapter exists, False otherwise.\n    \"\"\"\n    return cls._adapters is not None and model_name in cls._adapters\n</code></pre>"},{"location":"reference/llm/llm_factory/#src.llm.llm_factory.LLMFactory.list_available_models","title":"<code>list_available_models()</code>  <code>classmethod</code>","text":"<p>List all available model names that have initialized adapters.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of model names with initialized adapters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If adapters haven't been initialized.</p> Source code in <code>src/llm/llm_factory.py</code> <pre><code>@classmethod\ndef list_available_models(cls) -&gt; list:\n    \"\"\"List all available model names that have initialized adapters.\n\n    Returns:\n        list: List of model names with initialized adapters.\n\n    Raises:\n        ValueError: If adapters haven't been initialized.\n    \"\"\"\n    if cls._adapters is None:\n        raise ValueError(\"Adapters have not been initialized.\")\n\n    return list(cls._adapters.keys())\n</code></pre>"},{"location":"reference/llm/adapters/","title":"LLM Adapters","text":"<p>This directory contains a collection of adapter classes that provide a standardized interface for interacting with various Large Language Model (LLM) providers. Each adapter implements the <code>BaseVendorAdapter</code> abstract base class to ensure consistent handling of model interactions across different vendors.</p>"},{"location":"reference/llm/adapters/#overview","title":"Overview","text":"<p>These adapters enable our system to work seamlessly with multiple LLM providers by converting vendor-specific APIs into a unified interface. All adapters produce standardized Server-Sent Events (SSE) chunks when streaming text responses.</p>"},{"location":"reference/llm/adapters/#available-adapters","title":"Available Adapters","text":"Adapter Description BaseVendorAdapter Interface all adapters must implement AnthropicAdapter Adapter for Anthropic's Claude models OpenAIAdapter Adapter for OpenAI models OpenAICompatAdapter Adapter for APIs compatible with OpenAI's interface MistralAIAdapter Adapter for Mistral AI models XAIAdapter Adapter for xAI models WatsonxAdapter Adapter for IBM's watsonx.ai platform"},{"location":"reference/llm/adapters/#watsonx-submodule","title":"WatsonX Submodule","text":"<p>The watsonx directory contains specialized implementations for IBM's watsonx.ai platform:</p> <ul> <li>WatsonxAdapter - Main adapter for watsonx.ai models</li> <li>IBMTokenManager - Handles authentication with IBM Cloud</li> <li>WatsonxConfig - Configuration for watsonx.ai connections</li> </ul>"},{"location":"reference/llm/adapters/#core-features","title":"Core Features","text":"<p>All adapters implement these key methods:</p> <ul> <li><code>gen_sse_stream(prompt: str)</code> - Generate streaming responses from a text prompt</li> <li><code>gen_chat_sse_stream(messages: List[TextChatMessage], tools: Optional[List[Tool]])</code> - Generate streaming responses from a chat context</li> </ul>"},{"location":"reference/llm/adapters/#implementation-requirements","title":"Implementation Requirements","text":"<p>When implementing a new adapter:</p> <ol> <li>Inherit from <code>BaseVendorAdapter</code></li> <li>Implement all abstract methods</li> <li>Convert vendor-specific responses to our standardized <code>SSEChunk</code> format</li> <li>Handle streaming appropriately for the specific vendor API</li> </ol>"},{"location":"reference/llm/adapters/#usage-example","title":"Usage Example","text":"<pre><code># Example of using an adapter\nfrom src.llm.adapters import OpenAIAdapter\n\n# Initialize with model name and default parameters\nadapter = OpenAIAdapter(\n    model_name=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Generate streaming response\nasync for chunk in adapter.gen_sse_stream(\"Tell me about machine learning\"):\n    # Process each SSEChunk\n    if chunk.choices and chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        # Process the content fragment\n        print(content, end=\"\")\n</code></pre>"},{"location":"reference/llm/adapters/#adding-new-adapters","title":"Adding New Adapters","text":"<p>To add support for a new LLM provider:</p> <ol> <li>Create a new file named <code>your_provider_adapter.py</code></li> <li>Implement the <code>BaseVendorAdapter</code> interface</li> <li>Add docstrings and logging</li> <li>Update this index with your new adapter</li> </ol>"},{"location":"reference/llm/adapters/anthropic_adapter/","title":"Anthropic","text":""},{"location":"reference/llm/adapters/anthropic_adapter/#src.llm.adapters.anthropic_adapter.AnthropicAdapter","title":"<code>src.llm.adapters.anthropic_adapter.AnthropicAdapter</code>","text":"<p>               Bases: <code>BaseVendorAdapter</code></p> <p>Adapter for interacting with Anthropic's API.</p> Source code in <code>src/llm/adapters/anthropic_adapter.py</code> <pre><code>class AnthropicAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with Anthropic's API.\"\"\"\n\n    def __init__(self, model_name: str, **default_params):\n        \"\"\"Initialize Anthropic Adapter.\n\n        Args:\n            model_name (str): The name of the model to use.\n            **default_params: Additional default parameters for the adapter.\n\n        Raises:\n            ValueError: If the Anthropic API key is missing.\n        \"\"\"\n        self.api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"Missing Anthropic API key. Set the ANTHROPIC_API_KEY environment variable.\"\n            )\n        self.client = AsyncAnthropic(api_key=self.api_key)\n        self.model_name = model_name\n        self.default_params = default_params\n        logger.info(f\"Anthropic Adapter initialized with model: {self.model_name}\")\n\n    async def gen_sse_stream(\n        self, prompt: str, **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate an SSE stream from a text prompt.\n\n        Args:\n            prompt (str): The text prompt to generate an SSE stream for.\n            **kwargs: Additional keyword arguments for generation.\n\n        Yields:\n            AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.\n        \"\"\"\n        async for chunk in self.gen_chat_sse_stream([{\"role\": \"user\", \"content\": prompt}], **kwargs):\n            yield chunk\n\n    async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        **kwargs,\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response.\n\n        Args:\n            `messages` (List[TextChatMessage]): A list of chat messages.\n            `tools` (Optional[List[Tool]], optional): A list of Tool objects. Defaults to None.\n            `**kwargs`: Additional keyword arguments for generation.\n\n        Yields:\n            AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.\n        \"\"\"\n        request_payload = {\n            \"model\": self.model_name,\n            \"max_tokens\": self.default_params.get(\"max_tokens\", 1024),\n            \"stream\": True,\n            **self.default_params,\n            **kwargs,\n            **convert_messages_to_anthropic(messages),\n        }\n\n        if tools:\n            anthropic_tools = [convert_tool_to_anthropic_format(tool) for tool in tools]\n            request_payload[\"tools\"] = anthropic_tools\n            request_payload[\"tool_choice\"] = {\"type\": \"auto\"}\n\n        try:\n            stream = await self.client.messages.create(**request_payload)\n            async for event in stream:\n                yield await self._convert_to_sse_chunk(event)\n        except Exception as e:\n            logger.error(f\"Error in Anthropic streaming: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Anthropic API streaming failed: {str(e)}\") from e\n\n    async def _convert_to_sse_chunk(self, raw_event: Any) -&gt; SSEChunk:\n        \"\"\"Convert an Anthropic event to an SSEChunk.\n\n        Args:\n            `raw_event` (Any): The raw event from the Anthropic API.\n\n        Returns:\n            SSEChunk: The converted SSEChunk object.\n\n        Raises:\n            ValueError: If conversion of the event fails.\n        \"\"\"\n        try:\n            event_type = raw_event.type\n            current_time = int(time.time())\n\n            match event_type:\n                case \"content_block_start\":\n                    content_block = raw_event.content_block\n                    if content_block.type == \"text\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=getattr(content_block, \"text\", \"\"),\n                        )\n                    elif content_block.type == \"tool_use\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=\"\",\n                            tool_calls=[SSEToolCall(\n                                id=content_block.id,\n                                type=\"function\",\n                                function=SSEFunction(name=content_block.name, arguments=\"\"),\n                            )],\n                        )\n                    else:\n                        delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=raw_event.index, delta=delta)\n                    return SSEChunk(\n                        id=f\"content_block_start_{raw_event.index}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"content_block_delta\":\n                    delta_info = raw_event.delta\n                    if delta_info.type == \"text_delta\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=delta_info.text,\n                        )\n                    elif delta_info.type == \"input_json_delta\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=\"\",\n                            tool_calls=[SSEToolCall(\n                                type=\"function\",\n                                function=SSEFunction(\n                                    name=\"\",\n                                    arguments=delta_info.partial_json,\n                                ),\n                            )],\n                        )\n                    else:\n                        delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=raw_event.index, delta=delta)\n                    return SSEChunk(\n                        id=f\"delta_{raw_event.index}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"content_block_stop\":\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=raw_event.index, delta=delta)\n                    return SSEChunk(\n                        id=f\"block_stop_{raw_event.index}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"message_delta\":\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(\n                        index=0,\n                        delta=delta,\n                        finish_reason=getattr(raw_event.delta, \"stop_reason\", None),\n                    )\n                    return SSEChunk(\n                        id=\"message_delta\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"message_stop\":\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=0, delta=delta)\n                    return SSEChunk(\n                        id=\"message_stop\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case _:\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=0, delta=delta)\n                    return SSEChunk(\n                        id=f\"unknown_{event_type}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n        except Exception as e:\n            logger.error(f\"Error converting Anthropic event: {raw_event}\", exc_info=True)\n            raise ValueError(f\"Failed to convert Anthropic response to SSEChunk: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/anthropic_adapter/#src.llm.adapters.anthropic_adapter.AnthropicAdapter.__init__","title":"<code>__init__(model_name, **default_params)</code>","text":"<p>Initialize Anthropic Adapter.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>**default_params</code> <p>Additional default parameters for the adapter.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Anthropic API key is missing.</p> Source code in <code>src/llm/adapters/anthropic_adapter.py</code> <pre><code>def __init__(self, model_name: str, **default_params):\n    \"\"\"Initialize Anthropic Adapter.\n\n    Args:\n        model_name (str): The name of the model to use.\n        **default_params: Additional default parameters for the adapter.\n\n    Raises:\n        ValueError: If the Anthropic API key is missing.\n    \"\"\"\n    self.api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n    if not self.api_key:\n        raise ValueError(\n            \"Missing Anthropic API key. Set the ANTHROPIC_API_KEY environment variable.\"\n        )\n    self.client = AsyncAnthropic(api_key=self.api_key)\n    self.model_name = model_name\n    self.default_params = default_params\n    logger.info(f\"Anthropic Adapter initialized with model: {self.model_name}\")\n</code></pre>"},{"location":"reference/llm/adapters/anthropic_adapter/#src.llm.adapters.anthropic_adapter.AnthropicAdapter.gen_chat_sse_stream","title":"<code>gen_chat_sse_stream(messages, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate a streaming chat response.</p> <p>Parameters:</p> Name Type Description Default <code>`messages`</code> <code>List[TextChatMessage]</code> <p>A list of chat messages.</p> required <code>`tools`</code> <code>Optional[List[Tool]]</code> <p>A list of Tool objects. Defaults to None.</p> required <code>`**kwargs`</code> <p>Additional keyword arguments for generation.</p> required <p>Yields:</p> Type Description <code>AsyncGenerator[SSEChunk, None]</code> <p>AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.</p> Source code in <code>src/llm/adapters/anthropic_adapter.py</code> <pre><code>async def gen_chat_sse_stream(\n    self,\n    messages: List[TextChatMessage],\n    tools: Optional[List[Tool]] = None,\n    **kwargs,\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate a streaming chat response.\n\n    Args:\n        `messages` (List[TextChatMessage]): A list of chat messages.\n        `tools` (Optional[List[Tool]], optional): A list of Tool objects. Defaults to None.\n        `**kwargs`: Additional keyword arguments for generation.\n\n    Yields:\n        AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.\n    \"\"\"\n    request_payload = {\n        \"model\": self.model_name,\n        \"max_tokens\": self.default_params.get(\"max_tokens\", 1024),\n        \"stream\": True,\n        **self.default_params,\n        **kwargs,\n        **convert_messages_to_anthropic(messages),\n    }\n\n    if tools:\n        anthropic_tools = [convert_tool_to_anthropic_format(tool) for tool in tools]\n        request_payload[\"tools\"] = anthropic_tools\n        request_payload[\"tool_choice\"] = {\"type\": \"auto\"}\n\n    try:\n        stream = await self.client.messages.create(**request_payload)\n        async for event in stream:\n            yield await self._convert_to_sse_chunk(event)\n    except Exception as e:\n        logger.error(f\"Error in Anthropic streaming: {str(e)}\", exc_info=True)\n        raise RuntimeError(f\"Anthropic API streaming failed: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/anthropic_adapter/#src.llm.adapters.anthropic_adapter.AnthropicAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Generate an SSE stream from a text prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to generate an SSE stream for.</p> required <code>**kwargs</code> <p>Additional keyword arguments for generation.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[SSEChunk, None]</code> <p>AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.</p> Source code in <code>src/llm/adapters/anthropic_adapter.py</code> <pre><code>async def gen_sse_stream(\n    self, prompt: str, **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate an SSE stream from a text prompt.\n\n    Args:\n        prompt (str): The text prompt to generate an SSE stream for.\n        **kwargs: Additional keyword arguments for generation.\n\n    Yields:\n        AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.\n    \"\"\"\n    async for chunk in self.gen_chat_sse_stream([{\"role\": \"user\", \"content\": prompt}], **kwargs):\n        yield chunk\n</code></pre>"},{"location":"reference/llm/adapters/base_vendor_adapter/","title":"Base Adapter","text":""},{"location":"reference/llm/adapters/base_vendor_adapter/#src.llm.adapters.base_vendor_adapter.BaseVendorAdapter","title":"<code>src.llm.adapters.base_vendor_adapter.BaseVendorAdapter</code>","text":"<p>Abstract base class for any LLM vendor adapter. Must produce SSEChunk objects when streaming text.</p> Source code in <code>src/llm/adapters/base_vendor_adapter.py</code> <pre><code>class BaseVendorAdapter:\n    \"\"\"\n    Abstract base class for any LLM vendor adapter.\n    Must produce SSEChunk objects when streaming text.\n    \"\"\"\n\n    @abstractmethod\n    async def gen_sse_stream(self, prompt: str) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"\n        Generate SSEChunk objects in a streaming manner from the given prompt.\n        \"\"\"\n        pass\n\n    # Optionally, you can define a chat method if you differentiate chat vs text\n    @abstractmethod\n    async def gen_chat_sse_stream(self, messages: List[TextChatMessage], tools: Optional[List[Tool]]) -&gt; AsyncGenerator[SSEChunk, None]:\n        pass\n</code></pre>"},{"location":"reference/llm/adapters/base_vendor_adapter/#src.llm.adapters.base_vendor_adapter.BaseVendorAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate SSEChunk objects in a streaming manner from the given prompt.</p> Source code in <code>src/llm/adapters/base_vendor_adapter.py</code> <pre><code>@abstractmethod\nasync def gen_sse_stream(self, prompt: str) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"\n    Generate SSEChunk objects in a streaming manner from the given prompt.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llm/adapters/mistral_ai_adapter/","title":"MistralAI","text":""},{"location":"reference/llm/adapters/mistral_ai_adapter/#src.llm.adapters.mistral_ai_adapter.MistralAIAdapter","title":"<code>src.llm.adapters.mistral_ai_adapter.MistralAIAdapter</code>","text":"<p>               Bases: <code>BaseVendorAdapter</code></p> <p>Adapter for interacting with Mistral AI's API.</p> <p>This class implements the BaseVendorAdapter interface for Mistral's chat models, handling authentication, request formatting, and response streaming. It converts Mistral-specific response formats into standardized SSE chunks for consistent handling across different LLM providers.</p> <p>Attributes:</p> Name Type Description <code>`api_key`</code> <code>str</code> <p>Mistral API key loaded from environment variables.</p> <code>`client`</code> <code>Mistral</code> <p>Authenticated Mistral client instance.</p> <code>`model_name`</code> <code>str</code> <p>The Mistral model identifier (e.g., \"mistral-tiny\").</p> <code>`default_params`</code> <code>dict</code> <p>Default parameters for Mistral API calls.</p> Source code in <code>src/llm/adapters/mistral_ai_adapter.py</code> <pre><code>class MistralAIAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with Mistral AI's API.\n\n    This class implements the BaseVendorAdapter interface for Mistral's chat models,\n    handling authentication, request formatting, and response streaming. It converts\n    Mistral-specific response formats into standardized SSE chunks for consistent\n    handling across different LLM providers.\n\n    Attributes:\n        `api_key` (str): Mistral API key loaded from environment variables.\n        `client` (Mistral): Authenticated Mistral client instance.\n        `model_name` (str): The Mistral model identifier (e.g., \"mistral-tiny\").\n        `default_params` (dict): Default parameters for Mistral API calls.\n    \"\"\"\n\n    def __init__(self, model_name: str, **default_params):\n        \"\"\"Initialize the Mistral AI Adapter with model configuration.\n\n        Args:\n            `model_name` (str): The identifier of the Mistral model to use (e.g., \"mistral-tiny\").\n            `**default_params`: Additional parameters to include in all API calls.\n                Common parameters include temperature, max_tokens, etc.\n\n        Raises:\n            `ValueError`: If MISTRAL_API_KEY environment variable is not set.\n        \"\"\"\n        self.api_key = os.getenv(\"MISTRAL_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Missing Mistral API key. Set the MISTRAL_API_KEY environment variable.\")\n\n        self.client = Mistral(api_key=self.api_key)\n        self.model_name = model_name\n        self.default_params = default_params\n        logger.info(f\"Mistral AI Adapter initialized with model: {self.model_name}\")\n        logger.debug(f\"Default parameters configured: {default_params}\")\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate SSE stream from a single text prompt.\n\n        Converts a single prompt into a chat message and streams the response.\n\n        Args:\n            `prompt` (str): The text prompt to send to the model.\n            `**kwargs`: Additional parameters to override defaults for this request.\n\n        Yields:\n            `SSEChunk`: Standardized chunks of the streaming response.\n\n        Raises:\n            `RuntimeError`: If the streaming request fails.\n        \"\"\"\n        logger.debug(f\"Converting single prompt to chat format: {prompt[:50]}...\")\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n            yield chunk\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs,\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response from a sequence of messages.\n\n        Args:\n            `messages` (List[TextChatMessage]): List of chat messages for context.\n            `tools` (Optional[List[Tool]]): List of tools available to the model.\n            `**kwargs`: Additional parameters to override defaults for this request.\n\n        Yields:\n            `SSEChunk`: Standardized chunks of the streaming response.\n\n        Raises:\n            `RuntimeError`: If the Mistral API request fails.\n        \"\"\"\n        mistral_messages = [msg.model_dump() for msg in messages]\n        logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n\n        request_payload = {\n            \"model\": self.model_name,\n            \"messages\": mistral_messages,\n            **self.default_params,\n            **kwargs,\n        }\n\n        if tools:\n            logger.debug(f\"Adding {len(tools)} tools to request\")\n            request_payload[\"tools\"] = [tool.model_dump() for tool in tools]\n            request_payload[\"tool_choice\"] = \"auto\"\n\n        try:\n            logger.debug(\"Initiating Mistral streaming request\")\n            response = await self.client.chat.stream_async(**request_payload)\n            async for chunk in response:\n                yield await self._convert_to_sse_chunk(chunk)\n        except Exception as e:\n            logger.error(f\"Error in Mistral streaming: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Mistral API streaming failed: {str(e)}\") from e\n\n    async def _convert_to_sse_chunk(self, raw_chunk) -&gt; SSEChunk:\n        \"\"\"Convert Mistral's response chunk to standardized SSE format.\n\n        Uses model_dump_json() to convert Mistral's response to a clean JSON representation,\n        then constructs an SSEChunk from the parsed data.\n\n        Args:\n            raw_chunk: Raw chunk from Mistral's API, could be CompletionChunk or CompletionEvent.\n\n        Returns:\n            SSEChunk: Standardized chunk format for consistent handling.\n\n        Raises:\n            ValueError: If chunk conversion fails due to unexpected format.\n        \"\"\"\n        try:\n            # Use model_dump_json() to get a clean JSON representation where Unset values are omitted\n            chunk_json = raw_chunk.model_dump_json()\n            chunk_data = json.loads(chunk_json)\n\n            # Extract the 'data' field if present (based on the provided example)\n            if 'data' in chunk_data:\n                chunk_data = chunk_data['data']\n\n            logger.debug(f\"Converting chunk ID: {chunk_data.get('id', 'unknown')}\")\n\n            # Process choices\n            choices = []\n            for choice_data in chunk_data.get('choices', []):\n                delta_data = choice_data.get('delta', {})\n\n                # Process tool calls if present\n                tool_calls = None\n                if 'tool_calls' in delta_data:\n                    tool_calls = []\n                    for tc_data in delta_data['tool_calls']:\n                        function = None\n                        if 'function' in tc_data:\n                            # Ensure name and arguments are valid strings, defaulting to empty strings if missing or None\n                            fn_name = tc_data['function'].get('name')\n                            fn_name = '' if fn_name is None else fn_name\n\n                            fn_args = tc_data['function'].get('arguments')\n                            fn_args = '' if fn_args is None else fn_args\n\n                            function = SSEFunction(\n                                name=fn_name,\n                                arguments=fn_args\n                            )\n\n                        # Ensure type is always a string, defaulting to 'function' if missing or None\n                        tool_call_type = tc_data.get('type')\n                        if tool_call_type is None:\n                            tool_call_type = 'function'\n\n                        tool_calls.append(SSEToolCall(\n                            index=tc_data.get('index', 0),\n                            id=tc_data.get('id'),\n                            type=tool_call_type,\n                            function=function\n                        ))\n\n                # Create delta\n                delta = SSEDelta(\n                    role=delta_data.get('role'),\n                    content=delta_data.get('content'),\n                    tool_calls=tool_calls,\n                    refusal=delta_data.get('refusal')\n                )\n\n                # Create choice\n                choices.append(SSEChoice(\n                    index=choice_data.get('index', 0),\n                    delta=delta,\n                    logprobs=choice_data.get('logprobs'),\n                    finish_reason=choice_data.get('finish_reason')\n                ))\n\n            # Create and return the SSEChunk\n            return SSEChunk(\n                id=chunk_data.get('id', f\"gen-{id(chunk_data)}\"),\n                object=chunk_data.get('object', 'chat.completion.chunk'),\n                created=chunk_data.get('created', int(datetime.now().timestamp())),\n                model=chunk_data.get('model', self.model_name),\n                service_tier=None,  # Default to None if not provided by Mistral\n                system_fingerprint=None,  # Default to None if not provided by Mistral\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting Mistral chunk: {e}\", exc_info=True)\n            raise ValueError(f\"Failed to convert Mistral response to SSEChunk: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/mistral_ai_adapter/#src.llm.adapters.mistral_ai_adapter.MistralAIAdapter.__init__","title":"<code>__init__(model_name, **default_params)</code>","text":"<p>Initialize the Mistral AI Adapter with model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>`model_name`</code> <code>str</code> <p>The identifier of the Mistral model to use (e.g., \"mistral-tiny\").</p> required <code>`**default_params`</code> <p>Additional parameters to include in all API calls. Common parameters include temperature, max_tokens, etc.</p> required <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If MISTRAL_API_KEY environment variable is not set.</p> Source code in <code>src/llm/adapters/mistral_ai_adapter.py</code> <pre><code>def __init__(self, model_name: str, **default_params):\n    \"\"\"Initialize the Mistral AI Adapter with model configuration.\n\n    Args:\n        `model_name` (str): The identifier of the Mistral model to use (e.g., \"mistral-tiny\").\n        `**default_params`: Additional parameters to include in all API calls.\n            Common parameters include temperature, max_tokens, etc.\n\n    Raises:\n        `ValueError`: If MISTRAL_API_KEY environment variable is not set.\n    \"\"\"\n    self.api_key = os.getenv(\"MISTRAL_API_KEY\")\n    if not self.api_key:\n        raise ValueError(\"Missing Mistral API key. Set the MISTRAL_API_KEY environment variable.\")\n\n    self.client = Mistral(api_key=self.api_key)\n    self.model_name = model_name\n    self.default_params = default_params\n    logger.info(f\"Mistral AI Adapter initialized with model: {self.model_name}\")\n    logger.debug(f\"Default parameters configured: {default_params}\")\n</code></pre>"},{"location":"reference/llm/adapters/mistral_ai_adapter/#src.llm.adapters.mistral_ai_adapter.MistralAIAdapter.gen_chat_sse_stream","title":"<code>gen_chat_sse_stream(messages, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate a streaming chat response from a sequence of messages.</p> <p>Parameters:</p> Name Type Description Default <code>`messages`</code> <code>List[TextChatMessage]</code> <p>List of chat messages for context.</p> required <code>`tools`</code> <code>Optional[List[Tool]]</code> <p>List of tools available to the model.</p> required <code>`**kwargs`</code> <p>Additional parameters to override defaults for this request.</p> required <p>Yields:</p> Type Description <code>AsyncGenerator[SSEChunk, None]</code> <p><code>SSEChunk</code>: Standardized chunks of the streaming response.</p> <p>Raises:</p> Type Description <code>`RuntimeError`</code> <p>If the Mistral API request fails.</p> Source code in <code>src/llm/adapters/mistral_ai_adapter.py</code> <pre><code>async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        **kwargs,\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate a streaming chat response from a sequence of messages.\n\n    Args:\n        `messages` (List[TextChatMessage]): List of chat messages for context.\n        `tools` (Optional[List[Tool]]): List of tools available to the model.\n        `**kwargs`: Additional parameters to override defaults for this request.\n\n    Yields:\n        `SSEChunk`: Standardized chunks of the streaming response.\n\n    Raises:\n        `RuntimeError`: If the Mistral API request fails.\n    \"\"\"\n    mistral_messages = [msg.model_dump() for msg in messages]\n    logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n\n    request_payload = {\n        \"model\": self.model_name,\n        \"messages\": mistral_messages,\n        **self.default_params,\n        **kwargs,\n    }\n\n    if tools:\n        logger.debug(f\"Adding {len(tools)} tools to request\")\n        request_payload[\"tools\"] = [tool.model_dump() for tool in tools]\n        request_payload[\"tool_choice\"] = \"auto\"\n\n    try:\n        logger.debug(\"Initiating Mistral streaming request\")\n        response = await self.client.chat.stream_async(**request_payload)\n        async for chunk in response:\n            yield await self._convert_to_sse_chunk(chunk)\n    except Exception as e:\n        logger.error(f\"Error in Mistral streaming: {str(e)}\", exc_info=True)\n        raise RuntimeError(f\"Mistral API streaming failed: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/mistral_ai_adapter/#src.llm.adapters.mistral_ai_adapter.MistralAIAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Generate SSE stream from a single text prompt.</p> <p>Converts a single prompt into a chat message and streams the response.</p> <p>Parameters:</p> Name Type Description Default <code>`prompt`</code> <code>str</code> <p>The text prompt to send to the model.</p> required <code>`**kwargs`</code> <p>Additional parameters to override defaults for this request.</p> required <p>Yields:</p> Type Description <code>AsyncGenerator[SSEChunk, None]</code> <p><code>SSEChunk</code>: Standardized chunks of the streaming response.</p> <p>Raises:</p> Type Description <code>`RuntimeError`</code> <p>If the streaming request fails.</p> Source code in <code>src/llm/adapters/mistral_ai_adapter.py</code> <pre><code>async def gen_sse_stream(\n        self,\n        prompt: str,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate SSE stream from a single text prompt.\n\n    Converts a single prompt into a chat message and streams the response.\n\n    Args:\n        `prompt` (str): The text prompt to send to the model.\n        `**kwargs`: Additional parameters to override defaults for this request.\n\n    Yields:\n        `SSEChunk`: Standardized chunks of the streaming response.\n\n    Raises:\n        `RuntimeError`: If the streaming request fails.\n    \"\"\"\n    logger.debug(f\"Converting single prompt to chat format: {prompt[:50]}...\")\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n        yield chunk\n</code></pre>"},{"location":"reference/llm/adapters/openai_adapter/","title":"OpenAI","text":""},{"location":"reference/llm/adapters/openai_adapter/#src.llm.adapters.openai_adapter.OpenAIAdapter","title":"<code>src.llm.adapters.openai_adapter.OpenAIAdapter</code>","text":"<p>               Bases: <code>BaseVendorAdapter</code></p> <p>Adapter for interacting with OpenAI's API.</p> <p>This class implements the BaseVendorAdapter interface for OpenAI's chat models, handling authentication, request formatting, and response streaming. It converts OpenAI-specific response formats into standardized SSE chunks for consistent handling across different LLM providers.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>OpenAI API key loaded from environment variables.</p> <code>client</code> <code>AsyncOpenAI</code> <p>Authenticated OpenAI client instance.</p> <code>model_name</code> <code>str</code> <p>The OpenAI model identifier (e.g., \"gpt-4\").</p> <code>default_params</code> <code>dict</code> <p>Default parameters for OpenAI API calls.</p> Source code in <code>src/llm/adapters/openai_adapter.py</code> <pre><code>class OpenAIAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with OpenAI's API.\n\n    This class implements the BaseVendorAdapter interface for OpenAI's chat models,\n    handling authentication, request formatting, and response streaming. It converts\n    OpenAI-specific response formats into standardized SSE chunks for consistent\n    handling across different LLM providers.\n\n    Attributes:\n        api_key (str): OpenAI API key loaded from environment variables.\n        client (AsyncOpenAI): Authenticated OpenAI client instance.\n        model_name (str): The OpenAI model identifier (e.g., \"gpt-4\").\n        default_params (dict): Default parameters for OpenAI API calls.\n    \"\"\"\n\n    def __init__(self, model_name: str, **default_params):\n        \"\"\"Initialize the OpenAI Adapter with model configuration.\n\n        Args:\n            model_name (str): The identifier of the OpenAI model to use (e.g., \"gpt-4\").\n            **default_params: Additional parameters to include in all API calls.\n                Common parameters include temperature, max_tokens, etc.\n\n        Raises:\n            ValueError: If OPENAI_API_KEY environment variable is not set.\n        \"\"\"\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Missing OpenAI API key. Set the OPENAI_API_KEY environment variable.\")\n\n        self.client = AsyncOpenAI()\n        self.client.api_key = self.api_key\n\n        self.model_name = model_name\n        self.default_params = default_params\n        logger.info(f\"OpenAI Adapter initialized with model: {self.model_name}\")\n        logger.debug(f\"Default parameters configured: {default_params}\")\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate SSE stream from a single text prompt.\n\n        Converts a single prompt into a chat message and streams the response.\n\n        Args:\n            prompt (str): The text prompt to send to the model.\n            **kwargs: Additional parameters to override defaults for this request.\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response.\n\n        Raises:\n            RuntimeError: If the streaming request fails.\n        \"\"\"\n        logger.debug(f\"Converting single prompt to chat format: {prompt[:50]}...\")\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n            yield chunk\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs,\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response from a sequence of messages.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages for context.\n            tools (Optional[List[Tool]]): List of tools available to the model.\n            **kwargs: Additional parameters to override defaults for this request.\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response.\n\n        Raises:\n            RuntimeError: If the OpenAI API request fails.\n        \"\"\"\n        openai_messages = [msg.model_dump() for msg in messages]\n        logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n\n        request_payload = {\n            \"model\": self.model_name,\n            \"messages\": openai_messages,\n            \"stream\": True,\n            **self.default_params,\n            **kwargs,\n        }\n\n        if tools:\n            logger.debug(f\"Adding {len(tools)} tools to request\")\n            request_payload[\"tools\"] = [tool.model_dump() for tool in tools]\n            request_payload[\"tool_choice\"] = \"auto\"\n\n        try:\n            logger.debug(\"Initiating OpenAI streaming request\")\n            async for chunk in await self.client.chat.completions.create(**request_payload):\n                yield await self._convert_to_sse_chunk(chunk)\n        except Exception as e:\n            logger.error(f\"Error in OpenAI streaming: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"OpenAI API streaming failed: {str(e)}\") from e\n\n    async def _convert_to_sse_chunk(self, raw_chunk: ChatCompletionChunk) -&gt; SSEChunk:\n        \"\"\"Convert OpenAI's response chunk to standardized SSE format.\n\n        Transforms OpenAI's ChatCompletionChunk into the application's\n        standardized SSEChunk format, handling all possible response fields\n        including tool calls, content, and metadata.\n\n        Args:\n            raw_chunk (ChatCompletionChunk): Raw chunk from OpenAI's API.\n\n        Returns:\n            SSEChunk: Standardized chunk format for consistent handling.\n\n        Raises:\n            ValueError: If chunk conversion fails due to unexpected format.\n        \"\"\"\n        try:\n            logger.debug(f\"Converting chunk ID: {raw_chunk.id}\")\n            choices = []\n            for choice in raw_chunk.choices:\n                tool_calls = None\n                if choice.delta.tool_calls:\n                    tool_calls = []\n                    for tc in choice.delta.tool_calls:\n                        function = None\n                        if tc.function:\n                            function = SSEFunction(\n                                name=\"\" if tc.function.name is None else tc.function.name,\n                                arguments=\"\" if tc.function.arguments is None else tc.function.arguments\n                            )\n\n                        tool_calls.append(SSEToolCall(\n                            index=tc.index if tc.index is not None else 0,\n                            id=tc.id,\n                            type=tc.type if tc.type else \"function\",\n                            function=function\n                        ))\n\n                delta = SSEDelta(\n                    role=choice.delta.role,\n                    content=choice.delta.content,\n                    tool_calls=tool_calls,\n                    refusal=choice.delta.refusal\n                )\n\n                choices.append(SSEChoice(\n                    index=choice.index,\n                    delta=delta,\n                    logprobs=choice.logprobs,\n                    finish_reason=choice.finish_reason\n                ))\n\n            return SSEChunk(\n                id=raw_chunk.id,\n                object=raw_chunk.object,\n                created=raw_chunk.created,\n                model=raw_chunk.model,\n                service_tier=raw_chunk.service_tier,\n                system_fingerprint=raw_chunk.system_fingerprint,\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting OpenAI chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert OpenAI response to SSEChunk: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/openai_adapter/#src.llm.adapters.openai_adapter.OpenAIAdapter.__init__","title":"<code>__init__(model_name, **default_params)</code>","text":"<p>Initialize the OpenAI Adapter with model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The identifier of the OpenAI model to use (e.g., \"gpt-4\").</p> required <code>**default_params</code> <p>Additional parameters to include in all API calls. Common parameters include temperature, max_tokens, etc.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If OPENAI_API_KEY environment variable is not set.</p> Source code in <code>src/llm/adapters/openai_adapter.py</code> <pre><code>def __init__(self, model_name: str, **default_params):\n    \"\"\"Initialize the OpenAI Adapter with model configuration.\n\n    Args:\n        model_name (str): The identifier of the OpenAI model to use (e.g., \"gpt-4\").\n        **default_params: Additional parameters to include in all API calls.\n            Common parameters include temperature, max_tokens, etc.\n\n    Raises:\n        ValueError: If OPENAI_API_KEY environment variable is not set.\n    \"\"\"\n    self.api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not self.api_key:\n        raise ValueError(\"Missing OpenAI API key. Set the OPENAI_API_KEY environment variable.\")\n\n    self.client = AsyncOpenAI()\n    self.client.api_key = self.api_key\n\n    self.model_name = model_name\n    self.default_params = default_params\n    logger.info(f\"OpenAI Adapter initialized with model: {self.model_name}\")\n    logger.debug(f\"Default parameters configured: {default_params}\")\n</code></pre>"},{"location":"reference/llm/adapters/openai_adapter/#src.llm.adapters.openai_adapter.OpenAIAdapter.gen_chat_sse_stream","title":"<code>gen_chat_sse_stream(messages, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate a streaming chat response from a sequence of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[TextChatMessage]</code> <p>List of chat messages for context.</p> required <code>tools</code> <code>Optional[List[Tool]]</code> <p>List of tools available to the model.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to override defaults for this request.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the OpenAI API request fails.</p> Source code in <code>src/llm/adapters/openai_adapter.py</code> <pre><code>async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        **kwargs,\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate a streaming chat response from a sequence of messages.\n\n    Args:\n        messages (List[TextChatMessage]): List of chat messages for context.\n        tools (Optional[List[Tool]]): List of tools available to the model.\n        **kwargs: Additional parameters to override defaults for this request.\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response.\n\n    Raises:\n        RuntimeError: If the OpenAI API request fails.\n    \"\"\"\n    openai_messages = [msg.model_dump() for msg in messages]\n    logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n\n    request_payload = {\n        \"model\": self.model_name,\n        \"messages\": openai_messages,\n        \"stream\": True,\n        **self.default_params,\n        **kwargs,\n    }\n\n    if tools:\n        logger.debug(f\"Adding {len(tools)} tools to request\")\n        request_payload[\"tools\"] = [tool.model_dump() for tool in tools]\n        request_payload[\"tool_choice\"] = \"auto\"\n\n    try:\n        logger.debug(\"Initiating OpenAI streaming request\")\n        async for chunk in await self.client.chat.completions.create(**request_payload):\n            yield await self._convert_to_sse_chunk(chunk)\n    except Exception as e:\n        logger.error(f\"Error in OpenAI streaming: {str(e)}\", exc_info=True)\n        raise RuntimeError(f\"OpenAI API streaming failed: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/openai_adapter/#src.llm.adapters.openai_adapter.OpenAIAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Generate SSE stream from a single text prompt.</p> <p>Converts a single prompt into a chat message and streams the response.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to send to the model.</p> required <code>**kwargs</code> <p>Additional parameters to override defaults for this request.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the streaming request fails.</p> Source code in <code>src/llm/adapters/openai_adapter.py</code> <pre><code>async def gen_sse_stream(\n        self,\n        prompt: str,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate SSE stream from a single text prompt.\n\n    Converts a single prompt into a chat message and streams the response.\n\n    Args:\n        prompt (str): The text prompt to send to the model.\n        **kwargs: Additional parameters to override defaults for this request.\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response.\n\n    Raises:\n        RuntimeError: If the streaming request fails.\n    \"\"\"\n    logger.debug(f\"Converting single prompt to chat format: {prompt[:50]}...\")\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n        yield chunk\n</code></pre>"},{"location":"reference/llm/adapters/openai_compat_adapter/","title":"OpenAI-Compatible","text":""},{"location":"reference/llm/adapters/openai_compat_adapter/#src.llm.adapters.openai_compat_adapter.OpenAICompatAdapter","title":"<code>src.llm.adapters.openai_compat_adapter.OpenAICompatAdapter</code>","text":"<p>               Bases: <code>BaseVendorAdapter</code></p> <p>Adapter for interacting with Open AI compatible APIs.</p> <p>Supports both chat completions and completions endpoints. Handles streaming responses and converts them to standardized SSE chunks.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The model identifier being served</p> <code>base_url</code> <code>str</code> <p>URL of the server (default: http://localhost:8000/v1)</p> <code>api_key</code> <code>str</code> <p>API key for server authentication</p> <code>client</code> <code>AsyncOpenAI</code> <p>Configured OpenAI client</p> Source code in <code>src/llm/adapters/openai_compat_adapter.py</code> <pre><code>class OpenAICompatAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with Open AI compatible APIs.\n\n    Supports both chat completions and completions endpoints. Handles streaming responses and converts\n    them to standardized SSE chunks.\n\n    Attributes:\n        model_name (str): The model identifier being served\n        base_url (str): URL of the server (default: http://localhost:8000/v1)\n        api_key (str): API key for server authentication\n        client (AsyncOpenAI): Configured OpenAI client\n    \"\"\"\n\n    def __init__(\n            self,\n            model_name: str,\n            base_url: str = \"http://localhost:8000/v1\",\n            api_key: str = \"dummy-key\",\n            **default_params\n    ):\n        \"\"\"Initialize the adapter.\n\n        Args:\n            model_name (str): Name of the model being served (e.g. \"NousResearch/Llama-2-7b\")\n            base_url (str): URL of the server\n            api_key (str): API key for authentication\n            **default_params: Additional parameters for generation (temperature etc.)\n        \"\"\"\n        self.model_name = model_name\n        self.base_url = base_url\n        self.api_key = api_key\n        self.default_params = default_params\n\n        # Configure OpenAI client for server\n        self.client = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key\n        )\n\n        logger.info(f\"Initialized adapter for model: {self.model_name}\")\n        logger.debug(f\"Using server at: {self.base_url}\")\n        logger.debug(f\"Default parameters: {default_params}\")\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming chat completion using chat endpoint.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages\n            tools (Optional[List[Tool]]): Optional tools/functions\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        try:\n            # Convert messages to OpenAI format\n            openai_messages = [msg.model_dump() for msg in messages]\n\n            # Prepare request payload\n            request_params = {\n                \"model\": self.model_name,\n                \"messages\": openai_messages,\n                \"stream\": True,\n                **self.default_params,\n                **kwargs\n            }\n\n            # Add tools if provided\n            if tools:\n                request_params[\"tools\"] = [tool.model_dump() for tool in tools]\n                request_params[\"tool_choice\"] = \"auto\"\n\n            # Stream response\n            async for chunk in await self.client.chat.completions.create(**request_params):\n                yield self._convert_to_sse_chunk(chunk)\n\n        except Exception as e:\n            logger.error(f\"Error in chat stream: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Chat completion failed: {str(e)}\") from e\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming completion using completions endpoint.\n\n        Args:\n            prompt (str): Input text prompt\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        try:\n            # Prepare request payload\n            request_params = {\n                \"model\": self.model_name,\n                \"prompt\": prompt,\n                \"stream\": True,\n                **self.default_params,\n                **kwargs\n            }\n\n            logger.debug(f\"Making completions request with prompt: {prompt[:50]}...\")\n\n            # Use completions endpoint directly\n            async for chunk in await self.client.completions.create(**request_params):\n                yield self._convert_to_sse_chunk(chunk)\n\n        except Exception as e:\n            logger.error(f\"Error in completion stream: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Completion failed: {str(e)}\") from e\n\n    def _convert_to_sse_chunk(self, raw_chunk) -&gt; SSEChunk:\n        \"\"\"Convert response chunk to standardized SSE format.\n\n        Args:\n            raw_chunk: Raw chunk from API\n\n        Returns:\n            SSEChunk: Standardized chunk format\n        \"\"\"\n        try:\n            choices = []\n\n            # Check if this is a text completion or chat completion by looking at the object type\n            if raw_chunk.object == 'text_completion':\n                # Handle text completion format\n                for choice in raw_chunk.choices:\n                    choices.append(SSEChoice(\n                        index=choice.index,\n                        delta=SSEDelta(\n                            content=choice.text,\n                            role=\"assistant\"\n                        ),\n                        finish_reason=choice.finish_reason\n                    ))\n            else:\n                # Handle chat completion format\n                for choice in raw_chunk.choices:\n                    tool_calls = None\n                    if hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:\n                        tool_calls = []\n                        for tc in choice.delta.tool_calls:\n                            function = None\n                            if tc.function:\n                                function = SSEFunction(\n                                    name=tc.function.name or \"\",\n                                    arguments=tc.function.arguments or \"\"\n                                )\n                            tool_calls.append(SSEToolCall(\n                                index=tc.index or 0,\n                                id=tc.id,\n                                type=tc.type or \"function\",\n                                function=function\n                            ))\n\n                    choices.append(SSEChoice(\n                        index=choice.index,\n                        delta=SSEDelta(\n                            role=choice.delta.role if hasattr(choice.delta, 'role') else None,\n                            content=choice.delta.content if hasattr(choice.delta, 'content') else None,\n                            tool_calls=tool_calls\n                        ),\n                        finish_reason=choice.finish_reason\n                    ))\n\n            return SSEChunk(\n                id=raw_chunk.id,\n                object=raw_chunk.object,\n                created=raw_chunk.created,\n                model=raw_chunk.model,\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert response: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/openai_compat_adapter/#src.llm.adapters.openai_compat_adapter.OpenAICompatAdapter.__init__","title":"<code>__init__(model_name, base_url='http://localhost:8000/v1', api_key='dummy-key', **default_params)</code>","text":"<p>Initialize the adapter.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model being served (e.g. \"NousResearch/Llama-2-7b\")</p> required <code>base_url</code> <code>str</code> <p>URL of the server</p> <code>'http://localhost:8000/v1'</code> <code>api_key</code> <code>str</code> <p>API key for authentication</p> <code>'dummy-key'</code> <code>**default_params</code> <p>Additional parameters for generation (temperature etc.)</p> <code>{}</code> Source code in <code>src/llm/adapters/openai_compat_adapter.py</code> <pre><code>def __init__(\n        self,\n        model_name: str,\n        base_url: str = \"http://localhost:8000/v1\",\n        api_key: str = \"dummy-key\",\n        **default_params\n):\n    \"\"\"Initialize the adapter.\n\n    Args:\n        model_name (str): Name of the model being served (e.g. \"NousResearch/Llama-2-7b\")\n        base_url (str): URL of the server\n        api_key (str): API key for authentication\n        **default_params: Additional parameters for generation (temperature etc.)\n    \"\"\"\n    self.model_name = model_name\n    self.base_url = base_url\n    self.api_key = api_key\n    self.default_params = default_params\n\n    # Configure OpenAI client for server\n    self.client = AsyncOpenAI(\n        base_url=self.base_url,\n        api_key=self.api_key\n    )\n\n    logger.info(f\"Initialized adapter for model: {self.model_name}\")\n    logger.debug(f\"Using server at: {self.base_url}\")\n    logger.debug(f\"Default parameters: {default_params}\")\n</code></pre>"},{"location":"reference/llm/adapters/openai_compat_adapter/#src.llm.adapters.openai_compat_adapter.OpenAICompatAdapter.gen_chat_sse_stream","title":"<code>gen_chat_sse_stream(messages, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate streaming chat completion using chat endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[TextChatMessage]</code> <p>List of chat messages</p> required <code>tools</code> <code>Optional[List[Tool]]</code> <p>Optional tools/functions</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to override defaults</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response</p> Source code in <code>src/llm/adapters/openai_compat_adapter.py</code> <pre><code>async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate streaming chat completion using chat endpoint.\n\n    Args:\n        messages (List[TextChatMessage]): List of chat messages\n        tools (Optional[List[Tool]]): Optional tools/functions\n        **kwargs: Additional parameters to override defaults\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response\n    \"\"\"\n    try:\n        # Convert messages to OpenAI format\n        openai_messages = [msg.model_dump() for msg in messages]\n\n        # Prepare request payload\n        request_params = {\n            \"model\": self.model_name,\n            \"messages\": openai_messages,\n            \"stream\": True,\n            **self.default_params,\n            **kwargs\n        }\n\n        # Add tools if provided\n        if tools:\n            request_params[\"tools\"] = [tool.model_dump() for tool in tools]\n            request_params[\"tool_choice\"] = \"auto\"\n\n        # Stream response\n        async for chunk in await self.client.chat.completions.create(**request_params):\n            yield self._convert_to_sse_chunk(chunk)\n\n    except Exception as e:\n        logger.error(f\"Error in chat stream: {str(e)}\", exc_info=True)\n        raise RuntimeError(f\"Chat completion failed: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/openai_compat_adapter/#src.llm.adapters.openai_compat_adapter.OpenAICompatAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Generate streaming completion using completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Input text prompt</p> required <code>**kwargs</code> <p>Additional parameters to override defaults</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response</p> Source code in <code>src/llm/adapters/openai_compat_adapter.py</code> <pre><code>async def gen_sse_stream(\n        self,\n        prompt: str,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate streaming completion using completions endpoint.\n\n    Args:\n        prompt (str): Input text prompt\n        **kwargs: Additional parameters to override defaults\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response\n    \"\"\"\n    try:\n        # Prepare request payload\n        request_params = {\n            \"model\": self.model_name,\n            \"prompt\": prompt,\n            \"stream\": True,\n            **self.default_params,\n            **kwargs\n        }\n\n        logger.debug(f\"Making completions request with prompt: {prompt[:50]}...\")\n\n        # Use completions endpoint directly\n        async for chunk in await self.client.completions.create(**request_params):\n            yield self._convert_to_sse_chunk(chunk)\n\n    except Exception as e:\n        logger.error(f\"Error in completion stream: {str(e)}\", exc_info=True)\n        raise RuntimeError(f\"Completion failed: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/xai_adapter/","title":"xAI","text":""},{"location":"reference/llm/adapters/xai_adapter/#src.llm.adapters.xai_adapter.XAIAdapter","title":"<code>src.llm.adapters.xai_adapter.XAIAdapter</code>","text":"<p>               Bases: <code>BaseVendorAdapter</code></p> <p>Adapter for interacting with xAI's API.</p> <p>Utilizes the OpenAI client for compatibility with xAI API endpoints. Supports streaming responses and converts them to standardized SSE chunks.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The model identifier being served</p> <code>api_key</code> <code>str</code> <p>xAI API key for authentication</p> <code>base_url</code> <code>str</code> <p>URL of the xAI API server</p> <code>client</code> <code>AsyncOpenAI</code> <p>Configured OpenAI-compatible client for xAI</p> Source code in <code>src/llm/adapters/xai_adapter.py</code> <pre><code>class XAIAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with xAI's API.\n\n    Utilizes the OpenAI client for compatibility with xAI API endpoints.\n    Supports streaming responses and converts them to standardized SSE chunks.\n\n    Attributes:\n        model_name (str): The model identifier being served\n        api_key (str): xAI API key for authentication\n        base_url (str): URL of the xAI API server\n        client (AsyncOpenAI): Configured OpenAI-compatible client for xAI\n    \"\"\"\n\n    def __init__(\n            self,\n            model_name: str,\n            base_url: str = \"https://api.x.ai/v1\",\n            api_key: str = None,\n            **default_params\n    ):\n        \"\"\"Initialize the xAI adapter.\n\n        Args:\n            model_name (str): Name of the xAI model to use\n            base_url (str): URL of the xAI API server\n            api_key (str): xAI API key for authentication\n            **default_params: Additional parameters for generation (temperature etc.)\n        \"\"\"\n        self.model_name = model_name\n        self.base_url = base_url\n\n        # Get API key from environment or parameter\n        self.api_key = api_key or os.getenv(\"XAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"xAI API key is required. Provide as parameter or set `XAI_API_KEY` environment variable.\")\n\n        self.default_params = default_params\n\n        # Configure OpenAI-compatible client for X.AI\n        self.client = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key\n        )\n\n        logger.info(f\"Initialized xAI adapter for model: {self.model_name}\")\n        logger.debug(f\"Using X.AI server at: {self.base_url}\")\n        logger.debug(f\"Default parameters: {default_params}\")\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming chat completion.\n\n        Uses xAI's chat completions endpoint with streaming enabled.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages\n            tools (Optional[List[Tool]]): Optional tools/functions definitions\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        try:\n            # Convert messages to OpenAI format\n            openai_messages = [msg.model_dump() for msg in messages]\n\n            # Prepare request payload\n            request_params = {\n                \"model\": self.model_name,\n                \"messages\": openai_messages,\n                \"stream\": True,\n                **self.default_params,\n                **kwargs\n            }\n\n            # Add tools if provided\n            if tools:\n                request_params[\"tools\"] = [tool.model_dump() for tool in tools]\n                request_params[\"tool_choice\"] = \"auto\"\n\n            # Stream response\n            async for chunk in await self.client.chat.completions.create(**request_params):\n                yield self._convert_to_sse_chunk(chunk)\n\n        except Exception as e:\n            logger.error(f\"Error in xAI chat stream: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"xAI chat completion failed: {str(e)}\") from e\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming completion from a text prompt.\n\n        For xAI, this simply converts the text prompt to a chat message and calls gen_chat_sse_stream.\n\n        Args:\n            prompt (str): Input text prompt\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        # Convert the prompt to a single user message\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        # Use the chat completions endpoint\n        async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n            yield chunk\n\n    def _convert_to_sse_chunk(self, raw_chunk) -&gt; SSEChunk:\n        \"\"\"Convert xAI API response chunk to standardized SSE format.\n\n        Args:\n            raw_chunk: Raw chunk from xAI API\n\n        Returns:\n            SSEChunk: Standardized chunk format\n        \"\"\"\n        try:\n            choices = []\n\n            # Handle chat completion format\n            for choice in raw_chunk.choices:\n                tool_calls = None\n\n                # Handle tool calls if present\n                if hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:\n                    tool_calls = []\n                    for tc in choice.delta.tool_calls:\n                        function = None\n                        if tc.function:\n                            function = SSEFunction(\n                                name=tc.function.name or \"\",\n                                arguments=tc.function.arguments or \"\"\n                            )\n                        tool_calls.append(SSEToolCall(\n                            index=tc.index or 0,\n                            id=tc.id,\n                            type=tc.type or \"function\",\n                            function=function\n                        ))\n\n                choices.append(SSEChoice(\n                    index=choice.index,\n                    delta=SSEDelta(\n                        role=choice.delta.role if hasattr(choice.delta, 'role') else None,\n                        content=choice.delta.content if hasattr(choice.delta, 'content') else None,\n                        tool_calls=tool_calls\n                    ),\n                    finish_reason=choice.finish_reason\n                ))\n\n            return SSEChunk(\n                id=raw_chunk.id,\n                object=raw_chunk.object,\n                created=raw_chunk.created,\n                model=raw_chunk.model,\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting xAI chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert xAI response: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/xai_adapter/#src.llm.adapters.xai_adapter.XAIAdapter.__init__","title":"<code>__init__(model_name, base_url='https://api.x.ai/v1', api_key=None, **default_params)</code>","text":"<p>Initialize the xAI adapter.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the xAI model to use</p> required <code>base_url</code> <code>str</code> <p>URL of the xAI API server</p> <code>'https://api.x.ai/v1'</code> <code>api_key</code> <code>str</code> <p>xAI API key for authentication</p> <code>None</code> <code>**default_params</code> <p>Additional parameters for generation (temperature etc.)</p> <code>{}</code> Source code in <code>src/llm/adapters/xai_adapter.py</code> <pre><code>def __init__(\n        self,\n        model_name: str,\n        base_url: str = \"https://api.x.ai/v1\",\n        api_key: str = None,\n        **default_params\n):\n    \"\"\"Initialize the xAI adapter.\n\n    Args:\n        model_name (str): Name of the xAI model to use\n        base_url (str): URL of the xAI API server\n        api_key (str): xAI API key for authentication\n        **default_params: Additional parameters for generation (temperature etc.)\n    \"\"\"\n    self.model_name = model_name\n    self.base_url = base_url\n\n    # Get API key from environment or parameter\n    self.api_key = api_key or os.getenv(\"XAI_API_KEY\")\n    if not self.api_key:\n        raise ValueError(\"xAI API key is required. Provide as parameter or set `XAI_API_KEY` environment variable.\")\n\n    self.default_params = default_params\n\n    # Configure OpenAI-compatible client for X.AI\n    self.client = AsyncOpenAI(\n        base_url=self.base_url,\n        api_key=self.api_key\n    )\n\n    logger.info(f\"Initialized xAI adapter for model: {self.model_name}\")\n    logger.debug(f\"Using X.AI server at: {self.base_url}\")\n    logger.debug(f\"Default parameters: {default_params}\")\n</code></pre>"},{"location":"reference/llm/adapters/xai_adapter/#src.llm.adapters.xai_adapter.XAIAdapter.gen_chat_sse_stream","title":"<code>gen_chat_sse_stream(messages, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate streaming chat completion.</p> <p>Uses xAI's chat completions endpoint with streaming enabled.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[TextChatMessage]</code> <p>List of chat messages</p> required <code>tools</code> <code>Optional[List[Tool]]</code> <p>Optional tools/functions definitions</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to override defaults</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response</p> Source code in <code>src/llm/adapters/xai_adapter.py</code> <pre><code>async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate streaming chat completion.\n\n    Uses xAI's chat completions endpoint with streaming enabled.\n\n    Args:\n        messages (List[TextChatMessage]): List of chat messages\n        tools (Optional[List[Tool]]): Optional tools/functions definitions\n        **kwargs: Additional parameters to override defaults\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response\n    \"\"\"\n    try:\n        # Convert messages to OpenAI format\n        openai_messages = [msg.model_dump() for msg in messages]\n\n        # Prepare request payload\n        request_params = {\n            \"model\": self.model_name,\n            \"messages\": openai_messages,\n            \"stream\": True,\n            **self.default_params,\n            **kwargs\n        }\n\n        # Add tools if provided\n        if tools:\n            request_params[\"tools\"] = [tool.model_dump() for tool in tools]\n            request_params[\"tool_choice\"] = \"auto\"\n\n        # Stream response\n        async for chunk in await self.client.chat.completions.create(**request_params):\n            yield self._convert_to_sse_chunk(chunk)\n\n    except Exception as e:\n        logger.error(f\"Error in xAI chat stream: {str(e)}\", exc_info=True)\n        raise RuntimeError(f\"xAI chat completion failed: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/xai_adapter/#src.llm.adapters.xai_adapter.XAIAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Generate streaming completion from a text prompt.</p> <p>For xAI, this simply converts the text prompt to a chat message and calls gen_chat_sse_stream.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Input text prompt</p> required <code>**kwargs</code> <p>Additional parameters to override defaults</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response</p> Source code in <code>src/llm/adapters/xai_adapter.py</code> <pre><code>async def gen_sse_stream(\n        self,\n        prompt: str,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate streaming completion from a text prompt.\n\n    For xAI, this simply converts the text prompt to a chat message and calls gen_chat_sse_stream.\n\n    Args:\n        prompt (str): Input text prompt\n        **kwargs: Additional parameters to override defaults\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response\n    \"\"\"\n    # Convert the prompt to a single user message\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    # Use the chat completions endpoint\n    async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n        yield chunk\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/","title":"WatsonX Adapters Documentation","text":""},{"location":"reference/llm/adapters/watsonx/#overview","title":"Overview","text":"<p>The WatsonX Adapters Module provides integration with IBM\u2019s WatsonX AI services. It includes configuration management, authentication handling, and an adapter for seamless interaction with WatsonX models.</p>"},{"location":"reference/llm/adapters/watsonx/#components","title":"Components","text":""},{"location":"reference/llm/adapters/watsonx/#watsonx-adapter","title":"WatsonX Adapter","text":"<ul> <li><code>WatsonXAdapter</code> - The primary adapter for communicating with WatsonX models. It abstracts API interactions and standardizes response handling.</li> </ul>"},{"location":"reference/llm/adapters/watsonx/#configuration-management","title":"Configuration Management","text":"<ul> <li><code>WatsonXConfig</code> - A configuration class that stores API keys, model parameters, and other WatsonX-specific settings.</li> </ul>"},{"location":"reference/llm/adapters/watsonx/#authentication-handling","title":"Authentication Handling","text":"<ul> <li><code>IBMTokenManager</code> - Manages authentication and token retrieval for accessing WatsonX services.</li> </ul>"},{"location":"reference/llm/adapters/watsonx/ibm_token_manager/","title":"Token Manager","text":""},{"location":"reference/llm/adapters/watsonx/ibm_token_manager/#src.llm.adapters.watsonx.ibm_token_manager.IBMTokenManager","title":"<code>src.llm.adapters.watsonx.ibm_token_manager.IBMTokenManager</code>","text":"<p>Manages IBM Cloud OAuth2 token lifecycle for WatsonX API access.</p> <p>This class handles authentication token management for IBM Cloud services, including automatic token refresh and thread-safe token access. It implements a singleton pattern to maintain one token instance across the application.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>IBM Cloud API key for authentication.</p> <code>token_url</code> <code>str</code> <p>IBM IAM authentication endpoint URL.</p> <code>refresh_buffer</code> <code>int</code> <p>Time buffer in seconds before token expiry to trigger refresh.</p> <code>access_token</code> <code>Optional[str]</code> <p>Current valid access token.</p> <code>expiry_time</code> <code>float</code> <p>Unix timestamp when the current token expires.</p> <code>lock</code> <code>Lock</code> <p>Async lock for thread-safe token refresh operations.</p> Source code in <code>src/llm/adapters/watsonx/ibm_token_manager.py</code> <pre><code>class IBMTokenManager:\n    \"\"\"Manages IBM Cloud OAuth2 token lifecycle for WatsonX API access.\n\n    This class handles authentication token management for IBM Cloud services,\n    including automatic token refresh and thread-safe token access. It implements\n    a singleton pattern to maintain one token instance across the application.\n\n    Attributes:\n        api_key (str): IBM Cloud API key for authentication.\n        token_url (str): IBM IAM authentication endpoint URL.\n        refresh_buffer (int): Time buffer in seconds before token expiry to trigger refresh.\n        access_token (Optional[str]): Current valid access token.\n        expiry_time (float): Unix timestamp when the current token expires.\n        lock (asyncio.Lock): Async lock for thread-safe token refresh operations.\n    \"\"\"\n\n    def __init__(self, api_key: str, refresh_buffer: int = 60):\n        \"\"\"Initialize the IBM Token Manager.\n\n        Args:\n            api_key (str): IBM WatsonX API key for authentication.\n            refresh_buffer (int, optional): Buffer time in seconds before token expiry\n                to trigger a refresh. Defaults to 60 seconds.\n\n        Raises:\n            ValueError: If api_key is empty or None.\n            EnvironmentError: If IBM_AUTH_URL environment variable is not set.\n        \"\"\"\n        if not api_key:\n            raise ValueError(\"API key cannot be empty or None\")\n\n        self.api_key = api_key\n        self.token_url = os.getenv(\"IBM_AUTH_URL\")\n        if not self.token_url:\n            raise EnvironmentError(\"IBM_AUTH_URL environment variable not set\")\n\n        self.refresh_buffer = refresh_buffer\n        self.access_token: Optional[str] = None\n        self.expiry_time: float = 0\n        self.lock = asyncio.Lock()\n\n        logger.debug(\"Initialized IBMTokenManager with refresh buffer of %d seconds\", refresh_buffer)\n\n    async def _is_token_expired(self) -&gt; bool:\n        \"\"\"Check if the current token is expired or approaching expiry.\n\n        Returns:\n            bool: True if token is expired or will expire soon, False otherwise.\n        \"\"\"\n        current_time = time.time()\n        is_expired = self.access_token is None or current_time &gt; (self.expiry_time - self.refresh_buffer)\n\n        if is_expired:\n            logger.debug(\"Token is expired or approaching expiry\")\n        return is_expired\n\n    async def _refresh_token(self) -&gt; None:\n        \"\"\"Fetch a new OAuth token from IBM IAM.\n\n        Raises:\n            aiohttp.ClientError: If the token refresh request fails.\n            ValueError: If the response doesn't contain expected token information.\n            Exception: For any other unexpected errors during token refresh.\n        \"\"\"\n        async with self.lock:\n            if not await self._is_token_expired():\n                logger.debug(\"Token refresh skipped - current token still valid\")\n                return\n\n            logger.debug(\"Starting token refresh process\")\n            try:\n                headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n                payload = {\n                    \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n                    \"apikey\": self.api_key\n                }\n\n                async with aiohttp.ClientSession() as session:\n                    logger.debug(\"Making token refresh request to IBM IAM\")\n                    async with session.post(self.token_url, headers=headers, data=payload) as response:\n                        response.raise_for_status()\n                        token_info = await response.json()\n\n                        if \"access_token\" not in token_info or \"expires_in\" not in token_info:\n                            raise ValueError(\"Invalid token response from IBM IAM\")\n\n                        self.access_token = token_info[\"access_token\"]\n                        expires_in = int(token_info[\"expires_in\"])\n                        self.expiry_time = time.time() + expires_in\n\n                        logger.info(\"Successfully refreshed IBM token. Expires in %d seconds\", expires_in)\n                        logger.debug(\"Token expiry time set to %s\", time.ctime(self.expiry_time))\n\n            except aiohttp.ClientError as e:\n                logger.error(\"HTTP error during token refresh: %s\", str(e))\n                self.access_token = None\n                raise\n            except Exception as e:\n                logger.error(\"Unexpected error during token refresh: %s\", str(e))\n                self.access_token = None\n                raise\n\n    async def get_token(self) -&gt; Optional[str]:\n        \"\"\"Retrieve a valid access token, refreshing if necessary.\n\n        Returns:\n            Optional[str]: Valid access token if successful, None if token\n                refresh fails.\n\n        Raises:\n            Exception: If token refresh fails when attempted.\n        \"\"\"\n        logger.debug(\"Token requested\")\n        if await self._is_token_expired():\n            logger.debug(\"Token refresh needed before returning\")\n            await self._refresh_token()\n        return self.access_token\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/ibm_token_manager/#src.llm.adapters.watsonx.ibm_token_manager.IBMTokenManager.__init__","title":"<code>__init__(api_key, refresh_buffer=60)</code>","text":"<p>Initialize the IBM Token Manager.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>IBM WatsonX API key for authentication.</p> required <code>refresh_buffer</code> <code>int</code> <p>Buffer time in seconds before token expiry to trigger a refresh. Defaults to 60 seconds.</p> <code>60</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If api_key is empty or None.</p> <code>EnvironmentError</code> <p>If IBM_AUTH_URL environment variable is not set.</p> Source code in <code>src/llm/adapters/watsonx/ibm_token_manager.py</code> <pre><code>def __init__(self, api_key: str, refresh_buffer: int = 60):\n    \"\"\"Initialize the IBM Token Manager.\n\n    Args:\n        api_key (str): IBM WatsonX API key for authentication.\n        refresh_buffer (int, optional): Buffer time in seconds before token expiry\n            to trigger a refresh. Defaults to 60 seconds.\n\n    Raises:\n        ValueError: If api_key is empty or None.\n        EnvironmentError: If IBM_AUTH_URL environment variable is not set.\n    \"\"\"\n    if not api_key:\n        raise ValueError(\"API key cannot be empty or None\")\n\n    self.api_key = api_key\n    self.token_url = os.getenv(\"IBM_AUTH_URL\")\n    if not self.token_url:\n        raise EnvironmentError(\"IBM_AUTH_URL environment variable not set\")\n\n    self.refresh_buffer = refresh_buffer\n    self.access_token: Optional[str] = None\n    self.expiry_time: float = 0\n    self.lock = asyncio.Lock()\n\n    logger.debug(\"Initialized IBMTokenManager with refresh buffer of %d seconds\", refresh_buffer)\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/ibm_token_manager/#src.llm.adapters.watsonx.ibm_token_manager.IBMTokenManager.get_token","title":"<code>get_token()</code>  <code>async</code>","text":"<p>Retrieve a valid access token, refreshing if necessary.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: Valid access token if successful, None if token refresh fails.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If token refresh fails when attempted.</p> Source code in <code>src/llm/adapters/watsonx/ibm_token_manager.py</code> <pre><code>async def get_token(self) -&gt; Optional[str]:\n    \"\"\"Retrieve a valid access token, refreshing if necessary.\n\n    Returns:\n        Optional[str]: Valid access token if successful, None if token\n            refresh fails.\n\n    Raises:\n        Exception: If token refresh fails when attempted.\n    \"\"\"\n    logger.debug(\"Token requested\")\n    if await self._is_token_expired():\n        logger.debug(\"Token refresh needed before returning\")\n        await self._refresh_token()\n    return self.access_token\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/watsonx_adapter/","title":"Adapter","text":""},{"location":"reference/llm/adapters/watsonx/watsonx_adapter/#src.llm.adapters.watsonx.watsonx_adapter.WatsonXAdapter","title":"<code>src.llm.adapters.watsonx.watsonx_adapter.WatsonXAdapter</code>","text":"<p>               Bases: <code>BaseVendorAdapter</code></p> <p>Adapter for interacting with IBM WatsonX's API.</p> <p>This class implements the BaseVendorAdapter interface for WatsonX language models, handling authentication, streaming requests, and response parsing. It converts WatsonX-specific formats into standardized SSE chunks for consistent handling across different LLM providers.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>The WatsonX model identifier.</p> <code>model_params</code> <code>dict</code> <p>Default parameters for model requests.</p> <code>token_manager</code> <code>IBMTokenManager</code> <p>Manager for IBM Cloud authentication tokens.</p> <code>project_id</code> <code>str</code> <p>WatsonX project identifier.</p> <code>base_url</code> <code>str</code> <p>Base URL for WatsonX API endpoints.</p> <code>timeout</code> <code>ClientTimeout</code> <p>Timeout configuration for requests.</p> <code>_session</code> <code>Optional[ClientSession]</code> <p>Reusable HTTP session.</p> Source code in <code>src/llm/adapters/watsonx/watsonx_adapter.py</code> <pre><code>class WatsonXAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with IBM WatsonX's API.\n\n    This class implements the BaseVendorAdapter interface for WatsonX language models,\n    handling authentication, streaming requests, and response parsing. It converts\n    WatsonX-specific formats into standardized SSE chunks for consistent handling\n    across different LLM providers.\n\n    Attributes:\n        model_id (str): The WatsonX model identifier.\n        model_params (dict): Default parameters for model requests.\n        token_manager (IBMTokenManager): Manager for IBM Cloud authentication tokens.\n        project_id (str): WatsonX project identifier.\n        base_url (str): Base URL for WatsonX API endpoints.\n        timeout (aiohttp.ClientTimeout): Timeout configuration for requests.\n        _session (Optional[aiohttp.ClientSession]): Reusable HTTP session.\n    \"\"\"\n\n    def __init__(self,\n                 model_name: str,\n                 token_manager: IBMTokenManager,\n                 timeout: Optional[aiohttp.ClientTimeout] = None,\n                 **model_params):\n        \"\"\"Initialize the WatsonX Adapter with model configuration.\n\n        Args:\n            model_name (str): The identifier of the WatsonX model to use.\n            token_manager (IBMTokenManager): Manager for handling IBM authentication.\n            timeout (Optional[aiohttp.ClientTimeout]): Custom timeout configuration.\n            **model_params: Additional parameters to include in model requests.\n\n        Raises:\n            ValueError: If required configuration is missing.\n        \"\"\"\n        self.model_id = model_name\n        self.model_params = model_params or {}\n        self.token_manager = token_manager\n        self.project_id = WatsonXConfig.PROJECT_ID\n        self.base_url = \"https://us-south.ml.cloud.ibm.com/ml/v1/text\"\n        self._session: Optional[aiohttp.ClientSession] = None\n        self.timeout = timeout or DEFAULT_TIMEOUT\n        logger.info(f\"WatsonX Adapter initialized with model: {self.model_id}\")\n        logger.debug(f\"Model parameters configured: {model_params}\")\n        logger.debug(f\"Timeout configuration: {self.timeout}\")\n\n    @asynccontextmanager\n    async def _session_context(self):\n        \"\"\"Manage the lifecycle of an HTTP session.\n\n        Yields:\n            aiohttp.ClientSession: Active HTTP session for making requests.\n        \"\"\"\n        logger.debug(\"Creating new HTTP session\")\n        session = aiohttp.ClientSession(timeout=self.timeout)\n        try:\n            yield session\n        finally:\n            await session.close()\n            logger.debug(\"HTTP session closed\")\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            timeout: Optional[aiohttp.ClientTimeout] = None,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response from a sequence of messages.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages for context.\n            tools (Optional[List[Tool]]): List of tools available to the model.\n            timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n            **kwargs: Additional parameters to override defaults.\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response.\n\n        Raises:\n            RuntimeError: If the WatsonX API request fails.\n            TimeoutError: If the request times out.\n        \"\"\"\n        logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n        serialized_messages = [msg.model_dump() for msg in messages]\n        serialized_tools = [tool.model_dump() for tool in tools] if tools else None\n\n        payload = {\n            \"model_id\": self.model_id,\n            \"project_id\": self.project_id,\n            \"messages\": serialized_messages,\n            **kwargs\n        }\n        if serialized_tools:\n            logger.debug(f\"Adding {len(serialized_tools)} tools to request\")\n            payload[\"tools\"] = serialized_tools\n\n        async for raw_chunk in self._make_sse_request(\"chat_stream\", payload, timeout):\n            sse_chunk = self._convert_to_sse_chunk(raw_chunk)\n            yield sse_chunk\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            timeout: Optional[aiohttp.ClientTimeout] = None,\n            **kwargs\n    ) -&gt; AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate text using WatsonX's generation_stream endpoint.\n\n        Args:\n            prompt (str): The input text prompt.\n            timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n            **kwargs: Additional parameters to pass to the API.\n\n        Yields:\n            SSEChunk: Server-sent event chunks containing generated text.\n\n        Raises:\n            RuntimeError: If the streaming request fails.\n            TimeoutError: If the request times out.\n        \"\"\"\n        logger.debug(f\"Processing generation stream request. Prompt: {prompt[:50]}...\")\n        payload = {\n            \"model_id\": self.model_id,\n            \"project_id\": self.project_id,\n            \"input\": prompt,\n            \"parameters\": {\n                **self.model_params,\n                **kwargs\n            }\n        }\n\n        async for raw_chunk in self._make_sse_request(\"generation_stream\", payload, timeout):\n            sse_chunk = self._convert_to_sse_chunk(raw_chunk)\n            yield sse_chunk\n\n    @retry(\n        stop=stop_after_attempt(MAX_RETRIES),\n        wait=wait_exponential(multiplier=MIN_RETRY_WAIT, max=MAX_RETRY_WAIT),\n        retry=retry_if_exception_type((ClientError, TimeoutError)),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        reraise=True\n    )\n    async def _make_sse_request(self,\n                                endpoint: str,\n                                payload: Dict[str, Any],\n                                timeout: Optional[aiohttp.ClientTimeout] = None) \\\n            -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Make a streaming request to WatsonX API with retry logic.\n\n        Args:\n            endpoint (str): API endpoint to call.\n            payload (Dict[str, Any]): Request payload data.\n            timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n\n        Yields:\n            Dict[str, Any]: Raw response chunks from the API.\n\n        Raises:\n            aiohttp.ClientError: If all retry attempts fail with HTTP errors.\n            ValueError: If response parsing fails.\n            TimeoutError: If all retry attempts timeout.\n            Exception: If all retry attempts fail for other reasons.\n        \"\"\"\n        token = await self.token_manager.get_token()\n        url = f\"{self.base_url}/{endpoint}?version=2023-05-29\"\n        headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n        }\n\n        logger.debug(f\"Making request to endpoint: {endpoint}\")\n        logger.debug(f\"Request payload: {json.dumps(payload, indent=2)}\")\n\n        try:\n            async with self._session_context() as session:\n                async with session.post(url,\n                                        json=payload,\n                                        headers=headers,\n                                        timeout=timeout or self.timeout) as resp:\n                    resp.raise_for_status()\n                    logger.debug(f\"Stream connected, status: {resp.status}\")\n\n                    buffer = []\n                    async for raw_line in resp.content:\n                        line = raw_line.decode(\"utf-8\").strip()\n\n                        if not line:\n                            event_data = self._parse_sse_event(buffer)\n                            buffer = []\n\n                            if \"data\" in event_data:\n                                try:\n                                    data_parsed = json.loads(event_data[\"data\"])\n                                    yield data_parsed\n                                except json.JSONDecodeError:\n                                    logger.warning(f\"Skipping invalid SSE data: {event_data['data']}\")\n                            continue\n\n                        buffer.append(line)\n\n        except aiohttp.ClientError as e:\n            logger.error(f\"HTTP request failed: {str(e)}\", exc_info=True)\n            raise\n        except asyncio.TimeoutError as e:\n            logger.error(f\"Request timed out: {str(e)}\", exc_info=True)\n            raise TimeoutError(f\"Request to {endpoint} timed out\") from e\n\n    def _parse_sse_event(self, lines: List[str]) -&gt; Dict[str, str]:\n        \"\"\"Parse Server-Sent Events format into structured data.\n\n        Args:\n            lines (List[str]): Raw SSE message lines.\n\n        Returns:\n            Dict[str, str]: Parsed event data.\n        \"\"\"\n        event = {}\n        for line in lines:\n            if line.startswith(\"id:\"):\n                event[\"id\"] = line[len(\"id:\"):].strip()\n            elif line.startswith(\"event:\"):\n                event[\"event\"] = line[len(\"event:\"):].strip()\n            elif line.startswith(\"data:\"):\n                data_str = line[len(\"data:\"):].strip()\n                event[\"data\"] = event.get(\"data\", \"\") + data_str\n        return event\n\n    def _convert_to_sse_chunk(self, raw_chunk: dict) -&gt; SSEChunk:\n        \"\"\"Convert WatsonX response format to standardized SSE chunk.\n\n        Handles both generation_stream and chat_stream response formats.\n\n        Args:\n            raw_chunk (dict): Raw response data from WatsonX.\n\n        Returns:\n            SSEChunk: Standardized chunk format.\n\n        Raises:\n            ValueError: If chunk conversion fails.\n        \"\"\"\n        try:\n            logger.debug(f\"Converting chunk: {json.dumps(raw_chunk, indent=2)}\")\n            # Handle generation_stream format\n            if \"results\" in raw_chunk:\n                result = raw_chunk[\"results\"][0]\n                choices = [\n                    SSEChoice(\n                        index=0,\n                        delta=SSEDelta(\n                            content=result.get(\"generated_text\"),\n                            role=\"assistant\"\n                        ),\n                        logprobs=None,\n                        finish_reason=result.get(\"stop_reason\")\n                    )\n                ]\n            # Handle chat_stream format\n            else:\n                choices = []\n                for choice_dict in raw_chunk.get('choices', []):\n                    delta_data = choice_dict.get('delta', {})\n\n                    tool_calls = None\n                    if \"tool_calls\" in delta_data:\n                        tool_calls = [\n                            SSEToolCall(\n                                index=tc.get(\"index\", 0),\n                                id=tc.get(\"id\"),\n                                type=tc.get(\"type\", \"function\"),\n                                function=SSEFunction(\n                                    name=tc[\"function\"][\"name\"],\n                                    arguments=tc[\"function\"].get(\"arguments\", \"\")\n                                ) if tc.get(\"function\") else None\n                            ) for tc in delta_data[\"tool_calls\"]\n                        ]\n\n                    delta = SSEDelta(\n                        role=delta_data.get(\"role\"),\n                        content=delta_data.get(\"content\"),\n                        tool_calls=tool_calls,\n                        refusal=delta_data.get(\"refusal\"),\n                        status=delta_data.get(\"status\"),\n                        metadata=delta_data.get(\"metadata\")\n                    )\n\n                    choices.append(SSEChoice(\n                        index=choice_dict.get(\"index\", 0),\n                        delta=delta,\n                        logprobs=choice_dict.get(\"logprobs\"),\n                        finish_reason=choice_dict.get(\"finish_reason\")\n                    ))\n\n            return SSEChunk(\n                id=raw_chunk.get(\"id\", f\"watsonx-{int(time.time())}\"),\n                object=raw_chunk.get(\"object\", \"chat.completion.chunk\"),\n                created=raw_chunk.get(\"created\", int(time.time())),\n                model=raw_chunk.get(\"model\", self.model_id),\n                choices=choices\n            )\n        except Exception as e:\n            logger.error(f\"Error converting WatsonX chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert WatsonX response to SSEChunk: {str(e)}\") from e\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/watsonx_adapter/#src.llm.adapters.watsonx.watsonx_adapter.WatsonXAdapter.__init__","title":"<code>__init__(model_name, token_manager, timeout=None, **model_params)</code>","text":"<p>Initialize the WatsonX Adapter with model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The identifier of the WatsonX model to use.</p> required <code>token_manager</code> <code>IBMTokenManager</code> <p>Manager for handling IBM authentication.</p> required <code>timeout</code> <code>Optional[ClientTimeout]</code> <p>Custom timeout configuration.</p> <code>None</code> <code>**model_params</code> <p>Additional parameters to include in model requests.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required configuration is missing.</p> Source code in <code>src/llm/adapters/watsonx/watsonx_adapter.py</code> <pre><code>def __init__(self,\n             model_name: str,\n             token_manager: IBMTokenManager,\n             timeout: Optional[aiohttp.ClientTimeout] = None,\n             **model_params):\n    \"\"\"Initialize the WatsonX Adapter with model configuration.\n\n    Args:\n        model_name (str): The identifier of the WatsonX model to use.\n        token_manager (IBMTokenManager): Manager for handling IBM authentication.\n        timeout (Optional[aiohttp.ClientTimeout]): Custom timeout configuration.\n        **model_params: Additional parameters to include in model requests.\n\n    Raises:\n        ValueError: If required configuration is missing.\n    \"\"\"\n    self.model_id = model_name\n    self.model_params = model_params or {}\n    self.token_manager = token_manager\n    self.project_id = WatsonXConfig.PROJECT_ID\n    self.base_url = \"https://us-south.ml.cloud.ibm.com/ml/v1/text\"\n    self._session: Optional[aiohttp.ClientSession] = None\n    self.timeout = timeout or DEFAULT_TIMEOUT\n    logger.info(f\"WatsonX Adapter initialized with model: {self.model_id}\")\n    logger.debug(f\"Model parameters configured: {model_params}\")\n    logger.debug(f\"Timeout configuration: {self.timeout}\")\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/watsonx_adapter/#src.llm.adapters.watsonx.watsonx_adapter.WatsonXAdapter.gen_chat_sse_stream","title":"<code>gen_chat_sse_stream(messages, tools=None, timeout=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate a streaming chat response from a sequence of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[TextChatMessage]</code> <p>List of chat messages for context.</p> required <code>tools</code> <code>Optional[List[Tool]]</code> <p>List of tools available to the model.</p> <code>None</code> <code>timeout</code> <code>Optional[ClientTimeout]</code> <p>Optional request-specific timeout.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to override defaults.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Standardized chunks of the streaming response.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the WatsonX API request fails.</p> <code>TimeoutError</code> <p>If the request times out.</p> Source code in <code>src/llm/adapters/watsonx/watsonx_adapter.py</code> <pre><code>async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        timeout: Optional[aiohttp.ClientTimeout] = None,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate a streaming chat response from a sequence of messages.\n\n    Args:\n        messages (List[TextChatMessage]): List of chat messages for context.\n        tools (Optional[List[Tool]]): List of tools available to the model.\n        timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n        **kwargs: Additional parameters to override defaults.\n\n    Yields:\n        SSEChunk: Standardized chunks of the streaming response.\n\n    Raises:\n        RuntimeError: If the WatsonX API request fails.\n        TimeoutError: If the request times out.\n    \"\"\"\n    logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n    serialized_messages = [msg.model_dump() for msg in messages]\n    serialized_tools = [tool.model_dump() for tool in tools] if tools else None\n\n    payload = {\n        \"model_id\": self.model_id,\n        \"project_id\": self.project_id,\n        \"messages\": serialized_messages,\n        **kwargs\n    }\n    if serialized_tools:\n        logger.debug(f\"Adding {len(serialized_tools)} tools to request\")\n        payload[\"tools\"] = serialized_tools\n\n    async for raw_chunk in self._make_sse_request(\"chat_stream\", payload, timeout):\n        sse_chunk = self._convert_to_sse_chunk(raw_chunk)\n        yield sse_chunk\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/watsonx_adapter/#src.llm.adapters.watsonx.watsonx_adapter.WatsonXAdapter.gen_sse_stream","title":"<code>gen_sse_stream(prompt, timeout=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using WatsonX's generation_stream endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt.</p> required <code>timeout</code> <code>Optional[ClientTimeout]</code> <p>Optional request-specific timeout.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to pass to the API.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>SSEChunk</code> <code>AsyncGenerator[SSEChunk, None]</code> <p>Server-sent event chunks containing generated text.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the streaming request fails.</p> <code>TimeoutError</code> <p>If the request times out.</p> Source code in <code>src/llm/adapters/watsonx/watsonx_adapter.py</code> <pre><code>async def gen_sse_stream(\n        self,\n        prompt: str,\n        timeout: Optional[aiohttp.ClientTimeout] = None,\n        **kwargs\n) -&gt; AsyncGenerator[SSEChunk, None]:\n    \"\"\"Generate text using WatsonX's generation_stream endpoint.\n\n    Args:\n        prompt (str): The input text prompt.\n        timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n        **kwargs: Additional parameters to pass to the API.\n\n    Yields:\n        SSEChunk: Server-sent event chunks containing generated text.\n\n    Raises:\n        RuntimeError: If the streaming request fails.\n        TimeoutError: If the request times out.\n    \"\"\"\n    logger.debug(f\"Processing generation stream request. Prompt: {prompt[:50]}...\")\n    payload = {\n        \"model_id\": self.model_id,\n        \"project_id\": self.project_id,\n        \"input\": prompt,\n        \"parameters\": {\n            **self.model_params,\n            **kwargs\n        }\n    }\n\n    async for raw_chunk in self._make_sse_request(\"generation_stream\", payload, timeout):\n        sse_chunk = self._convert_to_sse_chunk(raw_chunk)\n        yield sse_chunk\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/watsonx_config/","title":"Config","text":""},{"location":"reference/llm/adapters/watsonx/watsonx_config/#src.llm.adapters.watsonx.watsonx_config.WatsonXConfig","title":"<code>src.llm.adapters.watsonx.watsonx_config.WatsonXConfig</code>","text":"<p>Configuration management for WatsonX credentials and settings.</p> <p>This class handles loading and validation of required WatsonX credentials from environment variables.</p> <p>Attributes:</p> Name Type Description <code>CREDS</code> <code>dict</code> <p>Dictionary containing API key and URL for WatsonX.</p> <code>PROJECT_ID</code> <code>str</code> <p>WatsonX project identifier.</p> Example <pre><code># Validate credentials before use\nWatsonXConfig.validate_credentials()\n\n# Access credentials\ncredentials = WatsonXConfig.CREDS\nproject_id = WatsonXConfig.PROJECT_ID\n</code></pre> Source code in <code>src/llm/adapters/watsonx/watsonx_config.py</code> <pre><code>class WatsonXConfig:\n    \"\"\"Configuration management for WatsonX credentials and settings.\n\n    This class handles loading and validation of required WatsonX credentials\n    from environment variables.\n\n    Attributes:\n        CREDS (dict): Dictionary containing API key and URL for WatsonX.\n        PROJECT_ID (str): WatsonX project identifier.\n\n    Example:\n        ```python\n        # Validate credentials before use\n        WatsonXConfig.validate_credentials()\n\n        # Access credentials\n        credentials = WatsonXConfig.CREDS\n        project_id = WatsonXConfig.PROJECT_ID\n        ```\n    \"\"\"\n    CREDS = {'apikey': os.getenv(\"WXAI_API_KEY\"), 'url': os.getenv(\"WXAI_URL\")}\n    PROJECT_ID = os.getenv(\"WXAI_PROJECT_ID\")\n\n    @classmethod\n    def validate_credentials(cls):\n        \"\"\"Validate the presence of required WatsonX credentials.\n\n        Checks for the presence of all required credentials and logs appropriate\n        messages for missing values.\n\n        Raises:\n            ValueError: If any required credential is missing.\n        \"\"\"\n        missing = [key for key, value in {\n            'WXAI_API_KEY': cls.CREDS['apikey'],\n            'WXAI_URL': cls.CREDS['url'],\n            'WXAI_PROJECT_ID': cls.PROJECT_ID\n        }.items() if not value]\n\n        if missing:\n            logger.error(f\"Missing WatsonX credentials: {', '.join(missing)}\")\n            raise ValueError(f\"Missing WatsonX credentials: {', '.join(missing)}\")\n\n        logger.info(\"All WatsonX credentials loaded successfully.\")\n</code></pre>"},{"location":"reference/llm/adapters/watsonx/watsonx_config/#src.llm.adapters.watsonx.watsonx_config.WatsonXConfig.validate_credentials","title":"<code>validate_credentials()</code>  <code>classmethod</code>","text":"<p>Validate the presence of required WatsonX credentials.</p> <p>Checks for the presence of all required credentials and logs appropriate messages for missing values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any required credential is missing.</p> Source code in <code>src/llm/adapters/watsonx/watsonx_config.py</code> <pre><code>@classmethod\ndef validate_credentials(cls):\n    \"\"\"Validate the presence of required WatsonX credentials.\n\n    Checks for the presence of all required credentials and logs appropriate\n    messages for missing values.\n\n    Raises:\n        ValueError: If any required credential is missing.\n    \"\"\"\n    missing = [key for key, value in {\n        'WXAI_API_KEY': cls.CREDS['apikey'],\n        'WXAI_URL': cls.CREDS['url'],\n        'WXAI_PROJECT_ID': cls.PROJECT_ID\n    }.items() if not value]\n\n    if missing:\n        logger.error(f\"Missing WatsonX credentials: {', '.join(missing)}\")\n        raise ValueError(f\"Missing WatsonX credentials: {', '.join(missing)}\")\n\n    logger.info(\"All WatsonX credentials loaded successfully.\")\n</code></pre>"},{"location":"reference/llm/pattern_detection/","title":"Pattern Detection Documentation","text":""},{"location":"reference/llm/pattern_detection/#overview","title":"Overview","text":"<p>The Pattern Detection Module provides utilities for detecting predefined patterns within text streams. This module leverages the Aho-Corasick algorithm \u2014 an efficient string-searching technique that matches multiple patterns simultaneously in linear time.</p>"},{"location":"reference/llm/pattern_detection/#core-components","title":"Core Components","text":"Automaton Classes Processor Classes <code>AhoCorasickAutomaton</code>\u2713 Trie-based pattern matching engine\u2713 Linear-time complexity\u2713 Simultaneous multi-pattern search <code>BaseBufferedProcessor</code>\u2713 Abstract base for text streaming\u2713 Buffer management\u2713 Chunk-based processing <code>AhoCorasickAutomatonNormalized</code>\u2713 Whitespace-insensitive matching\u2713 Pattern normalization\u2713 Original-to-normalized index mapping <code>AhoCorasickBufferedProcessor</code>\u2713 Exact pattern matching\u2713 YAML-configurable patterns\u2713 Streaming-ready implementation <code>AhoCorasickBufferedProcessorNormalized</code>\u2713 Whitespace-invariant detection\u2713 Flexible text matching\u2713 Preserves original text positions"},{"location":"reference/llm/pattern_detection/#utility-functions","title":"Utility Functions","text":"<ul> <li><code>load_patterns(yaml_path)</code> \u2014 Loads patterns from YAML configuration</li> <li><code>normalize_and_map(text)</code> \u2014 Removes whitespace while tracking original character positions</li> </ul>"},{"location":"reference/llm/pattern_detection/#how-it-works","title":"How It Works","text":"<p>The module processes text in two primary stages:</p> <ol> <li> <p>Pattern Preprocessing</p> <ul> <li>Patterns are loaded from YAML configuration</li> <li>The Aho-Corasick automaton is constructed from patterns</li> <li>Failure links connect states for efficient pattern transitions</li> </ul> </li> <li> <p>Buffered Text Processing</p> <ul> <li>Text is processed in manageable chunks</li> <li>Partial matches at chunk boundaries are preserved</li> <li>Match information includes pattern name and position</li> </ul> </li> </ol>"},{"location":"reference/llm/pattern_detection/#text-normalization-pipeline","title":"Text Normalization Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Input Text  \u2502 \u2500\u2500\u25ba \u2502 Normalize   \u2502 \u2500\u2500\u25ba \u2502 Pattern Match \u2502 \u2500\u2500\u25ba \u2502 Map to      \u2502\n\u2502             \u2502     \u2502 (Remove     \u2502     \u2502 (Using        \u2502     \u2502 Original    \u2502\n\u2502             \u2502     \u2502  Whitespace)\u2502     \u2502  Automaton)   \u2502     \u2502 Positions   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/llm/pattern_detection/#performance-considerations","title":"Performance Considerations","text":"<p>Time Complexity O(n + m + k) where:</p> <ul> <li>n = length of input text</li> <li>m = total length of all patterns</li> <li>k = number of pattern occurrences</li> </ul> <p>Space Efficiency:</p> <ul> <li>Buffered processing minimizes memory usage</li> <li>Suitable for streaming applications with unbounded input</li> </ul> <p>Flexibility vs. Performance:</p> <ul> <li>Standard processors offer exact matching with minimal overhead</li> <li>Normalized processors provide flexibility with slight computational cost</li> </ul>"},{"location":"reference/llm/pattern_detection/#usage-examples","title":"Usage Examples","text":"<pre><code># Standard pattern matching (exact)\nprocessor = AhoCorasickBufferedProcessor('patterns.yaml')\nresult = await processor.process_chunk(\"input text\")\nprint(f\"Pattern found: {result.pattern_name}\" if result.matched else \"No match\")\n\n# Whitespace-insensitive pattern matching\nnorm_processor = AhoCorasickBufferedProcessorNormalized('patterns.yaml')\nresult = await norm_processor.process_chunk(\"input   text  with   spacing\")\nprint(f\"Pattern found: {result.pattern_name}\" if result.matched else \"No match\")\n</code></pre>"},{"location":"reference/llm/pattern_detection/#additional-resources","title":"Additional Resources","text":"<ul> <li>LLM Module Overview</li> <li>Tool Call Detection</li> </ul>"},{"location":"reference/llm/pattern_detection/aho_corasick/","title":"Aho-Corasick","text":""},{"location":"reference/llm/pattern_detection/aho_corasick/#src.llm.pattern_detection.aho_corasick.AhoCorasickAutomaton","title":"<code>src.llm.pattern_detection.aho_corasick.AhoCorasickAutomaton</code>","text":"<p>An implementation of the Aho-Corasick string matching automaton.</p> <p>This class implements a finite state machine that can efficiently match multiple patterns simultaneously in a given text. It uses a trie data structure augmented with failure links to achieve linear-time pattern matching.</p> <p>Attributes:</p> Name Type Description <code>patterns</code> <p>A dictionary mapping pattern names to their string values.</p> <code>next_states</code> <code>List[Dict[str, int]]</code> <p>A list of dictionaries representing state transitions.</p> <code>fail</code> <code>List[int]</code> <p>A list of failure link states.</p> <code>output</code> <code>List[List[str]]</code> <p>A list of pattern names associated with each state.</p> <code>current_state</code> <p>The current state of the automaton.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Dict[str, str]</code> <p>A dictionary where keys are pattern names and values are the pattern strings to match.</p> required Source code in <code>src/llm/pattern_detection/aho_corasick.py</code> <pre><code>class AhoCorasickAutomaton:\n    \"\"\"An implementation of the Aho-Corasick string matching automaton.\n\n    This class implements a finite state machine that can efficiently match multiple\n    patterns simultaneously in a given text. It uses a trie data structure augmented\n    with failure links to achieve linear-time pattern matching.\n\n    Attributes:\n        patterns: A dictionary mapping pattern names to their string values.\n        next_states: A list of dictionaries representing state transitions.\n        fail: A list of failure link states.\n        output: A list of pattern names associated with each state.\n        current_state: The current state of the automaton.\n\n    Args:\n        patterns: A dictionary where keys are pattern names and values are the\n            pattern strings to match.\n    \"\"\"\n\n    def __init__(self, patterns: Dict[str, str]):\n        \"\"\"Initializes the Aho-Corasick automaton with the given patterns.\n\n        Args:\n            patterns: A dictionary mapping pattern names to their string values.\n        \"\"\"\n        self.patterns = patterns\n        self.next_states: List[Dict[str, int]] = []\n        self.fail: List[int] = []\n        self.output: List[List[str]] = []\n        self.current_state = 0\n        self._build_machine()\n\n    def _build_machine(self):\n        \"\"\"Builds the Aho-Corasick automaton.\n\n        Constructs the trie structure, sets up failure links, and computes output\n        functions for the automaton. This is called automatically during initialization.\n        \"\"\"\n        # Initialize root state\n        self.next_states.append({})\n        self.fail.append(0)\n        self.output.append([])\n\n        # Build the trie from patterns\n        for pattern_name, pattern_str in self.patterns.items():\n            self._insert(pattern_str, pattern_name)\n\n        # Build failure links using BFS\n        queue = deque()\n        for char, nxt_state in self.next_states[0].items():\n            self.fail[nxt_state] = 0\n            queue.append(nxt_state)\n\n        while queue:\n            state = queue.popleft()\n            for char, nxt_state in self.next_states[state].items():\n                queue.append(nxt_state)\n                f = self.fail[state]\n                while f &gt; 0 and char not in self.next_states[f]:\n                    f = self.fail[f]\n                f = self.next_states[f].get(char, 0)\n                self.fail[nxt_state] = f\n                self.output[nxt_state].extend(self.output[f])\n\n    def _insert(self, pattern_str: str, pattern_name: str):\n        \"\"\"Inserts a pattern into the trie structure of the automaton.\n\n         Args:\n             pattern_str: The string pattern to insert.\n             pattern_name: The name associated with the pattern.\n         \"\"\"\n        current_state = 0\n        for char in pattern_str:\n            if char not in self.next_states[current_state]:\n                self.next_states.append({})\n                self.fail.append(0)\n                self.output.append([])\n                self.next_states[current_state][char] = len(self.next_states) - 1\n            current_state = self.next_states[current_state][char]\n        self.output[current_state].append(pattern_name)\n\n    def reset_state(self):\n        \"\"\"Resets the automaton to its initial state.\n\n        This method should be called before starting a new search if the automaton\n        has been used previously.\n        \"\"\"\n        self.current_state = 0\n\n    def search_chunk(self, chunk: str) -&gt; List[Tuple[int, str]]:\n        \"\"\"Searches for pattern matches in the given text chunk.\n\n        Args:\n            chunk: The text string to search for pattern matches.\n\n        Returns:\n            A list of tuples, where each tuple contains:\n                - The ending index of the match in the chunk (int)\n                - The name of the matched pattern (str)\n\n        ``` python title=\"Example usage\"\n        automaton = AhoCorasickAutomaton({'pat1': 'abc', 'pat2': 'bc'})\n        automaton.search_chunk('abc')  # [(1, 'pat2'), (2, 'pat1')]\n        ```\n        \"\"\"\n        found_patterns = []\n        for i, char in enumerate(chunk):\n            while self.current_state &gt; 0 and char not in self.next_states[self.current_state]:\n                self.current_state = self.fail[self.current_state]\n            self.current_state = self.next_states[self.current_state].get(char, 0)\n            if self.output[self.current_state]:\n                for pattern_name in self.output[self.current_state]:\n                    found_patterns.append((i, pattern_name))\n        return found_patterns\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick/#src.llm.pattern_detection.aho_corasick.AhoCorasickAutomaton.__init__","title":"<code>__init__(patterns)</code>","text":"<p>Initializes the Aho-Corasick automaton with the given patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Dict[str, str]</code> <p>A dictionary mapping pattern names to their string values.</p> required Source code in <code>src/llm/pattern_detection/aho_corasick.py</code> <pre><code>def __init__(self, patterns: Dict[str, str]):\n    \"\"\"Initializes the Aho-Corasick automaton with the given patterns.\n\n    Args:\n        patterns: A dictionary mapping pattern names to their string values.\n    \"\"\"\n    self.patterns = patterns\n    self.next_states: List[Dict[str, int]] = []\n    self.fail: List[int] = []\n    self.output: List[List[str]] = []\n    self.current_state = 0\n    self._build_machine()\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick/#src.llm.pattern_detection.aho_corasick.AhoCorasickAutomaton.reset_state","title":"<code>reset_state()</code>","text":"<p>Resets the automaton to its initial state.</p> <p>This method should be called before starting a new search if the automaton has been used previously.</p> Source code in <code>src/llm/pattern_detection/aho_corasick.py</code> <pre><code>def reset_state(self):\n    \"\"\"Resets the automaton to its initial state.\n\n    This method should be called before starting a new search if the automaton\n    has been used previously.\n    \"\"\"\n    self.current_state = 0\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick/#src.llm.pattern_detection.aho_corasick.AhoCorasickAutomaton.search_chunk","title":"<code>search_chunk(chunk)</code>","text":"<p>Searches for pattern matches in the given text chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>str</code> <p>The text string to search for pattern matches.</p> required <p>Returns:</p> Type Description <code>List[Tuple[int, str]]</code> <p>A list of tuples, where each tuple contains: - The ending index of the match in the chunk (int) - The name of the matched pattern (str)</p> Example usage<pre><code>automaton = AhoCorasickAutomaton({'pat1': 'abc', 'pat2': 'bc'})\nautomaton.search_chunk('abc')  # [(1, 'pat2'), (2, 'pat1')]\n</code></pre> Source code in <code>src/llm/pattern_detection/aho_corasick.py</code> <pre><code>def search_chunk(self, chunk: str) -&gt; List[Tuple[int, str]]:\n    \"\"\"Searches for pattern matches in the given text chunk.\n\n    Args:\n        chunk: The text string to search for pattern matches.\n\n    Returns:\n        A list of tuples, where each tuple contains:\n            - The ending index of the match in the chunk (int)\n            - The name of the matched pattern (str)\n\n    ``` python title=\"Example usage\"\n    automaton = AhoCorasickAutomaton({'pat1': 'abc', 'pat2': 'bc'})\n    automaton.search_chunk('abc')  # [(1, 'pat2'), (2, 'pat1')]\n    ```\n    \"\"\"\n    found_patterns = []\n    for i, char in enumerate(chunk):\n        while self.current_state &gt; 0 and char not in self.next_states[self.current_state]:\n            self.current_state = self.fail[self.current_state]\n        self.current_state = self.next_states[self.current_state].get(char, 0)\n        if self.output[self.current_state]:\n            for pattern_name in self.output[self.current_state]:\n                found_patterns.append((i, pattern_name))\n    return found_patterns\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick_normalized/","title":"Aho-Corasick Normalized","text":""},{"location":"reference/llm/pattern_detection/aho_corasick_normalized/#src.llm.pattern_detection.aho_corasick_normalized.AhoCorasickAutomatonNormalized","title":"<code>src.llm.pattern_detection.aho_corasick_normalized.AhoCorasickAutomatonNormalized</code>","text":"<p>A wrapper for normalized pattern matching using the Aho-Corasick algorithm.</p> <p>This class normalizes patterns by removing whitespace variations before building the underlying Aho-Corasick automaton. This allows for pattern matching that is insensitive to whitespace differences.</p> <p>Attributes:</p> Name Type Description <code>`normalized_patterns`</code> <p>Dictionary mapping pattern names to their normalized forms.</p> <code>`pattern_lengths`</code> <p>Dictionary storing the lengths of normalized patterns.</p> <code>`automaton`</code> <p>The underlying AhoCorasickAutomaton instance.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Dict[str, str]</code> <p>Dictionary mapping pattern names to their original string patterns.</p> required Source code in <code>src/llm/pattern_detection/aho_corasick_normalized.py</code> <pre><code>class AhoCorasickAutomatonNormalized:\n    \"\"\"A wrapper for normalized pattern matching using the Aho-Corasick algorithm.\n\n    This class normalizes patterns by removing whitespace variations before building\n    the underlying Aho-Corasick automaton. This allows for pattern matching that is\n    insensitive to whitespace differences.\n\n    Attributes:\n        `normalized_patterns`: Dictionary mapping pattern names to their normalized forms.\n        `pattern_lengths`: Dictionary storing the lengths of normalized patterns.\n        `automaton`: The underlying AhoCorasickAutomaton instance.\n\n    Args:\n        patterns: Dictionary mapping pattern names to their original string patterns.\n    \"\"\"\n\n    def __init__(self, patterns: Dict[str, str]):\n        self.normalized_patterns = {}\n        self.pattern_lengths = {}\n        for name, pat in patterns.items():\n            norm_pat, _ = normalize_and_map(pat)\n            self.normalized_patterns[name] = norm_pat\n            self.pattern_lengths[name] = len(norm_pat)\n\n        self.automaton = AhoCorasickAutomaton(self.normalized_patterns)\n\n    def reset_state(self):\n        \"\"\"Resets the automaton to its initial state.\n\n        Should be called before starting a new search if the automaton has been\n        used previously.\n        \"\"\"\n        self.automaton.reset_state()\n\n    def search_chunk(self, norm_chunk: str) -&gt; List[Tuple[int, str]]:\n        \"\"\"Searches for pattern matches in normalized text.\n\n        Args:\n            `norm_chunk`: The normalized text chunk to search in. Should be\n                pre-normalized before calling this method.\n\n        Returns:\n            A list of tuples, where each tuple contains:\n\n                - The ending index of the match in the normalized text (int)\n                - The name of the matched pattern (str)\n\n        ``` python title=\"Example usage\"\n        automaton = AhoCorasickAutomatonNormalized({'pat1': 'hello world'})\n        matches = automaton.search_chunk('helloworld')\n        print(len(matches))  # 1\n        ```\n        \"\"\"\n        return self.automaton.search_chunk(norm_chunk)\n\n    def get_pattern_length(self, pattern_name: str) -&gt; int:\n        \"\"\"Returns the length of a normalized pattern.\n\n        Args:\n            `pattern_name`: The name of the pattern whose length is required.\n\n        Returns:\n            The length of the normalized pattern as an integer.\n\n        Raises:\n            `KeyError`: Error raised if the `pattern_name` is not found in the patterns dictionary.\n        \"\"\"\n        return self.pattern_lengths[pattern_name]\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick_normalized/#src.llm.pattern_detection.aho_corasick_normalized.AhoCorasickAutomatonNormalized.get_pattern_length","title":"<code>get_pattern_length(pattern_name)</code>","text":"<p>Returns the length of a normalized pattern.</p> <p>Parameters:</p> Name Type Description Default <code>`pattern_name`</code> <p>The name of the pattern whose length is required.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The length of the normalized pattern as an integer.</p> <p>Raises:</p> Type Description <code>`KeyError`</code> <p>Error raised if the <code>pattern_name</code> is not found in the patterns dictionary.</p> Source code in <code>src/llm/pattern_detection/aho_corasick_normalized.py</code> <pre><code>def get_pattern_length(self, pattern_name: str) -&gt; int:\n    \"\"\"Returns the length of a normalized pattern.\n\n    Args:\n        `pattern_name`: The name of the pattern whose length is required.\n\n    Returns:\n        The length of the normalized pattern as an integer.\n\n    Raises:\n        `KeyError`: Error raised if the `pattern_name` is not found in the patterns dictionary.\n    \"\"\"\n    return self.pattern_lengths[pattern_name]\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick_normalized/#src.llm.pattern_detection.aho_corasick_normalized.AhoCorasickAutomatonNormalized.reset_state","title":"<code>reset_state()</code>","text":"<p>Resets the automaton to its initial state.</p> <p>Should be called before starting a new search if the automaton has been used previously.</p> Source code in <code>src/llm/pattern_detection/aho_corasick_normalized.py</code> <pre><code>def reset_state(self):\n    \"\"\"Resets the automaton to its initial state.\n\n    Should be called before starting a new search if the automaton has been\n    used previously.\n    \"\"\"\n    self.automaton.reset_state()\n</code></pre>"},{"location":"reference/llm/pattern_detection/aho_corasick_normalized/#src.llm.pattern_detection.aho_corasick_normalized.AhoCorasickAutomatonNormalized.search_chunk","title":"<code>search_chunk(norm_chunk)</code>","text":"<p>Searches for pattern matches in normalized text.</p> <p>Parameters:</p> Name Type Description Default <code>`norm_chunk`</code> <p>The normalized text chunk to search in. Should be pre-normalized before calling this method.</p> required <p>Returns:</p> Type Description <code>List[Tuple[int, str]]</code> <p>A list of tuples, where each tuple contains:</p> <ul> <li>The ending index of the match in the normalized text (int)</li> <li>The name of the matched pattern (str)</li> </ul> Example usage<pre><code>automaton = AhoCorasickAutomatonNormalized({'pat1': 'hello world'})\nmatches = automaton.search_chunk('helloworld')\nprint(len(matches))  # 1\n</code></pre> Source code in <code>src/llm/pattern_detection/aho_corasick_normalized.py</code> <pre><code>def search_chunk(self, norm_chunk: str) -&gt; List[Tuple[int, str]]:\n    \"\"\"Searches for pattern matches in normalized text.\n\n    Args:\n        `norm_chunk`: The normalized text chunk to search in. Should be\n            pre-normalized before calling this method.\n\n    Returns:\n        A list of tuples, where each tuple contains:\n\n            - The ending index of the match in the normalized text (int)\n            - The name of the matched pattern (str)\n\n    ``` python title=\"Example usage\"\n    automaton = AhoCorasickAutomatonNormalized({'pat1': 'hello world'})\n    matches = automaton.search_chunk('helloworld')\n    print(len(matches))  # 1\n    ```\n    \"\"\"\n    return self.automaton.search_chunk(norm_chunk)\n</code></pre>"},{"location":"reference/llm/pattern_detection/base_buffered_processor/","title":"Base Processor Class","text":""},{"location":"reference/llm/pattern_detection/base_buffered_processor/#src.llm.pattern_detection.base_buffered_processor.BaseBufferedProcessor","title":"<code>src.llm.pattern_detection.base_buffered_processor.BaseBufferedProcessor</code>","text":"<p>Base class for buffered text processors.</p> <p>This abstract base class implements common buffering logic for streaming text processing. Subclasses must implement process_chunk_impl to define specific matching behavior.</p> <p>Attributes:</p> Name Type Description <code>`tool_call_message`</code> <p>Message to include when a tool call is detected.</p> <code>`trailing_buffer_original`</code> <p>Buffer containing text carried over from previous chunks.</p> <p>Parameters:</p> Name Type Description Default <code>`tool_call_message`</code> <p>Optional message to use when a tool call is detected. Defaults to \"Tool call detected.\"</p> required Source code in <code>src/llm/pattern_detection/base_buffered_processor.py</code> <pre><code>class BaseBufferedProcessor:\n    \"\"\"Base class for buffered text processors.\n\n    This abstract base class implements common buffering logic for streaming text\n    processing. Subclasses must implement process_chunk_impl to define specific\n    matching behavior.\n\n    Attributes:\n        `tool_call_message`: Message to include when a tool call is detected.\n        `trailing_buffer_original`: Buffer containing text carried over from previous chunks.\n\n    Args:\n        `tool_call_message`: Optional message to use when a tool call is detected.\n            Defaults to \"Tool call detected.\"\n    \"\"\"\n\n    def __init__(self, tool_call_message: str = \"Tool call detected.\"):\n        self.tool_call_message = tool_call_message\n        self.trailing_buffer_original = \"\"\n\n    def reset_states(self):\n        \"\"\"Resets the processor's internal state.\n\n        Clears the trailing buffer and resets any internal state to initial values.\n        Should be called before starting to process a new stream of text.\n        \"\"\"\n        self.trailing_buffer_original = \"\"\n\n    async def process_chunk(self, chunk: str) -&gt; PatternMatchResult:\n        \"\"\"Processes a chunk of text with buffering.\n\n        Combines the new chunk with any trailing text from previous chunks and\n        delegates the actual processing to process_chunk_impl.\n\n        Args:\n            `chunk`: The new text chunk to process.\n\n        Returns:\n            `PatternMatchResult` containing the processed output and any match information.\n\n        ``` python title=\"Example usage\"\n        processor = MyProcessor()\n        result = await processor.process_chunk(\"some text\")\n        print(result.output)  # some text\n        ```\n        \"\"\"\n        # Combine the trailing buffer with new chunk\n        combined_original = self.trailing_buffer_original + chunk\n\n        # Let the subclass handle the matching logic on combined_original.\n        result, new_trailing = self.process_chunk_impl(combined_original)\n\n        # Update the trailing buffer with what's left\n        self.trailing_buffer_original = new_trailing\n        return result\n\n    async def flush_buffer(self) -&gt; PatternMatchResult:\n        \"\"\"Flushes any remaining text in the trailing buffer.\n\n        Should be called after processing the final chunk to handle any remaining\n        buffered text.\n\n        Returns:\n            `PatternMatchResult` containing any remaining buffered text.\n\n        ``` python title=\"Example usage\"\n        processor = MyProcessor()\n        result = await processor.flush_buffer()\n        print(len(result.output))  # 0\n        ```\n        \"\"\"\n        from src.data_models.streaming import PatternMatchResult\n        result = PatternMatchResult()\n        if self.trailing_buffer_original:\n            result.output = self.trailing_buffer_original\n            self.trailing_buffer_original = \"\"\n        return result\n\n    @abstractmethod\n    def process_chunk_impl(self, combined_original: str):\n        \"\"\"Processes combined text to find pattern matches.\n\n        This abstract method must be implemented by subclasses to define specific\n        pattern matching behavior.\n\n        Args:\n            `combined_original`: Text to process, including any trailing text from\n                previous chunks.\n\n        Returns:\n            A tuple containing:\n\n                - `PatternMatchResult`: Result object with match information and\n                    processed text.\n                - `str`: Any trailing text to carry over to the next chunk.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/llm/pattern_detection/base_buffered_processor/#src.llm.pattern_detection.base_buffered_processor.BaseBufferedProcessor.flush_buffer","title":"<code>flush_buffer()</code>  <code>async</code>","text":"<p>Flushes any remaining text in the trailing buffer.</p> <p>Should be called after processing the final chunk to handle any remaining buffered text.</p> <p>Returns:</p> Type Description <code>PatternMatchResult</code> <p><code>PatternMatchResult</code> containing any remaining buffered text.</p> Example usage<pre><code>processor = MyProcessor()\nresult = await processor.flush_buffer()\nprint(len(result.output))  # 0\n</code></pre> Source code in <code>src/llm/pattern_detection/base_buffered_processor.py</code> <pre><code>async def flush_buffer(self) -&gt; PatternMatchResult:\n    \"\"\"Flushes any remaining text in the trailing buffer.\n\n    Should be called after processing the final chunk to handle any remaining\n    buffered text.\n\n    Returns:\n        `PatternMatchResult` containing any remaining buffered text.\n\n    ``` python title=\"Example usage\"\n    processor = MyProcessor()\n    result = await processor.flush_buffer()\n    print(len(result.output))  # 0\n    ```\n    \"\"\"\n    from src.data_models.streaming import PatternMatchResult\n    result = PatternMatchResult()\n    if self.trailing_buffer_original:\n        result.output = self.trailing_buffer_original\n        self.trailing_buffer_original = \"\"\n    return result\n</code></pre>"},{"location":"reference/llm/pattern_detection/base_buffered_processor/#src.llm.pattern_detection.base_buffered_processor.BaseBufferedProcessor.process_chunk","title":"<code>process_chunk(chunk)</code>  <code>async</code>","text":"<p>Processes a chunk of text with buffering.</p> <p>Combines the new chunk with any trailing text from previous chunks and delegates the actual processing to process_chunk_impl.</p> <p>Parameters:</p> Name Type Description Default <code>`chunk`</code> <p>The new text chunk to process.</p> required <p>Returns:</p> Type Description <code>PatternMatchResult</code> <p><code>PatternMatchResult</code> containing the processed output and any match information.</p> Example usage<pre><code>processor = MyProcessor()\nresult = await processor.process_chunk(\"some text\")\nprint(result.output)  # some text\n</code></pre> Source code in <code>src/llm/pattern_detection/base_buffered_processor.py</code> <pre><code>async def process_chunk(self, chunk: str) -&gt; PatternMatchResult:\n    \"\"\"Processes a chunk of text with buffering.\n\n    Combines the new chunk with any trailing text from previous chunks and\n    delegates the actual processing to process_chunk_impl.\n\n    Args:\n        `chunk`: The new text chunk to process.\n\n    Returns:\n        `PatternMatchResult` containing the processed output and any match information.\n\n    ``` python title=\"Example usage\"\n    processor = MyProcessor()\n    result = await processor.process_chunk(\"some text\")\n    print(result.output)  # some text\n    ```\n    \"\"\"\n    # Combine the trailing buffer with new chunk\n    combined_original = self.trailing_buffer_original + chunk\n\n    # Let the subclass handle the matching logic on combined_original.\n    result, new_trailing = self.process_chunk_impl(combined_original)\n\n    # Update the trailing buffer with what's left\n    self.trailing_buffer_original = new_trailing\n    return result\n</code></pre>"},{"location":"reference/llm/pattern_detection/base_buffered_processor/#src.llm.pattern_detection.base_buffered_processor.BaseBufferedProcessor.process_chunk_impl","title":"<code>process_chunk_impl(combined_original)</code>  <code>abstractmethod</code>","text":"<p>Processes combined text to find pattern matches.</p> <p>This abstract method must be implemented by subclasses to define specific pattern matching behavior.</p> <p>Parameters:</p> Name Type Description Default <code>`combined_original`</code> <p>Text to process, including any trailing text from previous chunks.</p> required <p>Returns:</p> Type Description <p>A tuple containing:</p> <ul> <li><code>PatternMatchResult</code>: Result object with match information and     processed text.</li> <li><code>str</code>: Any trailing text to carry over to the next chunk.</li> </ul> Source code in <code>src/llm/pattern_detection/base_buffered_processor.py</code> <pre><code>@abstractmethod\ndef process_chunk_impl(self, combined_original: str):\n    \"\"\"Processes combined text to find pattern matches.\n\n    This abstract method must be implemented by subclasses to define specific\n    pattern matching behavior.\n\n    Args:\n        `combined_original`: Text to process, including any trailing text from\n            previous chunks.\n\n    Returns:\n        A tuple containing:\n\n            - `PatternMatchResult`: Result object with match information and\n                processed text.\n            - `str`: Any trailing text to carry over to the next chunk.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llm/pattern_detection/base_buffered_processor/#src.llm.pattern_detection.base_buffered_processor.BaseBufferedProcessor.reset_states","title":"<code>reset_states()</code>","text":"<p>Resets the processor's internal state.</p> <p>Clears the trailing buffer and resets any internal state to initial values. Should be called before starting to process a new stream of text.</p> Source code in <code>src/llm/pattern_detection/base_buffered_processor.py</code> <pre><code>def reset_states(self):\n    \"\"\"Resets the processor's internal state.\n\n    Clears the trailing buffer and resets any internal state to initial values.\n    Should be called before starting to process a new stream of text.\n    \"\"\"\n    self.trailing_buffer_original = \"\"\n</code></pre>"},{"location":"reference/llm/pattern_detection/buffered_processor_normalized/","title":"Buffered Processor Normalized","text":""},{"location":"reference/llm/pattern_detection/buffered_processor_normalized/#src.llm.pattern_detection.buffered_processor_normalized.AhoCorasickBufferedProcessorNormalized","title":"<code>src.llm.pattern_detection.buffered_processor_normalized.AhoCorasickBufferedProcessorNormalized</code>","text":"<p>               Bases: <code>BaseBufferedProcessor</code></p> <p>A buffered processor that performs normalized pattern matching ignoring whitespace.</p> <p>This class implements pattern matching that is insensitive to whitespace variations by normalizing both patterns and input text. It uses the Aho-Corasick algorithm for efficient multiple pattern matching.</p> <p>Attributes:</p> Name Type Description <code>automaton</code> <p>An instance of AhoCorasickAutomatonNormalized for pattern matching.</p> <code>max_pattern_len</code> <p>The length of the longest pattern in the normalized patterns.</p> <code>tool_call_message</code> <p>Message to include when a tool call is detected.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to the YAML file containing pattern definitions.</p> required <code>tool_call_message</code> <code>str</code> <p>Optional message to use when a tool call is detected. Defaults to \"Tool call detected.\"</p> <code>'Tool call detected.'</code> Source code in <code>src/llm/pattern_detection/buffered_processor_normalized.py</code> <pre><code>class AhoCorasickBufferedProcessorNormalized(BaseBufferedProcessor):\n    \"\"\"A buffered processor that performs normalized pattern matching ignoring whitespace.\n\n    This class implements pattern matching that is insensitive to whitespace variations\n    by normalizing both patterns and input text. It uses the Aho-Corasick algorithm\n    for efficient multiple pattern matching.\n\n    Attributes:\n        automaton: An instance of AhoCorasickAutomatonNormalized for pattern matching.\n        max_pattern_len: The length of the longest pattern in the normalized patterns.\n        tool_call_message: Message to include when a tool call is detected.\n\n    Args:\n        yaml_path: Path to the YAML file containing pattern definitions.\n        tool_call_message: Optional message to use when a tool call is detected.\n            Defaults to \"Tool call detected.\"\n    \"\"\"\n\n    def __init__(self, yaml_path: str, tool_call_message: str = \"Tool call detected.\"):\n        super().__init__(tool_call_message)\n        raw_patterns = load_patterns(yaml_path)\n        self.automaton = AhoCorasickAutomatonNormalized(raw_patterns)\n        self.max_pattern_len = max(len(p) for p in self.automaton.normalized_patterns.values())\n        self.automaton.reset_state()\n\n    def process_chunk_impl(self, combined_original: str):\n        \"\"\"Processes a chunk of text to find pattern matches while ignoring whitespace.\n\n        This method normalizes the input text, performs pattern matching, and returns\n        the earliest match found along with any safe text that can be output.\n\n        Args:\n            `combined_original`: The original text chunk to process.\n\n        Returns:\n            A tuple containing:\n\n                - `PatternMatchResult`: Result object containing match information and\n                    processed text.\n                - `str`: Any trailing text that needs to be carried over to the next\n                    chunk.\n\n        ``` python title=\"Example usage\"\n        processor = AhoCorasickBufferedProcessorNormalized('patterns.yaml')\n        result, trailing = processor.process_chunk_impl('some text')\n        print(result.matched, result.pattern_name)  # False None\n        ```\n        \"\"\"\n        result = PatternMatchResult()\n        # Normalize the entire combined_original and get index mapping.\n        norm_combined, index_map = normalize_and_map(combined_original)\n        matches = self.automaton.search_chunk(norm_combined)\n\n        if not matches:\n            keep_len = min(self.max_pattern_len - 1, len(norm_combined))\n            if keep_len &gt; 0:\n                orig_keep_start = index_map[len(norm_combined) - keep_len]\n                safe_text = combined_original[:orig_keep_start]\n                new_trailing = combined_original[orig_keep_start:]\n            else:\n                safe_text = combined_original\n                new_trailing = \"\"\n            result.output = safe_text\n            return result, new_trailing\n\n        # Find the earliest match\n        earliest_original_index = None\n        earliest_match_pattern = None\n        earliest_norm_start_idx = None\n        for norm_end_idx, pattern_name in matches:\n            pat_len = self.automaton.get_pattern_length(pattern_name)\n            norm_start_idx = norm_end_idx - pat_len + 1\n            original_end = index_map[norm_end_idx]\n            original_start = index_map[norm_start_idx]\n            if earliest_original_index is None or original_end &lt; earliest_original_index:\n                earliest_original_index = original_end\n                earliest_match_pattern = pattern_name\n                earliest_norm_start_idx = norm_start_idx\n\n        original_start = index_map[earliest_norm_start_idx]\n        result.matched = True\n        result.pattern_name = earliest_match_pattern\n        result.tool_call_message = self.tool_call_message\n        result.output = combined_original[:original_start]\n        result.text_with_tool_call = combined_original[original_start:]\n        new_trailing = \"\"\n        self.automaton.reset_state()\n        return result, new_trailing\n</code></pre>"},{"location":"reference/llm/pattern_detection/buffered_processor_normalized/#src.llm.pattern_detection.buffered_processor_normalized.AhoCorasickBufferedProcessorNormalized.process_chunk_impl","title":"<code>process_chunk_impl(combined_original)</code>","text":"<p>Processes a chunk of text to find pattern matches while ignoring whitespace.</p> <p>This method normalizes the input text, performs pattern matching, and returns the earliest match found along with any safe text that can be output.</p> <p>Parameters:</p> Name Type Description Default <code>`combined_original`</code> <p>The original text chunk to process.</p> required <p>Returns:</p> Type Description <p>A tuple containing:</p> <ul> <li><code>PatternMatchResult</code>: Result object containing match information and     processed text.</li> <li><code>str</code>: Any trailing text that needs to be carried over to the next     chunk.</li> </ul> Example usage<pre><code>processor = AhoCorasickBufferedProcessorNormalized('patterns.yaml')\nresult, trailing = processor.process_chunk_impl('some text')\nprint(result.matched, result.pattern_name)  # False None\n</code></pre> Source code in <code>src/llm/pattern_detection/buffered_processor_normalized.py</code> <pre><code>def process_chunk_impl(self, combined_original: str):\n    \"\"\"Processes a chunk of text to find pattern matches while ignoring whitespace.\n\n    This method normalizes the input text, performs pattern matching, and returns\n    the earliest match found along with any safe text that can be output.\n\n    Args:\n        `combined_original`: The original text chunk to process.\n\n    Returns:\n        A tuple containing:\n\n            - `PatternMatchResult`: Result object containing match information and\n                processed text.\n            - `str`: Any trailing text that needs to be carried over to the next\n                chunk.\n\n    ``` python title=\"Example usage\"\n    processor = AhoCorasickBufferedProcessorNormalized('patterns.yaml')\n    result, trailing = processor.process_chunk_impl('some text')\n    print(result.matched, result.pattern_name)  # False None\n    ```\n    \"\"\"\n    result = PatternMatchResult()\n    # Normalize the entire combined_original and get index mapping.\n    norm_combined, index_map = normalize_and_map(combined_original)\n    matches = self.automaton.search_chunk(norm_combined)\n\n    if not matches:\n        keep_len = min(self.max_pattern_len - 1, len(norm_combined))\n        if keep_len &gt; 0:\n            orig_keep_start = index_map[len(norm_combined) - keep_len]\n            safe_text = combined_original[:orig_keep_start]\n            new_trailing = combined_original[orig_keep_start:]\n        else:\n            safe_text = combined_original\n            new_trailing = \"\"\n        result.output = safe_text\n        return result, new_trailing\n\n    # Find the earliest match\n    earliest_original_index = None\n    earliest_match_pattern = None\n    earliest_norm_start_idx = None\n    for norm_end_idx, pattern_name in matches:\n        pat_len = self.automaton.get_pattern_length(pattern_name)\n        norm_start_idx = norm_end_idx - pat_len + 1\n        original_end = index_map[norm_end_idx]\n        original_start = index_map[norm_start_idx]\n        if earliest_original_index is None or original_end &lt; earliest_original_index:\n            earliest_original_index = original_end\n            earliest_match_pattern = pattern_name\n            earliest_norm_start_idx = norm_start_idx\n\n    original_start = index_map[earliest_norm_start_idx]\n    result.matched = True\n    result.pattern_name = earliest_match_pattern\n    result.tool_call_message = self.tool_call_message\n    result.output = combined_original[:original_start]\n    result.text_with_tool_call = combined_original[original_start:]\n    new_trailing = \"\"\n    self.automaton.reset_state()\n    return result, new_trailing\n</code></pre>"},{"location":"reference/llm/pattern_detection/buffered_processor_standard/","title":"Buffered Processor Standard","text":""},{"location":"reference/llm/pattern_detection/buffered_processor_standard/#src.llm.pattern_detection.buffered_processor_standard.AhoCorasickBufferedProcessor","title":"<code>src.llm.pattern_detection.buffered_processor_standard.AhoCorasickBufferedProcessor</code>","text":"<p>               Bases: <code>BaseBufferedProcessor</code></p> <p>A buffered processor that performs exact pattern matching.</p> <p>This class implements exact pattern matching using the Aho-Corasick algorithm for efficient multiple pattern matching. Unlike the normalized version, this processor is sensitive to whitespace and performs exact string matching.</p> <p>Attributes:</p> Name Type Description <code>`automaton`</code> <p>An instance of AhoCorasickAutomaton for pattern matching.</p> <code>`max_pattern_len`</code> <p>The length of the longest pattern in the raw patterns.</p> <code>`tool_call_message`</code> <p>Message to include when a tool call is detected.</p> <p>Parameters:</p> Name Type Description Default <code>`yaml_path`</code> <p>Path to the YAML file containing pattern definitions.</p> required <code>`tool_call_message`</code> <p>Optional message to use when a tool call is detected. Defaults to \"Tool call detected.\"</p> required Source code in <code>src/llm/pattern_detection/buffered_processor_standard.py</code> <pre><code>class AhoCorasickBufferedProcessor(BaseBufferedProcessor):\n    \"\"\"A buffered processor that performs exact pattern matching.\n\n    This class implements exact pattern matching using the Aho-Corasick algorithm\n    for efficient multiple pattern matching. Unlike the normalized version, this\n    processor is sensitive to whitespace and performs exact string matching.\n\n    Attributes:\n        `automaton`: An instance of AhoCorasickAutomaton for pattern matching.\n        `max_pattern_len`: The length of the longest pattern in the raw patterns.\n        `tool_call_message`: Message to include when a tool call is detected.\n\n    Args:\n        `yaml_path`: Path to the YAML file containing pattern definitions.\n        `tool_call_message`: Optional message to use when a tool call is detected.\n            Defaults to \"Tool call detected.\"\n    \"\"\"\n\n    def __init__(self, yaml_path: str, tool_call_message: str = \"Tool call detected.\"):\n        super().__init__(tool_call_message)\n        raw_patterns = load_patterns(yaml_path)\n        self.automaton = AhoCorasickAutomaton(raw_patterns)\n        self.max_pattern_len = max(len(p) for p in raw_patterns.values())\n        self.automaton.reset_state()\n\n    def process_chunk_impl(self, combined_original: str):\n        \"\"\"Processes a chunk of text to find exact pattern matches.\n\n        This method performs exact pattern matching on the input text and returns\n        the earliest match found along with any safe text that can be output.\n\n        Args:\n            `combined_original`: The text chunk to process.\n\n        Returns:\n            A tuple containing:\n\n                - `PatternMatchResult`: Result object containing match information and\n                    processed text.\n                - `str`: Any trailing text that needs to be carried over to the next\n                    chunk.\n\n        ``` python title=\"Example usage\"\n        processor = AhoCorasickBufferedProcessor('patterns.yaml')\n        result, trailing = processor.process_chunk_impl('some text')\n        print(result.matched, result.pattern_name)  # False None\n        ```\n        \"\"\"\n        result = PatternMatchResult()\n        # Search in the original text\n        matches = self.automaton.search_chunk(combined_original)\n\n        if not matches:\n            # Keep up to max_pattern_len - 1 characters for partial match\n            keep_len = min(self.max_pattern_len - 1, len(combined_original))\n            if keep_len &gt; 0:\n                safe_text = combined_original[:-keep_len]\n                new_trailing = combined_original[-keep_len:]\n            else:\n                safe_text = combined_original\n                new_trailing = \"\"\n            result.output = safe_text\n            return result, new_trailing\n\n        # Otherwise, use the earliest match\n        earliest_end, pattern_name = min(matches, key=lambda x: x[0])\n        pattern_str = self.automaton.patterns[pattern_name]\n        match_start = earliest_end - len(pattern_str) + 1\n\n        result.matched = True\n        result.pattern_name = pattern_name\n        result.tool_call_message = self.tool_call_message\n        result.output = combined_original[:match_start]\n        result.text_with_tool_call = combined_original[match_start:]\n        new_trailing = \"\"\n        self.automaton.reset_state()\n        return result, new_trailing\n</code></pre>"},{"location":"reference/llm/pattern_detection/buffered_processor_standard/#src.llm.pattern_detection.buffered_processor_standard.AhoCorasickBufferedProcessor.process_chunk_impl","title":"<code>process_chunk_impl(combined_original)</code>","text":"<p>Processes a chunk of text to find exact pattern matches.</p> <p>This method performs exact pattern matching on the input text and returns the earliest match found along with any safe text that can be output.</p> <p>Parameters:</p> Name Type Description Default <code>`combined_original`</code> <p>The text chunk to process.</p> required <p>Returns:</p> Type Description <p>A tuple containing:</p> <ul> <li><code>PatternMatchResult</code>: Result object containing match information and     processed text.</li> <li><code>str</code>: Any trailing text that needs to be carried over to the next     chunk.</li> </ul> Example usage<pre><code>processor = AhoCorasickBufferedProcessor('patterns.yaml')\nresult, trailing = processor.process_chunk_impl('some text')\nprint(result.matched, result.pattern_name)  # False None\n</code></pre> Source code in <code>src/llm/pattern_detection/buffered_processor_standard.py</code> <pre><code>def process_chunk_impl(self, combined_original: str):\n    \"\"\"Processes a chunk of text to find exact pattern matches.\n\n    This method performs exact pattern matching on the input text and returns\n    the earliest match found along with any safe text that can be output.\n\n    Args:\n        `combined_original`: The text chunk to process.\n\n    Returns:\n        A tuple containing:\n\n            - `PatternMatchResult`: Result object containing match information and\n                processed text.\n            - `str`: Any trailing text that needs to be carried over to the next\n                chunk.\n\n    ``` python title=\"Example usage\"\n    processor = AhoCorasickBufferedProcessor('patterns.yaml')\n    result, trailing = processor.process_chunk_impl('some text')\n    print(result.matched, result.pattern_name)  # False None\n    ```\n    \"\"\"\n    result = PatternMatchResult()\n    # Search in the original text\n    matches = self.automaton.search_chunk(combined_original)\n\n    if not matches:\n        # Keep up to max_pattern_len - 1 characters for partial match\n        keep_len = min(self.max_pattern_len - 1, len(combined_original))\n        if keep_len &gt; 0:\n            safe_text = combined_original[:-keep_len]\n            new_trailing = combined_original[-keep_len:]\n        else:\n            safe_text = combined_original\n            new_trailing = \"\"\n        result.output = safe_text\n        return result, new_trailing\n\n    # Otherwise, use the earliest match\n    earliest_end, pattern_name = min(matches, key=lambda x: x[0])\n    pattern_str = self.automaton.patterns[pattern_name]\n    match_start = earliest_end - len(pattern_str) + 1\n\n    result.matched = True\n    result.pattern_name = pattern_name\n    result.tool_call_message = self.tool_call_message\n    result.output = combined_original[:match_start]\n    result.text_with_tool_call = combined_original[match_start:]\n    new_trailing = \"\"\n    self.automaton.reset_state()\n    return result, new_trailing\n</code></pre>"},{"location":"reference/llm/pattern_detection/pattern_utils/","title":"Pattern Utils","text":""},{"location":"reference/llm/pattern_detection/pattern_utils/#src.llm.pattern_detection.pattern_utils.load_patterns","title":"<code>src.llm.pattern_detection.pattern_utils.load_patterns(yaml_path)</code>","text":"<p>Load tool call patterns from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to the YAML file containing patterns</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping pattern names to pattern strings</p> Source code in <code>src/llm/pattern_detection/pattern_utils.py</code> <pre><code>def load_patterns(yaml_path: str) -&gt; Dict[str, str]:\n    \"\"\"\n    Load tool call patterns from a YAML file.\n\n    Args:\n        yaml_path: Path to the YAML file containing patterns\n\n    Returns:\n        Dictionary mapping pattern names to pattern strings\n    \"\"\"\n    with open(yaml_path, 'r') as f:\n        config = yaml.safe_load(f)\n    return config.get('patterns', {})\n</code></pre>"},{"location":"reference/llm/pattern_detection/pattern_utils/#src.llm.pattern_detection.pattern_utils.normalize_and_map","title":"<code>src.llm.pattern_detection.pattern_utils.normalize_and_map(text)</code>","text":"<p>Returns a tuple of normalized text with whitespace removed and an index mapping.</p> <p>Processes the input text by removing all whitespace characters and creating a mapping that tracks the original position of each character.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to be normalized.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing two elements:</p> <ul> <li><code>normalized_text</code> (str): The input text with all whitespace removed.</li> <li><code>index_map</code> (list): A list where index_map[i] is the original index   of the i-th character in normalized_text.</li> </ul> Example usage<pre><code>normalize_and_map(\"a b c\")  # ('abc', [0, 2, 4])\n</code></pre> Source code in <code>src/llm/pattern_detection/pattern_utils.py</code> <pre><code>def normalize_and_map(text: str):\n    \"\"\"Returns a tuple of normalized text with whitespace removed and an index mapping.\n\n    Processes the input text by removing all whitespace characters and creating\n    a mapping that tracks the original position of each character.\n\n    Args:\n        text: The input string to be normalized.\n\n    Returns:\n        tuple: A tuple containing two elements:\n\n            - `normalized_text` (str): The input text with all whitespace removed.\n            - `index_map` (list): A list where index_map[i] is the original index\n              of the i-th character in normalized_text.\n\n    ``` python title=\"Example usage\"\n    normalize_and_map(\"a b c\")  # ('abc', [0, 2, 4])\n    ```\n    \"\"\"\n    normalized_chars = []\n    index_map = []\n    for idx, ch in enumerate(text):\n        if not ch.isspace():\n            normalized_chars.append(ch)\n            index_map.append(idx)\n    return \"\".join(normalized_chars), index_map\n</code></pre>"},{"location":"reference/llm/tool_detection/","title":"Tool Detection Documentation","text":""},{"location":"reference/llm/tool_detection/#overview","title":"Overview","text":"<p>The Tool Detection Module is responsible for identifying and extracting tool calls from model responses. It provides various strategies to detect tool invocations, including manual and vendor-specific methods. This module is crucial for integrating LLMs with external tools and APIs.</p>"},{"location":"reference/llm/tool_detection/#components","title":"Components","text":""},{"location":"reference/llm/tool_detection/#detection-strategies","title":"Detection Strategies","text":"<ul> <li><code>BaseToolCallDetectionStrategy</code> - An abstract base class that defines the interface for all tool call detection strategies.</li> <li><code>ManualToolCallDetectionStrategy</code> - A manual approach where predefined patterns and heuristics are used to detect tool calls.</li> <li><code>VendorToolCallDetectionStrategy</code> - A strategy that utilizes vendor-specific methods for detecting tool calls.</li> </ul>"},{"location":"reference/llm/tool_detection/#detection-results","title":"Detection Results","text":"<ul> <li><code>DetectionState</code> - Represents the current state of tool call detection.</li> <li><code>DetectionResult</code> - Stores the final detection results, including extracted tool call information.</li> </ul>"},{"location":"reference/llm/tool_detection/#additional-resources","title":"Additional Resources","text":"<ul> <li>LLM Overview</li> <li>Pattern Detection</li> </ul>"},{"location":"reference/llm/tool_detection/base_detection_strategy/","title":"Base Strategy","text":""},{"location":"reference/llm/tool_detection/base_detection_strategy/#src.llm.tool_detection.base_detection_strategy.BaseToolCallDetectionStrategy","title":"<code>src.llm.tool_detection.base_detection_strategy.BaseToolCallDetectionStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for implementing tool call detection strategies.</p> <p>This class defines the interface for strategies that detect when an LLM wants to make tool calls within its response stream. Implementations should handle parsing of SSE chunks to identify tool call patterns and maintain any necessary state between chunks.</p> <p>The detection process happens in three phases: 1. Reset - Clear any accumulated state 2. Detect - Process incoming chunks sequentially 3. Finalize - Handle any remaining state and make final determination</p> Source code in <code>src/llm/tool_detection/base_detection_strategy.py</code> <pre><code>class BaseToolCallDetectionStrategy(ABC):\n    \"\"\"Abstract base class for implementing tool call detection strategies.\n\n    This class defines the interface for strategies that detect when an LLM\n    wants to make tool calls within its response stream. Implementations should\n    handle parsing of SSE chunks to identify tool call patterns and maintain\n    any necessary state between chunks.\n\n    The detection process happens in three phases:\n    1. Reset - Clear any accumulated state\n    2. Detect - Process incoming chunks sequentially\n    3. Finalize - Handle any remaining state and make final determination\n    \"\"\"\n\n    @abstractmethod\n    def reset(self) -&gt; None:\n        \"\"\"Reset the strategy's internal state.\n\n        This method should be called before starting a new detection sequence\n        to ensure no state is carried over from previous detections.\n\n        Implementation should clear any accumulated buffers, counters, or other\n        state variables used during detection.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def detect_chunk(\n            self,\n            sse_chunk: SSEChunk,\n            context: StreamContext\n    ) -&gt; DetectionResult:\n        \"\"\"Process an SSE chunk to detect potential tool calls.\n\n        Args:\n            sse_chunk (SSEChunk): The chunk of streaming response to analyze.\n                Contains delta updates and choice information.\n            context (StreamContext): Contextual information about the current\n                stream, including conversation history and available tools.\n\n        Returns:\n            DetectionResult: The result of analyzing this chunk, including\n                whether a tool call was detected and any extracted tool\n                call information.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def finalize_detection(\n            self,\n            context: StreamContext\n    ) -&gt; DetectionResult:\n        \"\"\"Complete the detection process and handle any remaining state.\n\n        This method should be called after all chunks have been processed\n        to handle any buffered content or partial tool calls that may need\n        final processing.\n\n        Args:\n            context (StreamContext): Contextual information about the current\n                stream, including conversation history and available tools.\n\n        Returns:\n            DetectionResult: Final detection result, including any tool calls\n                that were detected from accumulated state.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/llm/tool_detection/base_detection_strategy/#src.llm.tool_detection.base_detection_strategy.BaseToolCallDetectionStrategy.detect_chunk","title":"<code>detect_chunk(sse_chunk, context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process an SSE chunk to detect potential tool calls.</p> <p>Parameters:</p> Name Type Description Default <code>sse_chunk</code> <code>SSEChunk</code> <p>The chunk of streaming response to analyze. Contains delta updates and choice information.</p> required <code>context</code> <code>StreamContext</code> <p>Contextual information about the current stream, including conversation history and available tools.</p> required <p>Returns:</p> Name Type Description <code>DetectionResult</code> <code>DetectionResult</code> <p>The result of analyzing this chunk, including whether a tool call was detected and any extracted tool call information.</p> Source code in <code>src/llm/tool_detection/base_detection_strategy.py</code> <pre><code>@abstractmethod\nasync def detect_chunk(\n        self,\n        sse_chunk: SSEChunk,\n        context: StreamContext\n) -&gt; DetectionResult:\n    \"\"\"Process an SSE chunk to detect potential tool calls.\n\n    Args:\n        sse_chunk (SSEChunk): The chunk of streaming response to analyze.\n            Contains delta updates and choice information.\n        context (StreamContext): Contextual information about the current\n            stream, including conversation history and available tools.\n\n    Returns:\n        DetectionResult: The result of analyzing this chunk, including\n            whether a tool call was detected and any extracted tool\n            call information.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llm/tool_detection/base_detection_strategy/#src.llm.tool_detection.base_detection_strategy.BaseToolCallDetectionStrategy.finalize_detection","title":"<code>finalize_detection(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Complete the detection process and handle any remaining state.</p> <p>This method should be called after all chunks have been processed to handle any buffered content or partial tool calls that may need final processing.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StreamContext</code> <p>Contextual information about the current stream, including conversation history and available tools.</p> required <p>Returns:</p> Name Type Description <code>DetectionResult</code> <code>DetectionResult</code> <p>Final detection result, including any tool calls that were detected from accumulated state.</p> Source code in <code>src/llm/tool_detection/base_detection_strategy.py</code> <pre><code>@abstractmethod\nasync def finalize_detection(\n        self,\n        context: StreamContext\n) -&gt; DetectionResult:\n    \"\"\"Complete the detection process and handle any remaining state.\n\n    This method should be called after all chunks have been processed\n    to handle any buffered content or partial tool calls that may need\n    final processing.\n\n    Args:\n        context (StreamContext): Contextual information about the current\n            stream, including conversation history and available tools.\n\n    Returns:\n        DetectionResult: Final detection result, including any tool calls\n            that were detected from accumulated state.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llm/tool_detection/base_detection_strategy/#src.llm.tool_detection.base_detection_strategy.BaseToolCallDetectionStrategy.reset","title":"<code>reset()</code>  <code>abstractmethod</code>","text":"<p>Reset the strategy's internal state.</p> <p>This method should be called before starting a new detection sequence to ensure no state is carried over from previous detections.</p> <p>Implementation should clear any accumulated buffers, counters, or other state variables used during detection.</p> Source code in <code>src/llm/tool_detection/base_detection_strategy.py</code> <pre><code>@abstractmethod\ndef reset(self) -&gt; None:\n    \"\"\"Reset the strategy's internal state.\n\n    This method should be called before starting a new detection sequence\n    to ensure no state is carried over from previous detections.\n\n    Implementation should clear any accumulated buffers, counters, or other\n    state variables used during detection.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llm/tool_detection/detection_result/","title":"Detection Result","text":""},{"location":"reference/llm/tool_detection/detection_result/#src.llm.tool_detection.detection_result.DetectionState","title":"<code>src.llm.tool_detection.detection_result.DetectionState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of possible tool call detection states.</p> <p>Represents the different states a tool call detection can be in during the processing of streaming chunks.</p> <p>Attributes:</p> Name Type Description <code>NO_MATCH</code> <p>No tool call pattern was detected in the current content.</p> <code>PARTIAL_MATCH</code> <p>A potential tool call was detected but is incomplete.</p> <code>COMPLETE_MATCH</code> <p>A complete and valid tool call was detected.</p> Source code in <code>src/llm/tool_detection/detection_result.py</code> <pre><code>class DetectionState(str, Enum):\n    \"\"\"Enumeration of possible tool call detection states.\n\n    Represents the different states a tool call detection can be in during\n    the processing of streaming chunks.\n\n    Attributes:\n        NO_MATCH: No tool call pattern was detected in the current content.\n        PARTIAL_MATCH: A potential tool call was detected but is incomplete.\n        COMPLETE_MATCH: A complete and valid tool call was detected.\n    \"\"\"\n    NO_MATCH = \"no_match\"\n    PARTIAL_MATCH = \"partial_match\"\n    COMPLETE_MATCH = \"complete_match\"\n</code></pre>"},{"location":"reference/llm/tool_detection/detection_result/#src.llm.tool_detection.detection_result.DetectionResult","title":"<code>src.llm.tool_detection.detection_result.DetectionResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for tool call detection results.</p> <p>Encapsulates the result of processing an SSE chunk for tool calls, including the detection state, any found tool calls, accumulated content, and the original chunk.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>DetectionState</code> <p>The current state of tool call detection. Indicates whether a tool call was found and if it's complete.</p> <code>tool_calls</code> <code>Optional[List[ToolCall]]</code> <p>List of tool calls found in the content. Only present when state is COMPLETE_MATCH.</p> <code>content</code> <code>Optional[str]</code> <p>Accumulated text content from the chunk, excluding any tool call syntax.</p> <code>sse_chunk</code> <code>Optional[SSEChunk]</code> <p>The original SSE chunk that was processed, preserved for reference or further processing.</p> Source code in <code>src/llm/tool_detection/detection_result.py</code> <pre><code>class DetectionResult(BaseModel):\n    \"\"\"Container for tool call detection results.\n\n    Encapsulates the result of processing an SSE chunk for tool calls,\n    including the detection state, any found tool calls, accumulated content,\n    and the original chunk.\n\n    Attributes:\n        state (DetectionState): The current state of tool call detection.\n            Indicates whether a tool call was found and if it's complete.\n        tool_calls (Optional[List[ToolCall]]): List of tool calls found in the\n            content. Only present when state is COMPLETE_MATCH.\n        content (Optional[str]): Accumulated text content from the chunk,\n            excluding any tool call syntax.\n        sse_chunk (Optional[SSEChunk]): The original SSE chunk that was\n            processed, preserved for reference or further processing.\n    \"\"\"\n    state: DetectionState\n    tool_calls: Optional[List[ToolCall]] = None\n    content: Optional[str] = None\n    sse_chunk: Optional[SSEChunk] = None\n</code></pre>"},{"location":"reference/llm/tool_detection/manual_detection_strategy/","title":"Manual Strategy","text":""},{"location":"reference/llm/tool_detection/manual_detection_strategy/#src.llm.tool_detection.manual_detection_strategy.ManualToolCallDetectionStrategy","title":"<code>src.llm.tool_detection.manual_detection_strategy.ManualToolCallDetectionStrategy</code>","text":"<p>               Bases: <code>BaseToolCallDetectionStrategy</code></p> <p>A strategy for detecting tool calls in streaming LLM output using pattern matching.</p> <p>This class implements manual detection of tool calls by processing streaming chunks of text using the Aho-Corasick algorithm for pattern matching. It maintains internal state to track tool call boundaries and accumulate content.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>BaseToolCallParser</code> <p>Parser instance for processing detected tool calls.</p> required <code>pattern_config_path</code> <code>str</code> <p>Path to YAML config file containing tool call patterns. Defaults to \"src/configs/tool_call_patterns.yaml\".</p> <code>'src/configs/tool_call_patterns.yaml'</code> <p>Attributes:</p> Name Type Description <code>tool_call_parser</code> <code>BaseToolCallParser</code> <p>Parser for processing tool calls.</p> <code>pattern_detector</code> <code>AhoCorasickBufferedProcessor</code> <p>Pattern matching processor.</p> <code>pre_tool_call_content</code> <code>List[str]</code> <p>Buffer for content before tool call.</p> <code>tool_call_buffer</code> <code>str</code> <p>Buffer for accumulating tool call content.</p> <code>in_tool_call</code> <code>bool</code> <p>Flag indicating if currently processing a tool call.</p> <code>accumulation_mode</code> <code>bool</code> <p>Flag for content accumulation mode.</p> Example <pre><code>parser = JSONToolCallParser()\ndetector = ManualToolCallDetectionStrategy(parser)\n\n# Process streaming chunks\nasync for chunk in stream:\n    result = await detector.detect_chunk(chunk, context)\n    if result.state == DetectionState.COMPLETE_MATCH:\n        # Handle detected tool call\n        process_tool_calls(result.tool_calls)\n</code></pre> Source code in <code>src/llm/tool_detection/manual_detection_strategy.py</code> <pre><code>class ManualToolCallDetectionStrategy(BaseToolCallDetectionStrategy):\n    \"\"\"A strategy for detecting tool calls in streaming LLM output using pattern matching.\n\n    This class implements manual detection of tool calls by processing streaming chunks\n    of text using the Aho-Corasick algorithm for pattern matching. It maintains internal\n    state to track tool call boundaries and accumulate content.\n\n    Args:\n        parser (BaseToolCallParser): Parser instance for processing detected tool calls.\n        pattern_config_path (str, optional): Path to YAML config file containing tool call patterns.\n            Defaults to \"src/configs/tool_call_patterns.yaml\".\n\n    Attributes:\n        tool_call_parser (BaseToolCallParser): Parser for processing tool calls.\n        pattern_detector (AhoCorasickBufferedProcessor): Pattern matching processor.\n        pre_tool_call_content (List[str]): Buffer for content before tool call.\n        tool_call_buffer (str): Buffer for accumulating tool call content.\n        in_tool_call (bool): Flag indicating if currently processing a tool call.\n        accumulation_mode (bool): Flag for content accumulation mode.\n\n    Example:\n        ```python\n        parser = JSONToolCallParser()\n        detector = ManualToolCallDetectionStrategy(parser)\n\n        # Process streaming chunks\n        async for chunk in stream:\n            result = await detector.detect_chunk(chunk, context)\n            if result.state == DetectionState.COMPLETE_MATCH:\n                # Handle detected tool call\n                process_tool_calls(result.tool_calls)\n        ```\n    \"\"\"\n\n    def __init__(self, parser: BaseToolCallParser, pattern_config_path: str = \"src/configs/tool_call_patterns.yaml\"):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing ManualToolCallDetectionStrategy with config: %s\", pattern_config_path)\n        self.tool_call_parser = parser\n        self.pattern_detector = AhoCorasickBufferedProcessorNormalized(pattern_config_path)\n\n        self.pre_tool_call_content: List[str] = []\n        self.tool_call_buffer: str = \"\"\n        self.in_tool_call: bool = False\n        self.accumulation_mode: bool = False\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset all internal state of the detection strategy.\n\n        This method clears all buffers and resets flags to their initial state.\n        Should be called between processing different streams or after errors.\n        \"\"\"\n        self.logger.debug(\"Resetting detector state\")\n        self.pattern_detector.reset_states()\n        self.pre_tool_call_content = []\n        self.tool_call_buffer = \"\"\n        self.in_tool_call = False\n        self.accumulation_mode = False\n\n    async def detect_chunk(self, sse_chunk: SSEChunk, context: StreamContext) -&gt; DetectionResult:\n        \"\"\"\n        Process a single chunk of streaming content for tool call detection.\n\n        Args:\n            sse_chunk (SSEChunk): The chunk of streaming content to process.\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Result of processing the chunk, including detection state\n                and any content or tool calls found.\n        \"\"\"\n        # If the chunk has no valid delta content, return NO_MATCH.\n        if not sse_chunk.choices or not sse_chunk.choices[0].delta:\n            return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n        chunk_content = sse_chunk.choices[0].delta.content\n        if not chunk_content:\n            return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n        # If already in tool call detection mode, continue accumulating and report a partial match.\n        if self.in_tool_call:\n            self.tool_call_buffer += chunk_content\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                sse_chunk=sse_chunk\n            )\n\n        # Process the chunk through the pattern detector.\n        result = await self.pattern_detector.process_chunk(chunk_content)\n\n        if result.error:\n            return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n        # If a pattern is matched, switch into tool call mode.\n        # Return any content remaining that could have been in the same chunk as the patter or in a buffer\n        if result.matched:\n            self.in_tool_call = True\n            self.tool_call_buffer = result.text_with_tool_call\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=result.output,\n                sse_chunk=sse_chunk\n            )\n\n        # For regular content, if there is any output, return it as PARTIAL_MATCH.\n        if result.output:\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=result.output,\n                sse_chunk=sse_chunk\n            )\n\n        # If nothing of interest is found, return NO_MATCH.\n        return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n    async def finalize_detection(self, context: StreamContext) -&gt; DetectionResult:\n        \"\"\"Finalize the detection process and handle any accumulated content.\n\n        This method is called at the end of a stream to process any remaining content\n        in the buffers and return final detection results.\n\n        Args:\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Final result of the detection process, including any\n                complete tool calls or remaining content.\n        \"\"\"\n        self.logger.debug(\"Finalizing detection\")\n        # Flush any remaining content from pattern detector\n        final_result = await self.pattern_detector.flush_buffer()\n\n        if self.in_tool_call:\n            self.logger.debug(\"Processing final tool call buffer\")\n            if final_result.output:\n                self.tool_call_buffer += final_result.output\n\n            # Parse accumulated tool call\n            parsed_tool_call_data = self.tool_call_parser.parse(self.tool_call_buffer)\n            self.logger.debug(f\"Tool call buffer: {self.tool_call_buffer}\")\n            self.logger.debug(f\"Parsed tool call data: {parsed_tool_call_data}\")\n\n            if \"error\" in parsed_tool_call_data:\n                self.logger.error(f\"Tool call parsing failed: {parsed_tool_call_data['error']}\")\n                return DetectionResult(\n                    state=DetectionState.NO_MATCH,\n                    content=\"Sorry, but I was unable to complete your request. Please try again.\",\n                )\n\n            parsed_tool_calls = self._extract_tool_calls(parsed_tool_call_data)\n            self.logger.debug(\"Successfully parsed %d tool calls\", len(parsed_tool_calls))\n\n            return DetectionResult(\n                state=DetectionState.COMPLETE_MATCH,\n                tool_calls=parsed_tool_calls\n            )\n\n        # No tool call detected, return any final content\n        if final_result.output:\n            self.logger.debug(\"Returning final content: %s\", final_result.output[:50])\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=final_result.output\n            )\n\n        self.logger.debug(\"No final content to return\")\n        return DetectionResult(state=DetectionState.NO_MATCH)\n\n    def _extract_tool_calls(self, parsed_output: dict) -&gt; List[ToolCall]:\n        \"\"\"Extract structured tool calls from parsed JSON output.\n\n        Converts the parsed JSON format into a list of ToolCall objects with\n        appropriate structure and typing.\n\n        Args:\n            parsed_output (dict): The parsed JSON output containing tool call data.\n\n        Returns:\n            List[ToolCall]: List of structured tool call objects ready for processing.\n        \"\"\"\n        tool_calls = []\n        for tool_call_dict in parsed_output.get(\"tool_calls\", []):\n            tool_call_args = tool_call_dict.get(\"parameters\", tool_call_dict.get(\"arguments\"))\n            self.logger.debug(\"Extracting tool call arguments: %s\", tool_call_args)\n            tool_calls.append(ToolCall(\n                id='123456789',  # Placeholder ID; modify as needed\n                type=tool_call_dict.get(\"type\", \"function\"),\n                function=FunctionDetail(\n                    name=tool_call_dict.get(\"name\"),\n                    arguments=str(tool_call_args)\n                )\n            ))\n        return tool_calls\n</code></pre>"},{"location":"reference/llm/tool_detection/manual_detection_strategy/#src.llm.tool_detection.manual_detection_strategy.ManualToolCallDetectionStrategy.detect_chunk","title":"<code>detect_chunk(sse_chunk, context)</code>  <code>async</code>","text":"<p>Process a single chunk of streaming content for tool call detection.</p> <p>Parameters:</p> Name Type Description Default <code>sse_chunk</code> <code>SSEChunk</code> <p>The chunk of streaming content to process.</p> required <code>context</code> <code>StreamContext</code> <p>Context information for the current stream.</p> required <p>Returns:</p> Name Type Description <code>DetectionResult</code> <code>DetectionResult</code> <p>Result of processing the chunk, including detection state and any content or tool calls found.</p> Source code in <code>src/llm/tool_detection/manual_detection_strategy.py</code> <pre><code>async def detect_chunk(self, sse_chunk: SSEChunk, context: StreamContext) -&gt; DetectionResult:\n    \"\"\"\n    Process a single chunk of streaming content for tool call detection.\n\n    Args:\n        sse_chunk (SSEChunk): The chunk of streaming content to process.\n        context (StreamContext): Context information for the current stream.\n\n    Returns:\n        DetectionResult: Result of processing the chunk, including detection state\n            and any content or tool calls found.\n    \"\"\"\n    # If the chunk has no valid delta content, return NO_MATCH.\n    if not sse_chunk.choices or not sse_chunk.choices[0].delta:\n        return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n    chunk_content = sse_chunk.choices[0].delta.content\n    if not chunk_content:\n        return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n    # If already in tool call detection mode, continue accumulating and report a partial match.\n    if self.in_tool_call:\n        self.tool_call_buffer += chunk_content\n        return DetectionResult(\n            state=DetectionState.PARTIAL_MATCH,\n            sse_chunk=sse_chunk\n        )\n\n    # Process the chunk through the pattern detector.\n    result = await self.pattern_detector.process_chunk(chunk_content)\n\n    if result.error:\n        return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n    # If a pattern is matched, switch into tool call mode.\n    # Return any content remaining that could have been in the same chunk as the patter or in a buffer\n    if result.matched:\n        self.in_tool_call = True\n        self.tool_call_buffer = result.text_with_tool_call\n        return DetectionResult(\n            state=DetectionState.PARTIAL_MATCH,\n            content=result.output,\n            sse_chunk=sse_chunk\n        )\n\n    # For regular content, if there is any output, return it as PARTIAL_MATCH.\n    if result.output:\n        return DetectionResult(\n            state=DetectionState.PARTIAL_MATCH,\n            content=result.output,\n            sse_chunk=sse_chunk\n        )\n\n    # If nothing of interest is found, return NO_MATCH.\n    return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n</code></pre>"},{"location":"reference/llm/tool_detection/manual_detection_strategy/#src.llm.tool_detection.manual_detection_strategy.ManualToolCallDetectionStrategy.finalize_detection","title":"<code>finalize_detection(context)</code>  <code>async</code>","text":"<p>Finalize the detection process and handle any accumulated content.</p> <p>This method is called at the end of a stream to process any remaining content in the buffers and return final detection results.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StreamContext</code> <p>Context information for the current stream.</p> required <p>Returns:</p> Name Type Description <code>DetectionResult</code> <code>DetectionResult</code> <p>Final result of the detection process, including any complete tool calls or remaining content.</p> Source code in <code>src/llm/tool_detection/manual_detection_strategy.py</code> <pre><code>async def finalize_detection(self, context: StreamContext) -&gt; DetectionResult:\n    \"\"\"Finalize the detection process and handle any accumulated content.\n\n    This method is called at the end of a stream to process any remaining content\n    in the buffers and return final detection results.\n\n    Args:\n        context (StreamContext): Context information for the current stream.\n\n    Returns:\n        DetectionResult: Final result of the detection process, including any\n            complete tool calls or remaining content.\n    \"\"\"\n    self.logger.debug(\"Finalizing detection\")\n    # Flush any remaining content from pattern detector\n    final_result = await self.pattern_detector.flush_buffer()\n\n    if self.in_tool_call:\n        self.logger.debug(\"Processing final tool call buffer\")\n        if final_result.output:\n            self.tool_call_buffer += final_result.output\n\n        # Parse accumulated tool call\n        parsed_tool_call_data = self.tool_call_parser.parse(self.tool_call_buffer)\n        self.logger.debug(f\"Tool call buffer: {self.tool_call_buffer}\")\n        self.logger.debug(f\"Parsed tool call data: {parsed_tool_call_data}\")\n\n        if \"error\" in parsed_tool_call_data:\n            self.logger.error(f\"Tool call parsing failed: {parsed_tool_call_data['error']}\")\n            return DetectionResult(\n                state=DetectionState.NO_MATCH,\n                content=\"Sorry, but I was unable to complete your request. Please try again.\",\n            )\n\n        parsed_tool_calls = self._extract_tool_calls(parsed_tool_call_data)\n        self.logger.debug(\"Successfully parsed %d tool calls\", len(parsed_tool_calls))\n\n        return DetectionResult(\n            state=DetectionState.COMPLETE_MATCH,\n            tool_calls=parsed_tool_calls\n        )\n\n    # No tool call detected, return any final content\n    if final_result.output:\n        self.logger.debug(\"Returning final content: %s\", final_result.output[:50])\n        return DetectionResult(\n            state=DetectionState.PARTIAL_MATCH,\n            content=final_result.output\n        )\n\n    self.logger.debug(\"No final content to return\")\n    return DetectionResult(state=DetectionState.NO_MATCH)\n</code></pre>"},{"location":"reference/llm/tool_detection/manual_detection_strategy/#src.llm.tool_detection.manual_detection_strategy.ManualToolCallDetectionStrategy.reset","title":"<code>reset()</code>","text":"<p>Reset all internal state of the detection strategy.</p> <p>This method clears all buffers and resets flags to their initial state. Should be called between processing different streams or after errors.</p> Source code in <code>src/llm/tool_detection/manual_detection_strategy.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all internal state of the detection strategy.\n\n    This method clears all buffers and resets flags to their initial state.\n    Should be called between processing different streams or after errors.\n    \"\"\"\n    self.logger.debug(\"Resetting detector state\")\n    self.pattern_detector.reset_states()\n    self.pre_tool_call_content = []\n    self.tool_call_buffer = \"\"\n    self.in_tool_call = False\n    self.accumulation_mode = False\n</code></pre>"},{"location":"reference/llm/tool_detection/vendor_detection_strategy/","title":"Vendor Strategy","text":""},{"location":"reference/llm/tool_detection/vendor_detection_strategy/#src.llm.tool_detection.vendor_detection_strategy.VendorToolCallDetectionStrategy","title":"<code>src.llm.tool_detection.vendor_detection_strategy.VendorToolCallDetectionStrategy</code>","text":"<p>               Bases: <code>BaseToolCallDetectionStrategy</code></p> <p>A strategy for detecting tool calls using vendor-provided metadata in SSE chunks.</p> <p>This strategy processes tool calls by accumulating function names and arguments across multiple Server-Sent Events (SSE) chunks. It relies on vendor-specific metadata in the chunks to identify tool calls and their completion status.</p> <p>Attributes:</p> Name Type Description <code>found_complete_call</code> <code>bool</code> <p>Flag indicating if a complete tool call was found.</p> <code>collected_tool_calls</code> <code>List[ToolCall]</code> <p>List of fully collected tool calls.</p> <code>partial_name</code> <code>str</code> <p>Buffer for accumulating function name.</p> <code>partial_args</code> <code>str</code> <p>Buffer for accumulating function arguments.</p> Example <pre><code>detector = VendorToolCallDetectionStrategy()\n\nasync for chunk in stream:\n    result = await detector.detect_chunk(chunk, context)\n    if result.state == DetectionState.COMPLETE_MATCH:\n        tool_calls = result.tool_calls\n        # Process the complete tool calls\n</code></pre> Source code in <code>src/llm/tool_detection/vendor_detection_strategy.py</code> <pre><code>class VendorToolCallDetectionStrategy(BaseToolCallDetectionStrategy):\n    \"\"\"A strategy for detecting tool calls using vendor-provided metadata in SSE chunks.\n\n    This strategy processes tool calls by accumulating function names and arguments\n    across multiple Server-Sent Events (SSE) chunks. It relies on vendor-specific\n    metadata in the chunks to identify tool calls and their completion status.\n\n    Attributes:\n        found_complete_call (bool): Flag indicating if a complete tool call was found.\n        collected_tool_calls (List[ToolCall]): List of fully collected tool calls.\n        partial_name (str): Buffer for accumulating function name.\n        partial_args (str): Buffer for accumulating function arguments.\n\n    Example:\n        ```python\n        detector = VendorToolCallDetectionStrategy()\n\n        async for chunk in stream:\n            result = await detector.detect_chunk(chunk, context)\n            if result.state == DetectionState.COMPLETE_MATCH:\n                tool_calls = result.tool_calls\n                # Process the complete tool calls\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the vendor tool call detection strategy.\"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing VendorToolCallDetectionStrategy\")\n        self.found_complete_call = None\n        self.collected_tool_calls = None\n        self.partial_name = None\n        self.partial_args = None\n        self.reset()\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset all stored tool call information to initial state.\n\n        This method clears all accumulated data and resets flags, preparing the\n        detector for processing a new stream of chunks.\n        \"\"\"\n        self.logger.debug(\"Resetting detector state\")\n        self.partial_name = None\n        self.partial_args = \"\"  # Accumulates streamed arguments\n        self.collected_tool_calls = []\n        self.found_complete_call = False\n\n    async def detect_chunk(\n            self,\n            sse_chunk: SSEChunk,\n            context: StreamContext\n    ) -&gt; DetectionResult:\n        \"\"\"Process an SSE chunk for tool call detection.\n\n        Analyzes the chunk for tool call information using vendor-provided metadata.\n        Accumulates partial tool calls across multiple chunks until a complete call\n        is detected.\n\n        Args:\n            sse_chunk (SSEChunk): The chunk of streaming content to process.\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Result of processing the chunk, including detection state\n                and any content or tool calls found.\n\n        Note:\n            This method maintains state between calls detect_chunk() to properly handle tool calls\n            that span multiple chunks. It relies on the finish_reason field to\n            determine when a tool call is complete.\n        \"\"\"\n        tool_call_data = None\n        if not sse_chunk.choices:\n            return DetectionResult(state=DetectionState.NO_MATCH)\n\n        delta = sse_chunk.choices[0].delta\n        finish_reason = sse_chunk.choices[0].finish_reason\n\n        text_content = delta.content if delta.content else None\n\n        # Check if tool call data is present in this chunk\n        tool_calls = delta.tool_calls if delta.tool_calls else None\n\n        if tool_calls:\n            tool_call_data = tool_calls[0]  # Assuming index 0 for simplicity\n            function_name = tool_call_data.function.name if tool_call_data.function else None\n            arguments = tool_call_data.function.arguments if tool_call_data.function else None\n\n            # If this chunk contains a function name, store it\n            if function_name:\n                self.partial_name = function_name\n\n            # If arguments are being streamed, accumulate them\n            if arguments:\n                self.partial_args += arguments\n\n        # If finish_reason indicates the tool call is complete, finalize it\n        if finish_reason in [\"tool_calls\", \"tool_use\"]:\n            if self.partial_name:\n                try:\n                    parsed_args = json.loads(self.partial_args) if self.partial_args else {}\n                except json.JSONDecodeError:\n                    self.logger.warning(\"Failed to parse arguments as JSON: %s\", self.partial_args[:50])\n                    parsed_args = {\"_malformed\": self.partial_args}\n\n                tool_call = ToolCall(\n                    id=(tool_call_data and tool_call_data.id) or \"call_generated\",\n                    function=FunctionDetail(\n                        name=self.partial_name,\n                        arguments=str(parsed_args)\n                    )\n                )\n                self.collected_tool_calls.append(tool_call)\n                self.found_complete_call = True\n\n                return DetectionResult(\n                    state=DetectionState.COMPLETE_MATCH,\n                    tool_calls=[tool_call],\n                    content=text_content\n                )\n\n        # If we're still collecting tool call data, return PARTIAL_MATCH\n        if self.partial_name or self.partial_args:\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=text_content\n            )\n\n        # Otherwise, just return NO_MATCH and pass the text through\n        return DetectionResult(\n            state=DetectionState.NO_MATCH,\n            content=text_content\n        )\n\n    async def finalize_detection(self, context: StreamContext) -&gt; DetectionResult:\n        \"\"\"Finalize the detection process and handle any accumulated tool calls.\n\n        This method is called at the end of the SSE stream to process any remaining\n        tool call data and return final results.\n\n        Args:\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Final result of the detection process, including any\n                complete tool calls or remaining content.\n\n        Note:\n            This method handles cleanup of partial tool calls that were never\n            completed due to stream termination.\n        \"\"\"\n        self.logger.debug(\"Finalizing detection\")\n        if self.found_complete_call:\n            self.logger.debug(\"Returning %d collected tool calls\", len(self.collected_tool_calls))\n            return DetectionResult(\n                state=DetectionState.COMPLETE_MATCH,\n                tool_calls=self.collected_tool_calls\n            )\n\n        if self.partial_name or self.partial_args:\n            self.logger.debug(\"Incomplete tool call data at stream end\")\n            self.logger.debug(f\"Name: {self.partial_name}, Args: {self.partial_args}\")\n            return DetectionResult(state=DetectionState.NO_MATCH)\n\n        self.logger.debug(\"No tool calls to finalize\")\n        return DetectionResult(state=DetectionState.NO_MATCH)\n</code></pre>"},{"location":"reference/llm/tool_detection/vendor_detection_strategy/#src.llm.tool_detection.vendor_detection_strategy.VendorToolCallDetectionStrategy.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the vendor tool call detection strategy.</p> Source code in <code>src/llm/tool_detection/vendor_detection_strategy.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the vendor tool call detection strategy.\"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.logger.debug(\"Initializing VendorToolCallDetectionStrategy\")\n    self.found_complete_call = None\n    self.collected_tool_calls = None\n    self.partial_name = None\n    self.partial_args = None\n    self.reset()\n</code></pre>"},{"location":"reference/llm/tool_detection/vendor_detection_strategy/#src.llm.tool_detection.vendor_detection_strategy.VendorToolCallDetectionStrategy.detect_chunk","title":"<code>detect_chunk(sse_chunk, context)</code>  <code>async</code>","text":"<p>Process an SSE chunk for tool call detection.</p> <p>Analyzes the chunk for tool call information using vendor-provided metadata. Accumulates partial tool calls across multiple chunks until a complete call is detected.</p> <p>Parameters:</p> Name Type Description Default <code>sse_chunk</code> <code>SSEChunk</code> <p>The chunk of streaming content to process.</p> required <code>context</code> <code>StreamContext</code> <p>Context information for the current stream.</p> required <p>Returns:</p> Name Type Description <code>DetectionResult</code> <code>DetectionResult</code> <p>Result of processing the chunk, including detection state and any content or tool calls found.</p> Note <p>This method maintains state between calls detect_chunk() to properly handle tool calls that span multiple chunks. It relies on the finish_reason field to determine when a tool call is complete.</p> Source code in <code>src/llm/tool_detection/vendor_detection_strategy.py</code> <pre><code>async def detect_chunk(\n        self,\n        sse_chunk: SSEChunk,\n        context: StreamContext\n) -&gt; DetectionResult:\n    \"\"\"Process an SSE chunk for tool call detection.\n\n    Analyzes the chunk for tool call information using vendor-provided metadata.\n    Accumulates partial tool calls across multiple chunks until a complete call\n    is detected.\n\n    Args:\n        sse_chunk (SSEChunk): The chunk of streaming content to process.\n        context (StreamContext): Context information for the current stream.\n\n    Returns:\n        DetectionResult: Result of processing the chunk, including detection state\n            and any content or tool calls found.\n\n    Note:\n        This method maintains state between calls detect_chunk() to properly handle tool calls\n        that span multiple chunks. It relies on the finish_reason field to\n        determine when a tool call is complete.\n    \"\"\"\n    tool_call_data = None\n    if not sse_chunk.choices:\n        return DetectionResult(state=DetectionState.NO_MATCH)\n\n    delta = sse_chunk.choices[0].delta\n    finish_reason = sse_chunk.choices[0].finish_reason\n\n    text_content = delta.content if delta.content else None\n\n    # Check if tool call data is present in this chunk\n    tool_calls = delta.tool_calls if delta.tool_calls else None\n\n    if tool_calls:\n        tool_call_data = tool_calls[0]  # Assuming index 0 for simplicity\n        function_name = tool_call_data.function.name if tool_call_data.function else None\n        arguments = tool_call_data.function.arguments if tool_call_data.function else None\n\n        # If this chunk contains a function name, store it\n        if function_name:\n            self.partial_name = function_name\n\n        # If arguments are being streamed, accumulate them\n        if arguments:\n            self.partial_args += arguments\n\n    # If finish_reason indicates the tool call is complete, finalize it\n    if finish_reason in [\"tool_calls\", \"tool_use\"]:\n        if self.partial_name:\n            try:\n                parsed_args = json.loads(self.partial_args) if self.partial_args else {}\n            except json.JSONDecodeError:\n                self.logger.warning(\"Failed to parse arguments as JSON: %s\", self.partial_args[:50])\n                parsed_args = {\"_malformed\": self.partial_args}\n\n            tool_call = ToolCall(\n                id=(tool_call_data and tool_call_data.id) or \"call_generated\",\n                function=FunctionDetail(\n                    name=self.partial_name,\n                    arguments=str(parsed_args)\n                )\n            )\n            self.collected_tool_calls.append(tool_call)\n            self.found_complete_call = True\n\n            return DetectionResult(\n                state=DetectionState.COMPLETE_MATCH,\n                tool_calls=[tool_call],\n                content=text_content\n            )\n\n    # If we're still collecting tool call data, return PARTIAL_MATCH\n    if self.partial_name or self.partial_args:\n        return DetectionResult(\n            state=DetectionState.PARTIAL_MATCH,\n            content=text_content\n        )\n\n    # Otherwise, just return NO_MATCH and pass the text through\n    return DetectionResult(\n        state=DetectionState.NO_MATCH,\n        content=text_content\n    )\n</code></pre>"},{"location":"reference/llm/tool_detection/vendor_detection_strategy/#src.llm.tool_detection.vendor_detection_strategy.VendorToolCallDetectionStrategy.finalize_detection","title":"<code>finalize_detection(context)</code>  <code>async</code>","text":"<p>Finalize the detection process and handle any accumulated tool calls.</p> <p>This method is called at the end of the SSE stream to process any remaining tool call data and return final results.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StreamContext</code> <p>Context information for the current stream.</p> required <p>Returns:</p> Name Type Description <code>DetectionResult</code> <code>DetectionResult</code> <p>Final result of the detection process, including any complete tool calls or remaining content.</p> Note <p>This method handles cleanup of partial tool calls that were never completed due to stream termination.</p> Source code in <code>src/llm/tool_detection/vendor_detection_strategy.py</code> <pre><code>async def finalize_detection(self, context: StreamContext) -&gt; DetectionResult:\n    \"\"\"Finalize the detection process and handle any accumulated tool calls.\n\n    This method is called at the end of the SSE stream to process any remaining\n    tool call data and return final results.\n\n    Args:\n        context (StreamContext): Context information for the current stream.\n\n    Returns:\n        DetectionResult: Final result of the detection process, including any\n            complete tool calls or remaining content.\n\n    Note:\n        This method handles cleanup of partial tool calls that were never\n        completed due to stream termination.\n    \"\"\"\n    self.logger.debug(\"Finalizing detection\")\n    if self.found_complete_call:\n        self.logger.debug(\"Returning %d collected tool calls\", len(self.collected_tool_calls))\n        return DetectionResult(\n            state=DetectionState.COMPLETE_MATCH,\n            tool_calls=self.collected_tool_calls\n        )\n\n    if self.partial_name or self.partial_args:\n        self.logger.debug(\"Incomplete tool call data at stream end\")\n        self.logger.debug(f\"Name: {self.partial_name}, Args: {self.partial_args}\")\n        return DetectionResult(state=DetectionState.NO_MATCH)\n\n    self.logger.debug(\"No tool calls to finalize\")\n    return DetectionResult(state=DetectionState.NO_MATCH)\n</code></pre>"},{"location":"reference/llm/tool_detection/vendor_detection_strategy/#src.llm.tool_detection.vendor_detection_strategy.VendorToolCallDetectionStrategy.reset","title":"<code>reset()</code>","text":"<p>Reset all stored tool call information to initial state.</p> <p>This method clears all accumulated data and resets flags, preparing the detector for processing a new stream of chunks.</p> Source code in <code>src/llm/tool_detection/vendor_detection_strategy.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all stored tool call information to initial state.\n\n    This method clears all accumulated data and resets flags, preparing the\n    detector for processing a new stream of chunks.\n    \"\"\"\n    self.logger.debug(\"Resetting detector state\")\n    self.partial_name = None\n    self.partial_args = \"\"  # Accumulates streamed arguments\n    self.collected_tool_calls = []\n    self.found_complete_call = False\n</code></pre>"},{"location":"reference/prompt_builders/","title":"Prompt Builders Documentation","text":""},{"location":"reference/prompt_builders/#overview","title":"Overview","text":"<p>The Prompt Builders module provides utilities for constructing prompts tailored to various LLM (Large Language Model) providers. These builders help structure prompts according to the expected input format of different models, ensuring optimal performance.</p>"},{"location":"reference/prompt_builders/#components","title":"Components","text":""},{"location":"reference/prompt_builders/#base-prompt-builder","title":"Base Prompt Builder","text":"<ul> <li><code>BasePromptBuilder</code>: The foundational class that all other prompt builders extend.</li> </ul>"},{"location":"reference/prompt_builders/#provider-specific-prompt-builders","title":"Provider-Specific Prompt Builders","text":"<ul> <li><code>AnthropicPromptBuilder</code>: Generates prompts formatted for Anthropic's Claude models.</li> <li><code>MistralAIPromptBuilder</code>: Constructs prompts for Mistral AI models.</li> <li><code>OpenAIPromptBuilder</code>: Handles prompt generation for OpenAI's GPT models.</li> <li><code>GranitePromptBuilder</code>: Tailored for WatsonX Granite models.</li> <li><code>LlamaPromptBuilder</code>: Supports WatsonX Llama models.</li> <li><code>MistralPromptBuilder</code>: Specialized for WatsonX Mistral models.</li> <li><code>XAIPromptBuilder</code>: Specialized for xAI models.</li> </ul>"},{"location":"reference/prompt_builders/#prompt-models","title":"Prompt Models","text":"<ul> <li><code>PromptPayload</code>: Defines the structure of the prompt content.</li> <li><code>PromptBuilderOutput</code>: Represents the formatted output of a prompt builder.</li> </ul>"},{"location":"reference/prompt_builders/#using-different-providers","title":"Using Different Providers","text":"<p>Each builder follows a similar pattern but optimizes prompt formatting based on the model requirements.</p>"},{"location":"reference/prompt_builders/anthropic_prompt_builder/","title":"Anthropic","text":""},{"location":"reference/prompt_builders/anthropic_prompt_builder/#src.prompt_builders.anthropic.anthropic_prompt_builder.AnthropicPromptBuilder","title":"<code>src.prompt_builders.anthropic.anthropic_prompt_builder.AnthropicPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>A prompt builder specialized for Anthropic chat completion models.</p> <p>This class handles the construction of prompts for Anthropic models, with special handling for tool definitions and system messages. It loads configuration from a YAML file and supports embedding tool information into the conversation history.</p> Example <pre><code>builder = AnthropicPromptBuilder()\npayload = PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n)\noutput = await builder.build_chat(payload)\n</code></pre> Source code in <code>src/prompt_builders/anthropic/anthropic_prompt_builder.py</code> <pre><code>class AnthropicPromptBuilder(BasePromptBuilder):\n    \"\"\"A prompt builder specialized for Anthropic chat completion models.\n\n    This class handles the construction of prompts for Anthropic models,\n    with special handling for tool definitions and system messages. It loads\n    configuration from a YAML file and supports embedding tool information into\n    the conversation history.\n\n    Example:\n        ```python\n        builder = AnthropicPromptBuilder()\n        payload = PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        )\n        output = await builder.build_chat(payload)\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the Anthropic prompt builder.\n\n        Loads configuration from the prompt_builders.yaml file and sets up logging.\n        Raises FileNotFoundError if the config file is not found.\n        \"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing AnthropicPromptBuilder\")\n        super().__init__()\n        self.config = self._load_config()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build a chat completion prompt with optional tool definitions.\n\n        Constructs a prompt by potentially modifying the conversation history to\n        include tool information. If tools are defined, they are added to or merged\n        with the system message.\n\n        Args:\n            payload (PromptPayload): Contains conversation history and optional tool\n                definitions. History should be a list of message objects, and tool\n                definitions should be a list of tool specification objects.\n\n        Returns:\n            PromptBuilderOutput: Contains the modified chat messages ready for use\n                with Anthropic's chat completion API.\n\n        Note:\n            If the first message in history is a system message, tool information\n            will be prepended to it. Otherwise, a new system message will be created.\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        if not tool_definitions:\n            self.logger.debug(\"No tool definitions provided, returning original history\")\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        # Extract tool names from the tool definitions.\n        tool_names = [tool.function.name for tool in tool_definitions]\n\n        # Build a header for the tool section using our config.\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n\n        # Build the tool information block.\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(\n                content=f\"## tools:\\n\\n{existing_content}\\n{tool_info}\\n\\n\"\n            )\n        else:\n            system_msg = SystemMessage(content=tool_info)\n            modified_history.insert(0, system_msg)\n\n        self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, context: Dict) -&gt; str:\n        \"\"\"Text completion is not supported for Anthropic models.\n\n        Args:\n            context (Dict): Unused context dictionary.\n\n        Raises:\n            NotImplementedError: Always raised as this method is not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"Anthropic models primarily use chat completions. Use build_chat() instead.\"\n        )\n\n    @staticmethod\n    def _load_config() -&gt; Dict:\n        \"\"\"Load the Anthropic-specific configuration from the prompt builders YAML file.\n\n        Returns:\n            Dict: Configuration dictionary containing Anthropic-specific settings.\n\n        Raises:\n            FileNotFoundError: If the config file doesn't exist.\n            yaml.YAMLError: If the config file is malformed.\n        \"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('anthropic', {})\n</code></pre>"},{"location":"reference/prompt_builders/anthropic_prompt_builder/#src.prompt_builders.anthropic.anthropic_prompt_builder.AnthropicPromptBuilder.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Anthropic prompt builder.</p> <p>Loads configuration from the prompt_builders.yaml file and sets up logging. Raises FileNotFoundError if the config file is not found.</p> Source code in <code>src/prompt_builders/anthropic/anthropic_prompt_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the Anthropic prompt builder.\n\n    Loads configuration from the prompt_builders.yaml file and sets up logging.\n    Raises FileNotFoundError if the config file is not found.\n    \"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.logger.debug(\"Initializing AnthropicPromptBuilder\")\n    super().__init__()\n    self.config = self._load_config()\n</code></pre>"},{"location":"reference/prompt_builders/anthropic_prompt_builder/#src.prompt_builders.anthropic.anthropic_prompt_builder.AnthropicPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build a chat completion prompt with optional tool definitions.</p> <p>Constructs a prompt by potentially modifying the conversation history to include tool information. If tools are defined, they are added to or merged with the system message.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>Contains conversation history and optional tool definitions. History should be a list of message objects, and tool definitions should be a list of tool specification objects.</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified chat messages ready for use with Anthropic's chat completion API.</p> Note <p>If the first message in history is a system message, tool information will be prepended to it. Otherwise, a new system message will be created.</p> Source code in <code>src/prompt_builders/anthropic/anthropic_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build a chat completion prompt with optional tool definitions.\n\n    Constructs a prompt by potentially modifying the conversation history to\n    include tool information. If tools are defined, they are added to or merged\n    with the system message.\n\n    Args:\n        payload (PromptPayload): Contains conversation history and optional tool\n            definitions. History should be a list of message objects, and tool\n            definitions should be a list of tool specification objects.\n\n    Returns:\n        PromptBuilderOutput: Contains the modified chat messages ready for use\n            with Anthropic's chat completion API.\n\n    Note:\n        If the first message in history is a system message, tool information\n        will be prepended to it. Otherwise, a new system message will be created.\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    if not tool_definitions:\n        self.logger.debug(\"No tool definitions provided, returning original history\")\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    # Extract tool names from the tool definitions.\n    tool_names = [tool.function.name for tool in tool_definitions]\n\n    # Build a header for the tool section using our config.\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n\n    # Build the tool information block.\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(\n            content=f\"## tools:\\n\\n{existing_content}\\n{tool_info}\\n\\n\"\n        )\n    else:\n        system_msg = SystemMessage(content=tool_info)\n        modified_history.insert(0, system_msg)\n\n    self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/anthropic_prompt_builder/#src.prompt_builders.anthropic.anthropic_prompt_builder.AnthropicPromptBuilder.build_text","title":"<code>build_text(context)</code>  <code>async</code>","text":"<p>Text completion is not supported for Anthropic models.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Dict</code> <p>Unused context dictionary.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised as this method is not supported.</p> Source code in <code>src/prompt_builders/anthropic/anthropic_prompt_builder.py</code> <pre><code>async def build_text(self, context: Dict) -&gt; str:\n    \"\"\"Text completion is not supported for Anthropic models.\n\n    Args:\n        context (Dict): Unused context dictionary.\n\n    Raises:\n        NotImplementedError: Always raised as this method is not supported.\n    \"\"\"\n    raise NotImplementedError(\n        \"Anthropic models primarily use chat completions. Use build_chat() instead.\"\n    )\n</code></pre>"},{"location":"reference/prompt_builders/base_prompt_builder/","title":"Base Builder","text":""},{"location":"reference/prompt_builders/base_prompt_builder/#src.prompt_builders.base_prompt_builder.BasePromptBuilder","title":"<code>src.prompt_builders.base_prompt_builder.BasePromptBuilder</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/prompt_builders/base_prompt_builder.py</code> <pre><code>class BasePromptBuilder(ABC):\n    @abstractmethod\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"\n        Build prompt string for text generation endpoint.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data.\n\n        Returns:\n            str: Formatted prompt string for text generation\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"\n        Build message list for chat completions endpoint.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data.\n\n        Returns:\n            PromptBuilderOutput: Contains either text_prompt or chat_messages for the LLM\n        \"\"\"\n        pass\n\n    @staticmethod\n    async def _build_system_content(tool_definitions: List[Tool], header: str, instructions: str) -&gt; str:\n        \"\"\"Build system message content incorporating tool information.\n\n        Args:\n            tool_definitions (List[Tool]): List of tool definitions to include\n            header (str): Header text for the system message\n            instructions (str): Instructions text for tool usage\n\n        Returns:\n            str: Formatted system message content with tool information\n\n        Note:\n            Should only be called when tool_definitions is non-empty.\n        \"\"\"\n        tool_sections = []\n        for tool in tool_definitions:\n            tool_str = (\n                f\"Use the function '{tool.function.name}' to: {tool.function.description}\\n\"\n                f\"{tool.function.parameters.model_dump_json()}\\n\"\n            )\n            tool_sections.append(tool_str)\n\n        return f\"{header}\\n\\n{instructions}\\n\\n\" + \"\\n\".join(tool_sections)\n\n    @staticmethod\n    def _format_conversation_history(\n            messages: List[TextChatMessage],\n            include_roles: bool = True\n    ) -&gt; str:\n        \"\"\"\n        Helper method to format conversation history as a string.\n\n        Args:\n            messages: List of chat messages to format\n            include_roles: Whether to include role labels in the output\n\n        Returns:\n            str: Formatted conversation history\n        \"\"\"\n        formatted = []\n        for msg in messages:\n            if include_roles:\n                formatted.append(f\"{msg.role}: {msg.content}\")\n            else:\n                formatted.append(msg.content)\n        return \"\\n\\n\".join(formatted)\n</code></pre>"},{"location":"reference/prompt_builders/base_prompt_builder/#src.prompt_builders.base_prompt_builder.BasePromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Build message list for chat completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data.</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains either text_prompt or chat_messages for the LLM</p> Source code in <code>src/prompt_builders/base_prompt_builder.py</code> <pre><code>@abstractmethod\nasync def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"\n    Build message list for chat completions endpoint.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data.\n\n    Returns:\n        PromptBuilderOutput: Contains either text_prompt or chat_messages for the LLM\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompt_builders/base_prompt_builder/#src.prompt_builders.base_prompt_builder.BasePromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Build prompt string for text generation endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>PromptBuilderOutput</code> <p>Formatted prompt string for text generation</p> Source code in <code>src/prompt_builders/base_prompt_builder.py</code> <pre><code>@abstractmethod\nasync def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"\n    Build prompt string for text generation endpoint.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data.\n\n    Returns:\n        str: Formatted prompt string for text generation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompt_builders/mistral_ai_prompt_builder/","title":"MistralAI","text":""},{"location":"reference/prompt_builders/mistral_ai_prompt_builder/#src.prompt_builders.mistral_ai.mistral_ai_prompt_builder.MistralAIPromptBuilder","title":"<code>src.prompt_builders.mistral_ai.mistral_ai_prompt_builder.MistralAIPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>A prompt builder specialized for Mistral AI chat completion models.</p> <p>This class handles the construction of prompts for Mistral models, with special handling for tool definitions and system messages. It loads configuration from a YAML file and supports embedding tool information into the conversation history.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration dictionary loaded from prompt_builders.yaml. Expected to contain 'system_prompt' with 'header' and 'tool_instructions'.</p> Example <pre><code>builder = MistralAIPromptBuilder()\npayload = PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n)\noutput = await builder.build_chat(payload)\n# Use output.chat_messages with Mistral API\n</code></pre> Source code in <code>src/prompt_builders/mistral_ai/mistral_ai_prompt_builder.py</code> <pre><code>class MistralAIPromptBuilder(BasePromptBuilder):\n    \"\"\"A prompt builder specialized for Mistral AI chat completion models.\n\n    This class handles the construction of prompts for Mistral models, with special\n    handling for tool definitions and system messages. It loads configuration from\n    a YAML file and supports embedding tool information into the conversation history.\n\n    Attributes:\n        config (Dict): Configuration dictionary loaded from prompt_builders.yaml.\n            Expected to contain 'system_prompt' with 'header' and 'tool_instructions'.\n\n    Example:\n        ```python\n        builder = MistralAIPromptBuilder()\n        payload = PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        )\n        output = await builder.build_chat(payload)\n        # Use output.chat_messages with Mistral API\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the Mistral prompt builder.\n\n        Loads configuration from the prompt_builders.yaml file and sets up logging.\n        Raises FileNotFoundError if the config file is not found.\n        \"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing MistralAIPromptBuilder\")\n        super().__init__()\n        self.config = self._load_config()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build a chat completion prompt with optional tool definitions.\n\n        Constructs a prompt by potentially modifying the conversation history to\n        include tool information. If tools are defined, they are added to or merged\n        with the system message.\n\n        Args:\n            payload (PromptPayload): Contains conversation history and optional tool\n                definitions. History should be a list of message objects, and tool\n                definitions should be a list of tool specification objects.\n\n        Returns:\n            PromptBuilderOutput: Contains the modified chat messages ready for use\n                with Mistral's chat completion API.\n\n        Note:\n            If the first message in history is a system message, tool information\n            will be prepended to it. Otherwise, a new system message will be created.\n        \"\"\"\n\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        if not tool_definitions:\n            self.logger.debug(\"No tool definitions provided, returning original history\")\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        tool_names = [tool.function.name for tool in tool_definitions]\n\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n        modified_history = conversation_history.copy()\n\n        # Mistral's formatting for tools in system messages might differ slightly\n        # Format according to Mistral's requirements\n        formatted_tool_info = f\"[AVAILABLE_TOOLS]\\n{tool_info}\\n[/AVAILABLE_TOOLS]\"\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(\n                content=f\"{existing_content}\\n\\n{formatted_tool_info}\"\n            )\n        else:\n            system_msg = SystemMessage(content=formatted_tool_info)\n            modified_history.insert(0, system_msg)\n\n        self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Text completion is not fully implemented for Mistral models.\n\n        This method is included to satisfy the interface but raises NotImplementedError.\n\n        Args:\n            payload (PromptPayload): Unused payload.\n\n        Raises:\n            NotImplementedError: Always raised as this method is not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"Mistral models primarily use chat completions. Use build_chat() instead.\"\n        )\n\n    @staticmethod\n    def _load_config() -&gt; Dict:\n        \"\"\"Load the Mistral-specific configuration from the prompt builders YAML file.\n\n        Returns:\n            Dict: Configuration dictionary containing Mistral-specific settings.\n\n        Raises:\n            FileNotFoundError: If the config file doesn't exist.\n            yaml.YAMLError: If the config file is malformed.\n        \"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('mistral', {})\n</code></pre>"},{"location":"reference/prompt_builders/mistral_ai_prompt_builder/#src.prompt_builders.mistral_ai.mistral_ai_prompt_builder.MistralAIPromptBuilder.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Mistral prompt builder.</p> <p>Loads configuration from the prompt_builders.yaml file and sets up logging. Raises FileNotFoundError if the config file is not found.</p> Source code in <code>src/prompt_builders/mistral_ai/mistral_ai_prompt_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the Mistral prompt builder.\n\n    Loads configuration from the prompt_builders.yaml file and sets up logging.\n    Raises FileNotFoundError if the config file is not found.\n    \"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.logger.debug(\"Initializing MistralAIPromptBuilder\")\n    super().__init__()\n    self.config = self._load_config()\n</code></pre>"},{"location":"reference/prompt_builders/mistral_ai_prompt_builder/#src.prompt_builders.mistral_ai.mistral_ai_prompt_builder.MistralAIPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build a chat completion prompt with optional tool definitions.</p> <p>Constructs a prompt by potentially modifying the conversation history to include tool information. If tools are defined, they are added to or merged with the system message.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>Contains conversation history and optional tool definitions. History should be a list of message objects, and tool definitions should be a list of tool specification objects.</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified chat messages ready for use with Mistral's chat completion API.</p> Note <p>If the first message in history is a system message, tool information will be prepended to it. Otherwise, a new system message will be created.</p> Source code in <code>src/prompt_builders/mistral_ai/mistral_ai_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build a chat completion prompt with optional tool definitions.\n\n    Constructs a prompt by potentially modifying the conversation history to\n    include tool information. If tools are defined, they are added to or merged\n    with the system message.\n\n    Args:\n        payload (PromptPayload): Contains conversation history and optional tool\n            definitions. History should be a list of message objects, and tool\n            definitions should be a list of tool specification objects.\n\n    Returns:\n        PromptBuilderOutput: Contains the modified chat messages ready for use\n            with Mistral's chat completion API.\n\n    Note:\n        If the first message in history is a system message, tool information\n        will be prepended to it. Otherwise, a new system message will be created.\n    \"\"\"\n\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    if not tool_definitions:\n        self.logger.debug(\"No tool definitions provided, returning original history\")\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    tool_names = [tool.function.name for tool in tool_definitions]\n\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n    modified_history = conversation_history.copy()\n\n    # Mistral's formatting for tools in system messages might differ slightly\n    # Format according to Mistral's requirements\n    formatted_tool_info = f\"[AVAILABLE_TOOLS]\\n{tool_info}\\n[/AVAILABLE_TOOLS]\"\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(\n            content=f\"{existing_content}\\n\\n{formatted_tool_info}\"\n        )\n    else:\n        system_msg = SystemMessage(content=formatted_tool_info)\n        modified_history.insert(0, system_msg)\n\n    self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/mistral_ai_prompt_builder/#src.prompt_builders.mistral_ai.mistral_ai_prompt_builder.MistralAIPromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>async</code>","text":"<p>Text completion is not fully implemented for Mistral models.</p> <p>This method is included to satisfy the interface but raises NotImplementedError.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>Unused payload.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised as this method is not supported.</p> Source code in <code>src/prompt_builders/mistral_ai/mistral_ai_prompt_builder.py</code> <pre><code>async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Text completion is not fully implemented for Mistral models.\n\n    This method is included to satisfy the interface but raises NotImplementedError.\n\n    Args:\n        payload (PromptPayload): Unused payload.\n\n    Raises:\n        NotImplementedError: Always raised as this method is not supported.\n    \"\"\"\n    raise NotImplementedError(\n        \"Mistral models primarily use chat completions. Use build_chat() instead.\"\n    )\n</code></pre>"},{"location":"reference/prompt_builders/openai_prompt_builder/","title":"OpenAI","text":""},{"location":"reference/prompt_builders/openai_prompt_builder/#src.prompt_builders.openai.openai_prompt_builder.OpenAIPromptBuilder","title":"<code>src.prompt_builders.openai.openai_prompt_builder.OpenAIPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>A prompt builder specialized for OpenAI chat completion models.</p> <p>This class handles the construction of prompts for OpenAI models, with special handling for tool definitions and system messages. It loads configuration from a YAML file and supports embedding tool information into the conversation history.</p> <p>The builder primarily supports chat completions, which is the recommended format for OpenAI models. Text completions are not supported.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration dictionary loaded from prompt_builders.yaml. Expected to contain 'system_prompt' with 'header' and 'tool_instructions'.</p> Example <pre><code>builder = OpenAIPromptBuilder()\npayload = PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n)\noutput = await builder.build_chat(payload)\n# Use output.chat_messages with OpenAI API\n</code></pre> Source code in <code>src/prompt_builders/openai/openai_prompt_builder.py</code> <pre><code>class OpenAIPromptBuilder(BasePromptBuilder):\n    \"\"\"A prompt builder specialized for OpenAI chat completion models.\n\n    This class handles the construction of prompts for OpenAI models, with special\n    handling for tool definitions and system messages. It loads configuration from\n    a YAML file and supports embedding tool information into the conversation history.\n\n    The builder primarily supports chat completions, which is the recommended format\n    for OpenAI models. Text completions are not supported.\n\n    Attributes:\n        config (Dict): Configuration dictionary loaded from prompt_builders.yaml.\n            Expected to contain 'system_prompt' with 'header' and 'tool_instructions'.\n\n    Example:\n        ```python\n        builder = OpenAIPromptBuilder()\n        payload = PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        )\n        output = await builder.build_chat(payload)\n        # Use output.chat_messages with OpenAI API\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the OpenAI prompt builder.\n\n        Loads configuration from the prompt_builders.yaml file and sets up logging.\n        Raises FileNotFoundError if the config file is not found.\n        \"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing OpenAIPromptBuilder\")\n        super().__init__()\n        self.config = self._load_config()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build a chat completion prompt with optional tool definitions.\n\n        Constructs a prompt by potentially modifying the conversation history to\n        include tool information. If tools are defined, they are added to or merged\n        with the system message.\n\n        Args:\n            payload (PromptPayload): Contains conversation history and optional tool\n                definitions. History should be a list of message objects, and tool\n                definitions should be a list of tool specification objects.\n\n        Returns:\n            PromptBuilderOutput: Contains the modified chat messages ready for use\n                with OpenAI's chat completion API.\n\n        Note:\n            If the first message in history is a system message, tool information\n            will be prepended to it. Otherwise, a new system message will be created.\n        \"\"\"\n\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        if not tool_definitions:\n            self.logger.debug(\"No tool definitions provided, returning original history\")\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        tool_names = [tool.function.name for tool in tool_definitions]\n\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(\n                content=f\"## tools:\\n\\n{existing_content}\\n{tool_info}\\n\\n\"\n            )\n        else:\n            system_msg = SystemMessage(content=tool_info)\n            modified_history.insert(0, system_msg)\n\n        self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, context: Dict) -&gt; str:\n        \"\"\"Text completion is not supported for OpenAI models.\n\n        Args:\n            context (Dict): Unused context dictionary.\n\n        Raises:\n            NotImplementedError: Always raised as this method is not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"OpenAI models primarily use chat completions. Use build_chat() instead.\"\n        )\n\n    @staticmethod\n    def _load_config() -&gt; Dict:\n        \"\"\"Load the OpenAI-specific configuration from the prompt builders YAML file.\n\n        Returns:\n            Dict: Configuration dictionary containing OpenAI-specific settings.\n\n        Raises:\n            FileNotFoundError: If the config file doesn't exist.\n            yaml.YAMLError: If the config file is malformed.\n        \"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('openai', {})\n</code></pre>"},{"location":"reference/prompt_builders/openai_prompt_builder/#src.prompt_builders.openai.openai_prompt_builder.OpenAIPromptBuilder.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the OpenAI prompt builder.</p> <p>Loads configuration from the prompt_builders.yaml file and sets up logging. Raises FileNotFoundError if the config file is not found.</p> Source code in <code>src/prompt_builders/openai/openai_prompt_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the OpenAI prompt builder.\n\n    Loads configuration from the prompt_builders.yaml file and sets up logging.\n    Raises FileNotFoundError if the config file is not found.\n    \"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.logger.debug(\"Initializing OpenAIPromptBuilder\")\n    super().__init__()\n    self.config = self._load_config()\n</code></pre>"},{"location":"reference/prompt_builders/openai_prompt_builder/#src.prompt_builders.openai.openai_prompt_builder.OpenAIPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build a chat completion prompt with optional tool definitions.</p> <p>Constructs a prompt by potentially modifying the conversation history to include tool information. If tools are defined, they are added to or merged with the system message.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>Contains conversation history and optional tool definitions. History should be a list of message objects, and tool definitions should be a list of tool specification objects.</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified chat messages ready for use with OpenAI's chat completion API.</p> Note <p>If the first message in history is a system message, tool information will be prepended to it. Otherwise, a new system message will be created.</p> Source code in <code>src/prompt_builders/openai/openai_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build a chat completion prompt with optional tool definitions.\n\n    Constructs a prompt by potentially modifying the conversation history to\n    include tool information. If tools are defined, they are added to or merged\n    with the system message.\n\n    Args:\n        payload (PromptPayload): Contains conversation history and optional tool\n            definitions. History should be a list of message objects, and tool\n            definitions should be a list of tool specification objects.\n\n    Returns:\n        PromptBuilderOutput: Contains the modified chat messages ready for use\n            with OpenAI's chat completion API.\n\n    Note:\n        If the first message in history is a system message, tool information\n        will be prepended to it. Otherwise, a new system message will be created.\n    \"\"\"\n\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    if not tool_definitions:\n        self.logger.debug(\"No tool definitions provided, returning original history\")\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    tool_names = [tool.function.name for tool in tool_definitions]\n\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(\n            content=f\"## tools:\\n\\n{existing_content}\\n{tool_info}\\n\\n\"\n        )\n    else:\n        system_msg = SystemMessage(content=tool_info)\n        modified_history.insert(0, system_msg)\n\n    self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/openai_prompt_builder/#src.prompt_builders.openai.openai_prompt_builder.OpenAIPromptBuilder.build_text","title":"<code>build_text(context)</code>  <code>async</code>","text":"<p>Text completion is not supported for OpenAI models.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Dict</code> <p>Unused context dictionary.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised as this method is not supported.</p> Source code in <code>src/prompt_builders/openai/openai_prompt_builder.py</code> <pre><code>async def build_text(self, context: Dict) -&gt; str:\n    \"\"\"Text completion is not supported for OpenAI models.\n\n    Args:\n        context (Dict): Unused context dictionary.\n\n    Raises:\n        NotImplementedError: Always raised as this method is not supported.\n    \"\"\"\n    raise NotImplementedError(\n        \"OpenAI models primarily use chat completions. Use build_chat() instead.\"\n    )\n</code></pre>"},{"location":"reference/prompt_builders/prompt_models/","title":"Prompt Models","text":""},{"location":"reference/prompt_builders/prompt_models/#src.prompt_builders.prompt_models.PromptPayload","title":"<code>src.prompt_builders.prompt_models.PromptPayload</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for prompt generation input data.</p> <p>This class encapsulates the necessary data for generating prompts, including conversation history and available tools.</p> <p>Attributes:</p> Name Type Description <code>conversation_history</code> <code>List[TextChatMessage]</code> <p>The complete conversation history as a chronological list of messages between user and assistant.</p> <code>tool_definitions</code> <code>Optional[List[Tool]]</code> <p>List of tools that are available for the model to use in its responses. Defaults to None if no tools are available.</p> Source code in <code>src/prompt_builders/prompt_models.py</code> <pre><code>class PromptPayload(BaseModel):\n    \"\"\"Container for prompt generation input data.\n\n    This class encapsulates the necessary data for generating prompts,\n    including conversation history and available tools.\n\n    Attributes:\n        conversation_history (List[TextChatMessage]): The complete conversation\n            history as a chronological list of messages between user and assistant.\n        tool_definitions (Optional[List[Tool]]): List of tools that are available\n            for the model to use in its responses. Defaults to None if no tools\n            are available.\n    \"\"\"\n\n    conversation_history: List[TextChatMessage] = Field(\n        ...,\n        description=\"The conversation history as a list of messages\"\n    )\n    tool_definitions: Optional[List[Tool]] = Field(\n        None,\n        description=\"Available tools that can be used by the model\"\n    )\n</code></pre>"},{"location":"reference/prompt_builders/prompt_models/#src.prompt_builders.prompt_models.PromptBuilderOutput","title":"<code>src.prompt_builders.prompt_models.PromptBuilderOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output container for prompt builder results.</p> <p>Stores either a text prompt or a list of chat messages, providing a unified interface for different types of prompt outputs.</p> <p>Attributes:</p> Name Type Description <code>text_prompt</code> <code>Optional[str]</code> <p>A single string containing the generated prompt. Mutually exclusive with chat_messages.</p> <code>chat_messages</code> <code>Optional[List[TextChatMessage]]</code> <p>A list of chat messages representing the prompt. Mutually exclusive with text_prompt.</p> Source code in <code>src/prompt_builders/prompt_models.py</code> <pre><code>class PromptBuilderOutput(BaseModel):\n    \"\"\"Output container for prompt builder results.\n\n    Stores either a text prompt or a list of chat messages, providing\n    a unified interface for different types of prompt outputs.\n\n    Attributes:\n        text_prompt (Optional[str]): A single string containing the generated\n            prompt. Mutually exclusive with chat_messages.\n        chat_messages (Optional[List[TextChatMessage]]): A list of chat messages\n            representing the prompt. Mutually exclusive with text_prompt.\n    \"\"\"\n\n    text_prompt: Optional[str] = None\n    chat_messages: Optional[List[TextChatMessage]] = None\n\n    def get_output(self) -&gt; Union[str, List[TextChatMessage]]:\n        \"\"\"Retrieve the prompt output in its appropriate format.\n\n        Returns either the text prompt or chat messages, depending on which\n        was set during initialization.\n\n        Returns:\n            Union[str, List[TextChatMessage]]: Either a string containing the\n                text prompt or a list of chat messages.\n\n        Raises:\n            ValueError: If neither text_prompt nor chat_messages was set.\n        \"\"\"\n        if self.text_prompt is not None:\n            return self.text_prompt\n        if self.chat_messages is not None:\n            return self.chat_messages\n        raise ValueError(\"Neither text_prompt nor chat_messages was set\")\n</code></pre>"},{"location":"reference/prompt_builders/prompt_models/#src.prompt_builders.prompt_models.PromptBuilderOutput.get_output","title":"<code>get_output()</code>","text":"<p>Retrieve the prompt output in its appropriate format.</p> <p>Returns either the text prompt or chat messages, depending on which was set during initialization.</p> <p>Returns:</p> Type Description <code>Union[str, List[TextChatMessage]]</code> <p>Union[str, List[TextChatMessage]]: Either a string containing the text prompt or a list of chat messages.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither text_prompt nor chat_messages was set.</p> Source code in <code>src/prompt_builders/prompt_models.py</code> <pre><code>def get_output(self) -&gt; Union[str, List[TextChatMessage]]:\n    \"\"\"Retrieve the prompt output in its appropriate format.\n\n    Returns either the text prompt or chat messages, depending on which\n    was set during initialization.\n\n    Returns:\n        Union[str, List[TextChatMessage]]: Either a string containing the\n            text prompt or a list of chat messages.\n\n    Raises:\n        ValueError: If neither text_prompt nor chat_messages was set.\n    \"\"\"\n    if self.text_prompt is not None:\n        return self.text_prompt\n    if self.chat_messages is not None:\n        return self.chat_messages\n    raise ValueError(\"Neither text_prompt nor chat_messages was set\")\n</code></pre>"},{"location":"reference/prompt_builders/xai_prompt_builder/","title":"xAI","text":""},{"location":"reference/prompt_builders/xai_prompt_builder/#src.prompt_builders.xai.xai_prompt_builder.XAIPromptBuilder","title":"<code>src.prompt_builders.xai.xai_prompt_builder.XAIPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>A prompt builder specialized for xAI's chat completion models.</p> <p>This class handles the construction of prompts for xAI models, with special handling for tool definitions and system messages. It loads configuration from a YAML file and supports embedding tool information into the conversation history.</p> <p>The builder primarily supports chat completions, following OpenAI-compatible format which aligns with xAI's API. Text completions are not supported.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration dictionary loaded from prompt_builders.yaml. Expected to contain 'system_prompt' with 'header' and 'tool_instructions'.</p> Example <pre><code>builder = XAIPromptBuilder()\npayload = PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n)\noutput = await builder.build_chat(payload)\n# Use output.chat_messages with xAI API\n</code></pre> Source code in <code>src/prompt_builders/xai/xai_prompt_builder.py</code> <pre><code>class XAIPromptBuilder(BasePromptBuilder):\n    \"\"\"A prompt builder specialized for xAI's chat completion models.\n\n    This class handles the construction of prompts for xAI models, with special\n    handling for tool definitions and system messages. It loads configuration from\n    a YAML file and supports embedding tool information into the conversation history.\n\n    The builder primarily supports chat completions, following OpenAI-compatible format\n    which aligns with xAI's API. Text completions are not supported.\n\n    Attributes:\n        config (Dict): Configuration dictionary loaded from prompt_builders.yaml.\n            Expected to contain 'system_prompt' with 'header' and 'tool_instructions'.\n\n    Example:\n        ```python\n        builder = XAIPromptBuilder()\n        payload = PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        )\n        output = await builder.build_chat(payload)\n        # Use output.chat_messages with xAI API\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the xAI prompt builder.\n\n        Loads configuration from the prompt_builders.yaml file and sets up logging.\n        Raises FileNotFoundError if the config file is not found.\n        \"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing XAIPromptBuilder\")\n        super().__init__()\n        self.config = self._load_config()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build a chat completion prompt with optional tool definitions.\n\n        Constructs a prompt by potentially modifying the conversation history to\n        include tool information. If tools are defined, they are added to or merged\n        with the system message.\n\n        Args:\n            payload (PromptPayload): Contains conversation history and optional tool\n                definitions. History should be a list of message objects, and tool\n                definitions should be a list of tool specification objects.\n\n        Returns:\n            PromptBuilderOutput: Contains the modified chat messages ready for use\n                with xAI's chat completion API.\n\n        Note:\n            If the first message in history is a system message, tool information\n            will be prepended to it. Otherwise, a new system message will be created.\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        if not tool_definitions:\n            self.logger.debug(\"No tool definitions provided, returning original history\")\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        tool_names = [tool.function.name for tool in tool_definitions]\n\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(\n                content=f\"{existing_content}\\n\\n## Tools Available:\\n\\n{tool_info}\\n\\n\"\n            )\n        else:\n            system_msg = SystemMessage(content=f\"## Tools Available:\\n\\n{tool_info}\")\n            modified_history.insert(0, system_msg)\n\n        self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def _build_system_content(self, tool_definitions, header, instructions):\n        \"\"\"Build the system content that includes tool definitions.\n\n        Args:\n            tool_definitions (List): List of tool definition objects\n            header (str): Header text for the tools section\n            instructions (str): General instructions for using tools\n\n        Returns:\n            str: Formatted system content with tool information\n        \"\"\"\n        tool_descriptions = []\n\n        for tool in tool_definitions:\n            function = tool.function\n            name = function.name\n            description = function.description or \"No description available\"\n            parameters = function.parameters.model_dump() if function.parameters else {}\n\n            # Format parameter information\n            param_info = \"\"\n            if parameters and \"properties\" in parameters:\n                properties = parameters[\"properties\"]\n                required = parameters.get(\"required\", [])\n                param_info = \"\\nParameters:\\n\"\n\n                for param_name, param_details in properties.items():\n                    req_status = \"(required)\" if param_name in required else \"(optional)\"\n                    param_desc = param_details.get(\"description\", \"No description\")\n                    param_type = param_details.get(\"type\", \"any\")\n                    param_info += f\"- {param_name} {req_status}: {param_desc} (Type: {param_type})\\n\"\n\n            tool_descriptions.append(f\"### {name}\\n{description}\\n{param_info}\")\n\n        formatted_tools = \"\\n\\n\".join(tool_descriptions)\n        return f\"{header}\\n\\n{formatted_tools}\\n\\n{instructions}\"\n\n    async def build_text(self, context: Dict) -&gt; str:\n        \"\"\"Text completion is not supported for xAI models.\n\n        Args:\n            context (Dict): Unused context dictionary.\n\n        Raises:\n            NotImplementedError: Always raised as this method is not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"xAI models use chat completions interface. Use build_chat() instead.\"\n        )\n\n    @staticmethod\n    def _load_config() -&gt; Dict:\n        \"\"\"Load the xAI-specific configuration from the prompt builders YAML file.\n\n        Returns:\n            Dict: Configuration dictionary containing Mistral-specific settings.\n\n        Raises:\n            FileNotFoundError: If the config file doesn't exist.\n            yaml.YAMLError: If the config file is malformed.\n        \"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('xai', {})\n</code></pre>"},{"location":"reference/prompt_builders/xai_prompt_builder/#src.prompt_builders.xai.xai_prompt_builder.XAIPromptBuilder.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the xAI prompt builder.</p> <p>Loads configuration from the prompt_builders.yaml file and sets up logging. Raises FileNotFoundError if the config file is not found.</p> Source code in <code>src/prompt_builders/xai/xai_prompt_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the xAI prompt builder.\n\n    Loads configuration from the prompt_builders.yaml file and sets up logging.\n    Raises FileNotFoundError if the config file is not found.\n    \"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.logger.debug(\"Initializing XAIPromptBuilder\")\n    super().__init__()\n    self.config = self._load_config()\n</code></pre>"},{"location":"reference/prompt_builders/xai_prompt_builder/#src.prompt_builders.xai.xai_prompt_builder.XAIPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build a chat completion prompt with optional tool definitions.</p> <p>Constructs a prompt by potentially modifying the conversation history to include tool information. If tools are defined, they are added to or merged with the system message.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>Contains conversation history and optional tool definitions. History should be a list of message objects, and tool definitions should be a list of tool specification objects.</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified chat messages ready for use with xAI's chat completion API.</p> Note <p>If the first message in history is a system message, tool information will be prepended to it. Otherwise, a new system message will be created.</p> Source code in <code>src/prompt_builders/xai/xai_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build a chat completion prompt with optional tool definitions.\n\n    Constructs a prompt by potentially modifying the conversation history to\n    include tool information. If tools are defined, they are added to or merged\n    with the system message.\n\n    Args:\n        payload (PromptPayload): Contains conversation history and optional tool\n            definitions. History should be a list of message objects, and tool\n            definitions should be a list of tool specification objects.\n\n    Returns:\n        PromptBuilderOutput: Contains the modified chat messages ready for use\n            with xAI's chat completion API.\n\n    Note:\n        If the first message in history is a system message, tool information\n        will be prepended to it. Otherwise, a new system message will be created.\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    if not tool_definitions:\n        self.logger.debug(\"No tool definitions provided, returning original history\")\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    tool_names = [tool.function.name for tool in tool_definitions]\n\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(\n            content=f\"{existing_content}\\n\\n## Tools Available:\\n\\n{tool_info}\\n\\n\"\n        )\n    else:\n        system_msg = SystemMessage(content=f\"## Tools Available:\\n\\n{tool_info}\")\n        modified_history.insert(0, system_msg)\n\n    self.logger.debug(\"Returning modified history with %d messages\", len(modified_history))\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/xai_prompt_builder/#src.prompt_builders.xai.xai_prompt_builder.XAIPromptBuilder.build_text","title":"<code>build_text(context)</code>  <code>async</code>","text":"<p>Text completion is not supported for xAI models.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Dict</code> <p>Unused context dictionary.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised as this method is not supported.</p> Source code in <code>src/prompt_builders/xai/xai_prompt_builder.py</code> <pre><code>async def build_text(self, context: Dict) -&gt; str:\n    \"\"\"Text completion is not supported for xAI models.\n\n    Args:\n        context (Dict): Unused context dictionary.\n\n    Raises:\n        NotImplementedError: Always raised as this method is not supported.\n    \"\"\"\n    raise NotImplementedError(\n        \"xAI models use chat completions interface. Use build_chat() instead.\"\n    )\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/granite_prompt_builder/","title":"Granite","text":""},{"location":"reference/prompt_builders/openai_compat/granite_prompt_builder/#src.prompt_builders.openai_compat.granite.granite_prompt_builder.OpenAICompatGranitePromptBuilder","title":"<code>src.prompt_builders.openai_compat.granite.granite_prompt_builder.OpenAICompatGranitePromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>Prompt builder for the Granite model architecture.</p> <p>This class handles the construction of prompts and chat messages specifically formatted for Granite models, with support for tool definitions and context-aware message formatting.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration loaded from prompt_builders.yaml</p> <code>template</code> <code>Template</code> <p>Jinja2 template for text generation prompts</p> Source code in <code>src/prompt_builders/openai_compat/granite/granite_prompt_builder.py</code> <pre><code>class OpenAICompatGranitePromptBuilder(BasePromptBuilder):\n    \"\"\"Prompt builder for the Granite model architecture.\n\n    This class handles the construction of prompts and chat messages specifically\n    formatted for Granite models, with support for tool definitions and\n    context-aware message formatting.\n\n    Attributes:\n        config (Dict): Configuration loaded from prompt_builders.yaml\n        template (Template): Jinja2 template for text generation prompts\n    \"\"\"\n    def __init__(self, template_dir: Optional[str] = None):\n        \"\"\"Initialize the Granite prompt builder.\n\n        Args:\n            template_dir (Optional[str]): Custom directory for template files.\n                If None, uses default directory 'src/prompt_builders/granite'.\n        \"\"\"\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.config = self._load_config()\n        self.template = self._load_template(template_dir) if template_dir else self._load_template()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build chat messages with tools embedded in system message.\n\n        Creates or modifies the system message to include tool definitions\n        while preserving the existing conversation structure.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the modified message list with tool information\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # If no tools, return history as is\n        if not tool_definitions:\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        # Generate tool information\n        tool_names = [tool.function.name for tool in tool_definitions]\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n\n        # Create modified history list\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            # Prepend tool info to existing system message\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(\n                content=f\"{existing_content}\\n&lt;|start_of_role|&gt;tools&lt;|end_of_role|&gt;{tool_info}&lt;|end_of_text|&gt;\"\n            )\n        else:\n            # Create new system message with tool info and prepend to history\n            system_msg = SystemMessage(content=tool_info)\n            modified_history.insert(0, system_msg)\n\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build text prompt using Granite-specific template.\n\n        Constructs a complete prompt string using the Jinja2 template,\n        incorporating conversation history and tool definitions.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            str: Formatted prompt string ready for text generation\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # Preprocess conversation history to flatten user message content\n        processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n        # Format tool definitions for template\n        formatted_tools = [\n            self._format_tool_for_template(tool)\n            for tool in tool_definitions\n        ] if tool_definitions else None\n\n        # Prepare template variables\n        template_vars = {\n            'messages': processed_history,\n            'tools': formatted_tools,\n            'tools_in_user_message': False,\n            'add_generation_prompt': True,\n            'date_string': datetime.now().strftime(\"%d %b %Y\"),\n            'tool_instructions': self.config['system_prompt']['tool_instructions']\n        }\n        self.logger.debug(f'template_vars: {template_vars}')\n\n        return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n\n    @staticmethod\n    def _preprocess_message(message: TextChatMessage) -&gt; TextChatMessage:\n        \"\"\"Preprocess message for Granite format.\"\"\"\n        if not isinstance(message, UserMessage):\n            return message\n\n        if isinstance(message.content, list):\n            # Extract text content from array of content objects\n            text_contents = []\n            for content in message.content:\n                if getattr(content, 'type', None) == 'text':\n                    text_contents.append(content.text)\n\n            # Create new UserMessage with flattened content\n            return UserMessage(content=\" \".join(text_contents))\n\n        return message\n\n    @staticmethod\n    def _format_tool_for_template(tool: Tool) -&gt; dict:\n        \"\"\"Format tool definition for Granite template usage.\"\"\"\n        return {\n            \"name\": tool.function.name,\n            \"description\": tool.function.description,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": tool.function.parameters.properties,\n                \"required\": tool.function.parameters.required\n            }\n        }\n\n    @staticmethod\n    def _load_config() -&gt; dict:\n        \"\"\"Load Granite-specific configuration from YAML file.\"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('openai-compat-granite')\n\n    @staticmethod\n    def _load_template(template_dir: str = \"src/prompt_builders/openai_compat/granite\") -&gt; Template:\n        \"\"\"Load Jinja2 template for Granite prompt generation.\"\"\"\n        template_path = Path(template_dir) / \"granite-3.1-8b.jinja\"\n        with open(template_path) as f:\n            return Template(f.read())\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/granite_prompt_builder/#src.prompt_builders.openai_compat.granite.granite_prompt_builder.OpenAICompatGranitePromptBuilder.__init__","title":"<code>__init__(template_dir=None)</code>","text":"<p>Initialize the Granite prompt builder.</p> <p>Parameters:</p> Name Type Description Default <code>template_dir</code> <code>Optional[str]</code> <p>Custom directory for template files. If None, uses default directory 'src/prompt_builders/granite'.</p> <code>None</code> Source code in <code>src/prompt_builders/openai_compat/granite/granite_prompt_builder.py</code> <pre><code>def __init__(self, template_dir: Optional[str] = None):\n    \"\"\"Initialize the Granite prompt builder.\n\n    Args:\n        template_dir (Optional[str]): Custom directory for template files.\n            If None, uses default directory 'src/prompt_builders/granite'.\n    \"\"\"\n    super().__init__()\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.config = self._load_config()\n    self.template = self._load_template(template_dir) if template_dir else self._load_template()\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/granite_prompt_builder/#src.prompt_builders.openai_compat.granite.granite_prompt_builder.OpenAICompatGranitePromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build chat messages with tools embedded in system message.</p> <p>Creates or modifies the system message to include tool definitions while preserving the existing conversation structure.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified message list with tool information</p> Source code in <code>src/prompt_builders/openai_compat/granite/granite_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build chat messages with tools embedded in system message.\n\n    Creates or modifies the system message to include tool definitions\n    while preserving the existing conversation structure.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the modified message list with tool information\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # If no tools, return history as is\n    if not tool_definitions:\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    # Generate tool information\n    tool_names = [tool.function.name for tool in tool_definitions]\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n\n    # Create modified history list\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        # Prepend tool info to existing system message\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(\n            content=f\"{existing_content}\\n&lt;|start_of_role|&gt;tools&lt;|end_of_role|&gt;{tool_info}&lt;|end_of_text|&gt;\"\n        )\n    else:\n        # Create new system message with tool info and prepend to history\n        system_msg = SystemMessage(content=tool_info)\n        modified_history.insert(0, system_msg)\n\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/granite_prompt_builder/#src.prompt_builders.openai_compat.granite.granite_prompt_builder.OpenAICompatGranitePromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>async</code>","text":"<p>Build text prompt using Granite-specific template.</p> <p>Constructs a complete prompt string using the Jinja2 template, incorporating conversation history and tool definitions.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>PromptBuilderOutput</code> <p>Formatted prompt string ready for text generation</p> Source code in <code>src/prompt_builders/openai_compat/granite/granite_prompt_builder.py</code> <pre><code>async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build text prompt using Granite-specific template.\n\n    Constructs a complete prompt string using the Jinja2 template,\n    incorporating conversation history and tool definitions.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        str: Formatted prompt string ready for text generation\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # Preprocess conversation history to flatten user message content\n    processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n    # Format tool definitions for template\n    formatted_tools = [\n        self._format_tool_for_template(tool)\n        for tool in tool_definitions\n    ] if tool_definitions else None\n\n    # Prepare template variables\n    template_vars = {\n        'messages': processed_history,\n        'tools': formatted_tools,\n        'tools_in_user_message': False,\n        'add_generation_prompt': True,\n        'date_string': datetime.now().strftime(\"%d %b %Y\"),\n        'tool_instructions': self.config['system_prompt']['tool_instructions']\n    }\n    self.logger.debug(f'template_vars: {template_vars}')\n\n    return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/llama_prompt_builder/","title":"Llama","text":""},{"location":"reference/prompt_builders/openai_compat/llama_prompt_builder/#src.prompt_builders.openai_compat.llama.llama_prompt_builder.OpenAICompatLlamaPromptBuilder","title":"<code>src.prompt_builders.openai_compat.llama.llama_prompt_builder.OpenAICompatLlamaPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>Prompt builder for the Llama model architecture.</p> <p>This class handles the construction of prompts and chat messages specifically formatted for Llama models, with support for tool definitions and context-aware message formatting.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration loaded from prompt_builders.yaml.</p> <code>template</code> <code>Template</code> <p>Jinja2 template for text generation prompts.</p> Example <pre><code>builder = LlamaPromptBuilder()\n\n# Build chat messages\noutput = await builder.build_chat(PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n))\n\n# Build text prompt\noutput = await builder.build_text(PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n))\n</code></pre> Source code in <code>src/prompt_builders/openai_compat/llama/llama_prompt_builder.py</code> <pre><code>class OpenAICompatLlamaPromptBuilder(BasePromptBuilder):\n    \"\"\"Prompt builder for the Llama model architecture.\n\n    This class handles the construction of prompts and chat messages specifically\n    formatted for Llama models, with support for tool definitions and\n    context-aware message formatting.\n\n    Attributes:\n        config (Dict): Configuration loaded from prompt_builders.yaml.\n        template (Template): Jinja2 template for text generation prompts.\n\n    Example:\n        ```python\n        builder = LlamaPromptBuilder()\n\n        # Build chat messages\n        output = await builder.build_chat(PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        ))\n\n        # Build text prompt\n        output = await builder.build_text(PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        ))\n        ```\n    \"\"\"\n    def __init__(self, template_dir: Optional[str] = None):\n        \"\"\"Initialize the Llama prompt builder.\n\n        Args:\n            template_dir (Optional[str]): Custom directory for template files.\n                If None, uses default directory 'src/prompt_builders/llama'.\n        \"\"\"\n        super().__init__()\n        self.config = self._load_config()\n        self.template = self._load_template(template_dir) if template_dir else self._load_template()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build chat messages with tools embedded in system message.\n\n        Creates or modifies the system message to include tool definitions\n        while preserving the existing conversation structure.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the modified message list with tool information\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        if not tool_definitions:\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        tool_names = [tool.function.name for tool in tool_definitions]\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(content=f\"{tool_info}\\n{existing_content}\")\n        else:\n            system_msg = SystemMessage(content=tool_info)\n            modified_history.insert(0, system_msg)\n\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build text prompt using Llama-specific template.\n\n        Constructs a complete prompt string using the Jinja2 template,\n        incorporating conversation history, tool definitions, and model-specific\n        tokens.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the formatted text prompt for generation\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # Preprocess conversation history to flatten user message content\n        processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n        # Format tool definitions for template\n        formatted_tools = [\n            self._format_tool_for_template(tool)\n            for tool in tool_definitions\n        ] if tool_definitions else None\n\n        # Prepare template variables\n        template_vars = {\n            'messages': processed_history,\n            'tools': formatted_tools,\n            'tools_in_user_message': False,\n            'add_generation_prompt': True,\n            'date_string': datetime.now().strftime(\"%d %b %Y\"),\n            'bos_token': self.config['tokens']['begin_text'],\n            'tool_instructions': self.config['system_prompt']['tool_instructions']\n        }\n\n        return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n\n    @staticmethod\n    def _preprocess_message(message: TextChatMessage) -&gt; TextChatMessage:\n        \"\"\"Preprocess message for Llama format.\"\"\"\n        if not isinstance(message, UserMessage):\n            return message\n\n        if isinstance(message.content, list):\n            text_contents = []\n            for content in message.content:\n                if getattr(content, 'type', None) == 'text':\n                    text_contents.append(content.text)\n\n            return UserMessage(content=\" \".join(text_contents))\n\n        return message\n\n    @staticmethod\n    def _load_config() -&gt; dict:\n        \"\"\"Load Llama-specific configuration from YAML file.\"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('openai-compat-llama')\n\n    @staticmethod\n    def _load_template(template_dir: str = \"src/prompt_builders/openai_compat/llama\") -&gt; Template:\n        \"\"\"Load Jinja2 template for Llama prompt generation.\"\"\"\n        template_path = Path(template_dir) / \"llama-3.3-70b.jinja\"\n        with open(template_path) as f:\n            return Template(f.read())\n\n    @staticmethod\n    def _format_tool_for_template(tool: Tool) -&gt; dict:\n        \"\"\"Format tool definition for Llama template usage.\"\"\"\n        return {\n            \"name\": tool.function.name,\n            \"description\": tool.function.description,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": tool.function.parameters.properties,\n                \"required\": tool.function.parameters.required\n            }\n        }\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/llama_prompt_builder/#src.prompt_builders.openai_compat.llama.llama_prompt_builder.OpenAICompatLlamaPromptBuilder.__init__","title":"<code>__init__(template_dir=None)</code>","text":"<p>Initialize the Llama prompt builder.</p> <p>Parameters:</p> Name Type Description Default <code>template_dir</code> <code>Optional[str]</code> <p>Custom directory for template files. If None, uses default directory 'src/prompt_builders/llama'.</p> <code>None</code> Source code in <code>src/prompt_builders/openai_compat/llama/llama_prompt_builder.py</code> <pre><code>def __init__(self, template_dir: Optional[str] = None):\n    \"\"\"Initialize the Llama prompt builder.\n\n    Args:\n        template_dir (Optional[str]): Custom directory for template files.\n            If None, uses default directory 'src/prompt_builders/llama'.\n    \"\"\"\n    super().__init__()\n    self.config = self._load_config()\n    self.template = self._load_template(template_dir) if template_dir else self._load_template()\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/llama_prompt_builder/#src.prompt_builders.openai_compat.llama.llama_prompt_builder.OpenAICompatLlamaPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build chat messages with tools embedded in system message.</p> <p>Creates or modifies the system message to include tool definitions while preserving the existing conversation structure.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified message list with tool information</p> Source code in <code>src/prompt_builders/openai_compat/llama/llama_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build chat messages with tools embedded in system message.\n\n    Creates or modifies the system message to include tool definitions\n    while preserving the existing conversation structure.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the modified message list with tool information\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    if not tool_definitions:\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    tool_names = [tool.function.name for tool in tool_definitions]\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(content=f\"{tool_info}\\n{existing_content}\")\n    else:\n        system_msg = SystemMessage(content=tool_info)\n        modified_history.insert(0, system_msg)\n\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/openai_compat/llama_prompt_builder/#src.prompt_builders.openai_compat.llama.llama_prompt_builder.OpenAICompatLlamaPromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>async</code>","text":"<p>Build text prompt using Llama-specific template.</p> <p>Constructs a complete prompt string using the Jinja2 template, incorporating conversation history, tool definitions, and model-specific tokens.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the formatted text prompt for generation</p> Source code in <code>src/prompt_builders/openai_compat/llama/llama_prompt_builder.py</code> <pre><code>async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build text prompt using Llama-specific template.\n\n    Constructs a complete prompt string using the Jinja2 template,\n    incorporating conversation history, tool definitions, and model-specific\n    tokens.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the formatted text prompt for generation\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # Preprocess conversation history to flatten user message content\n    processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n    # Format tool definitions for template\n    formatted_tools = [\n        self._format_tool_for_template(tool)\n        for tool in tool_definitions\n    ] if tool_definitions else None\n\n    # Prepare template variables\n    template_vars = {\n        'messages': processed_history,\n        'tools': formatted_tools,\n        'tools_in_user_message': False,\n        'add_generation_prompt': True,\n        'date_string': datetime.now().strftime(\"%d %b %Y\"),\n        'bos_token': self.config['tokens']['begin_text'],\n        'tool_instructions': self.config['system_prompt']['tool_instructions']\n    }\n\n    return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/","title":"WatsonX Prompt Builders Documentation","text":""},{"location":"reference/prompt_builders/watsonx/#overview","title":"Overview","text":"<p>The WatsonX Prompt Builders module provides specialized utilities for constructing prompts optimized for IBM's WatsonX AI models. These builders ensure that prompts are formatted correctly for different WatsonX models, enabling seamless interaction and improved response accuracy.</p>"},{"location":"reference/prompt_builders/watsonx/#components","title":"Components","text":""},{"location":"reference/prompt_builders/watsonx/#model-specific-prompt-builders","title":"Model-Specific Prompt Builders","text":"<ul> <li><code>GranitePromptBuilder</code>: Designed for WatsonX Granite models, ensuring optimal prompt structuring.</li> <li><code>LlamaPromptBuilder</code>: Tailored for WatsonX Llama models, adhering to their input format requirements.</li> <li><code>MistralPromptBuilder</code>: Specialized for WatsonX Mistral models, enhancing compatibility and performance.</li> </ul>"},{"location":"reference/prompt_builders/watsonx/granite/granite_prompt_builder/","title":"Granite","text":""},{"location":"reference/prompt_builders/watsonx/granite/granite_prompt_builder/#src.prompt_builders.watsonx.granite.granite_prompt_builder.WatsonXGranitePromptBuilder","title":"<code>src.prompt_builders.watsonx.granite.granite_prompt_builder.WatsonXGranitePromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>Prompt builder for the Granite model architecture.</p> <p>This class handles the construction of prompts and chat messages specifically formatted for Granite models, with support for tool definitions and context-aware message formatting.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration loaded from prompt_builders.yaml</p> <code>template</code> <code>Template</code> <p>Jinja2 template for text generation prompts</p> Source code in <code>src/prompt_builders/watsonx/granite/granite_prompt_builder.py</code> <pre><code>class WatsonXGranitePromptBuilder(BasePromptBuilder):\n    \"\"\"Prompt builder for the Granite model architecture.\n\n    This class handles the construction of prompts and chat messages specifically\n    formatted for Granite models, with support for tool definitions and\n    context-aware message formatting.\n\n    Attributes:\n        config (Dict): Configuration loaded from prompt_builders.yaml\n        template (Template): Jinja2 template for text generation prompts\n    \"\"\"\n    def __init__(self, template_dir: Optional[str] = None):\n        \"\"\"Initialize the Granite prompt builder.\n\n        Args:\n            template_dir (Optional[str]): Custom directory for template files.\n                If None, uses default directory 'src/prompt_builders/granite'.\n        \"\"\"\n        super().__init__()\n        self.config = self._load_config()\n        self.template = self._load_template(template_dir) if template_dir else self._load_template()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build chat messages with tools embedded in system message.\n\n        Creates or modifies the system message to include tool definitions\n        while preserving the existing conversation structure.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the modified message list with tool information\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # If no tools, return history as is\n        if not tool_definitions:\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        # Generate tool information\n        tool_names = [tool.function.name for tool in tool_definitions]\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n\n        # Create modified history list\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            # Prepend tool info to existing system message\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(\n                content=f\"{existing_content}\\n&lt;|start_of_role|&gt;tools&lt;|end_of_role|&gt;{tool_info}&lt;|end_of_text|&gt;\"\n            )\n        else:\n            # Create new system message with tool info and prepend to history\n            system_msg = SystemMessage(content=tool_info)\n            modified_history.insert(0, system_msg)\n\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build text prompt using Granite-specific template.\n\n        Constructs a complete prompt string using the Jinja2 template,\n        incorporating conversation history and tool definitions.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            str: Formatted prompt string ready for text generation\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # Preprocess conversation history to flatten user message content\n        processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n        # Format tool definitions for template\n        formatted_tools = [\n            self._format_tool_for_template(tool)\n            for tool in tool_definitions\n        ] if tool_definitions else None\n\n        # Prepare template variables\n        template_vars = {\n            'messages': processed_history,\n            'tools': formatted_tools,\n            'tools_in_user_message': False,\n            'add_generation_prompt': True,\n            'date_string': datetime.now().strftime(\"%d %b %Y\"),\n            'tool_instructions': self.config['system_prompt']['tool_instructions']\n        }\n\n        return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n\n    @staticmethod\n    def _preprocess_message(message: TextChatMessage) -&gt; TextChatMessage:\n        \"\"\"Preprocess message for Granite format.\"\"\"\n        if not isinstance(message, UserMessage):\n            return message\n\n        if isinstance(message.content, list):\n            # Extract text content from array of content objects\n            text_contents = []\n            for content in message.content:\n                if getattr(content, 'type', None) == 'text':\n                    text_contents.append(content.text)\n\n            # Create new UserMessage with flattened content\n            return UserMessage(content=\" \".join(text_contents))\n\n        return message\n\n    @staticmethod\n    def _format_tool_for_template(tool: Tool) -&gt; dict:\n        \"\"\"Format tool definition for Granite template usage.\"\"\"\n        return {\n            \"name\": tool.function.name,\n            \"description\": tool.function.description,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": tool.function.parameters.properties,\n                \"required\": tool.function.parameters.required\n            }\n        }\n\n    @staticmethod\n    def _load_config() -&gt; dict:\n        \"\"\"Load Granite-specific configuration from YAML file.\"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('watsonx-granite')\n\n    @staticmethod\n    def _load_template(template_dir: str = \"src/prompt_builders/watsonx/granite\") -&gt; Template:\n        \"\"\"Load Jinja2 template for Granite prompt generation.\"\"\"\n        template_path = Path(template_dir) / \"granite-3.1-8b.jinja\"\n        with open(template_path) as f:\n            return Template(f.read())\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/granite/granite_prompt_builder/#src.prompt_builders.watsonx.granite.granite_prompt_builder.WatsonXGranitePromptBuilder.__init__","title":"<code>__init__(template_dir=None)</code>","text":"<p>Initialize the Granite prompt builder.</p> <p>Parameters:</p> Name Type Description Default <code>template_dir</code> <code>Optional[str]</code> <p>Custom directory for template files. If None, uses default directory 'src/prompt_builders/granite'.</p> <code>None</code> Source code in <code>src/prompt_builders/watsonx/granite/granite_prompt_builder.py</code> <pre><code>def __init__(self, template_dir: Optional[str] = None):\n    \"\"\"Initialize the Granite prompt builder.\n\n    Args:\n        template_dir (Optional[str]): Custom directory for template files.\n            If None, uses default directory 'src/prompt_builders/granite'.\n    \"\"\"\n    super().__init__()\n    self.config = self._load_config()\n    self.template = self._load_template(template_dir) if template_dir else self._load_template()\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/granite/granite_prompt_builder/#src.prompt_builders.watsonx.granite.granite_prompt_builder.WatsonXGranitePromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build chat messages with tools embedded in system message.</p> <p>Creates or modifies the system message to include tool definitions while preserving the existing conversation structure.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified message list with tool information</p> Source code in <code>src/prompt_builders/watsonx/granite/granite_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build chat messages with tools embedded in system message.\n\n    Creates or modifies the system message to include tool definitions\n    while preserving the existing conversation structure.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the modified message list with tool information\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # If no tools, return history as is\n    if not tool_definitions:\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    # Generate tool information\n    tool_names = [tool.function.name for tool in tool_definitions]\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n\n    # Create modified history list\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        # Prepend tool info to existing system message\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(\n            content=f\"{existing_content}\\n&lt;|start_of_role|&gt;tools&lt;|end_of_role|&gt;{tool_info}&lt;|end_of_text|&gt;\"\n        )\n    else:\n        # Create new system message with tool info and prepend to history\n        system_msg = SystemMessage(content=tool_info)\n        modified_history.insert(0, system_msg)\n\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/granite/granite_prompt_builder/#src.prompt_builders.watsonx.granite.granite_prompt_builder.WatsonXGranitePromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>async</code>","text":"<p>Build text prompt using Granite-specific template.</p> <p>Constructs a complete prompt string using the Jinja2 template, incorporating conversation history and tool definitions.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>PromptBuilderOutput</code> <p>Formatted prompt string ready for text generation</p> Source code in <code>src/prompt_builders/watsonx/granite/granite_prompt_builder.py</code> <pre><code>async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build text prompt using Granite-specific template.\n\n    Constructs a complete prompt string using the Jinja2 template,\n    incorporating conversation history and tool definitions.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        str: Formatted prompt string ready for text generation\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # Preprocess conversation history to flatten user message content\n    processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n    # Format tool definitions for template\n    formatted_tools = [\n        self._format_tool_for_template(tool)\n        for tool in tool_definitions\n    ] if tool_definitions else None\n\n    # Prepare template variables\n    template_vars = {\n        'messages': processed_history,\n        'tools': formatted_tools,\n        'tools_in_user_message': False,\n        'add_generation_prompt': True,\n        'date_string': datetime.now().strftime(\"%d %b %Y\"),\n        'tool_instructions': self.config['system_prompt']['tool_instructions']\n    }\n\n    return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/llama/llama_prompt_builder/","title":"Llama","text":""},{"location":"reference/prompt_builders/watsonx/llama/llama_prompt_builder/#src.prompt_builders.watsonx.llama.llama_prompt_builder.WatsonXLlamaPromptBuilder","title":"<code>src.prompt_builders.watsonx.llama.llama_prompt_builder.WatsonXLlamaPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>Prompt builder for the Llama model architecture.</p> <p>This class handles the construction of prompts and chat messages specifically formatted for Llama models, with support for tool definitions and context-aware message formatting.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration loaded from prompt_builders.yaml.</p> <code>template</code> <code>Template</code> <p>Jinja2 template for text generation prompts.</p> Example <pre><code>builder = LlamaPromptBuilder()\n\n# Build chat messages\noutput = await builder.build_chat(PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n))\n\n# Build text prompt\noutput = await builder.build_text(PromptPayload(\n    conversation_history=history,\n    tool_definitions=tools\n))\n</code></pre> Source code in <code>src/prompt_builders/watsonx/llama/llama_prompt_builder.py</code> <pre><code>class WatsonXLlamaPromptBuilder(BasePromptBuilder):\n    \"\"\"Prompt builder for the Llama model architecture.\n\n    This class handles the construction of prompts and chat messages specifically\n    formatted for Llama models, with support for tool definitions and\n    context-aware message formatting.\n\n    Attributes:\n        config (Dict): Configuration loaded from prompt_builders.yaml.\n        template (Template): Jinja2 template for text generation prompts.\n\n    Example:\n        ```python\n        builder = LlamaPromptBuilder()\n\n        # Build chat messages\n        output = await builder.build_chat(PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        ))\n\n        # Build text prompt\n        output = await builder.build_text(PromptPayload(\n            conversation_history=history,\n            tool_definitions=tools\n        ))\n        ```\n    \"\"\"\n    def __init__(self, template_dir: Optional[str] = None):\n        \"\"\"Initialize the Llama prompt builder.\n\n        Args:\n            template_dir (Optional[str]): Custom directory for template files.\n                If None, uses default directory 'src/prompt_builders/llama'.\n        \"\"\"\n        super().__init__()\n        self.config = self._load_config()\n        self.template = self._load_template(template_dir) if template_dir else self._load_template()\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build chat messages with tools embedded in system message.\n\n        Creates or modifies the system message to include tool definitions\n        while preserving the existing conversation structure.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the modified message list with tool information\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        if not tool_definitions:\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        tool_names = [tool.function.name for tool in tool_definitions]\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n        tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n        modified_history = conversation_history.copy()\n\n        if conversation_history and isinstance(conversation_history[0], SystemMessage):\n            existing_content = conversation_history[0].content\n            modified_history[0] = SystemMessage(content=f\"{tool_info}\\n{existing_content}\")\n        else:\n            system_msg = SystemMessage(content=tool_info)\n            modified_history.insert(0, system_msg)\n\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build text prompt using Llama-specific template.\n\n        Constructs a complete prompt string using the Jinja2 template,\n        incorporating conversation history, tool definitions, and model-specific\n        tokens.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the formatted text prompt for generation\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # Preprocess conversation history to flatten user message content\n        processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n        # Format tool definitions for template\n        formatted_tools = [\n            self._format_tool_for_template(tool)\n            for tool in tool_definitions\n        ] if tool_definitions else None\n\n        # Prepare template variables\n        template_vars = {\n            'messages': processed_history,\n            'tools': formatted_tools,\n            'tools_in_user_message': False,\n            'add_generation_prompt': True,\n            'date_string': datetime.now().strftime(\"%d %b %Y\"),\n            'bos_token': self.config['tokens']['begin_text'],\n            'tool_instructions': self.config['system_prompt']['tool_instructions']\n        }\n\n        return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n\n    @staticmethod\n    def _preprocess_message(message: TextChatMessage) -&gt; TextChatMessage:\n        \"\"\"Preprocess message for Llama format.\"\"\"\n        if not isinstance(message, UserMessage):\n            return message\n\n        if isinstance(message.content, list):\n            # Extract text content from array of content objects\n            text_contents = []\n            for content in message.content:\n                if getattr(content, 'type', None) == 'text':\n                    text_contents.append(content.text)\n\n            # Create new UserMessage with flattened content\n            return UserMessage(content=\" \".join(text_contents))\n\n        return message\n\n    @staticmethod\n    def _load_config() -&gt; dict:\n        \"\"\"Load Llama-specific configuration from YAML file.\"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('watsonx-llama')\n\n    @staticmethod\n    def _load_template(template_dir: str = \"src/prompt_builders/watsonx/llama\") -&gt; Template:\n        \"\"\"Load Jinja2 template for Llama prompt generation.\"\"\"\n        template_path = Path(template_dir) / \"llama-3.3-70b.jinja\"\n        with open(template_path) as f:\n            return Template(f.read())\n\n    @staticmethod\n    def _format_tool_for_template(tool: Tool) -&gt; dict:\n        \"\"\"Format tool definition for Llama template usage.\"\"\"\n        return {\n            \"name\": tool.function.name,\n            \"description\": tool.function.description,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": tool.function.parameters.properties,\n                \"required\": tool.function.parameters.required\n            }\n        }\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/llama/llama_prompt_builder/#src.prompt_builders.watsonx.llama.llama_prompt_builder.WatsonXLlamaPromptBuilder.__init__","title":"<code>__init__(template_dir=None)</code>","text":"<p>Initialize the Llama prompt builder.</p> <p>Parameters:</p> Name Type Description Default <code>template_dir</code> <code>Optional[str]</code> <p>Custom directory for template files. If None, uses default directory 'src/prompt_builders/llama'.</p> <code>None</code> Source code in <code>src/prompt_builders/watsonx/llama/llama_prompt_builder.py</code> <pre><code>def __init__(self, template_dir: Optional[str] = None):\n    \"\"\"Initialize the Llama prompt builder.\n\n    Args:\n        template_dir (Optional[str]): Custom directory for template files.\n            If None, uses default directory 'src/prompt_builders/llama'.\n    \"\"\"\n    super().__init__()\n    self.config = self._load_config()\n    self.template = self._load_template(template_dir) if template_dir else self._load_template()\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/llama/llama_prompt_builder/#src.prompt_builders.watsonx.llama.llama_prompt_builder.WatsonXLlamaPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build chat messages with tools embedded in system message.</p> <p>Creates or modifies the system message to include tool definitions while preserving the existing conversation structure.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified message list with tool information</p> Source code in <code>src/prompt_builders/watsonx/llama/llama_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build chat messages with tools embedded in system message.\n\n    Creates or modifies the system message to include tool definitions\n    while preserving the existing conversation structure.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the modified message list with tool information\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    if not tool_definitions:\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    tool_names = [tool.function.name for tool in tool_definitions]\n    tool_section_header = self.config['system_prompt']['header'].format(\n        tools=\", \".join(tool_names),\n        date=datetime.now().strftime('%Y-%m-%d')\n    )\n    tool_instructions = self.config['system_prompt']['tool_instructions']\n    tool_info = await self._build_system_content(tool_definitions, tool_section_header, tool_instructions)\n    modified_history = conversation_history.copy()\n\n    if conversation_history and isinstance(conversation_history[0], SystemMessage):\n        existing_content = conversation_history[0].content\n        modified_history[0] = SystemMessage(content=f\"{tool_info}\\n{existing_content}\")\n    else:\n        system_msg = SystemMessage(content=tool_info)\n        modified_history.insert(0, system_msg)\n\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/llama/llama_prompt_builder/#src.prompt_builders.watsonx.llama.llama_prompt_builder.WatsonXLlamaPromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>async</code>","text":"<p>Build text prompt using Llama-specific template.</p> <p>Constructs a complete prompt string using the Jinja2 template, incorporating conversation history, tool definitions, and model-specific tokens.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the formatted text prompt for generation</p> Source code in <code>src/prompt_builders/watsonx/llama/llama_prompt_builder.py</code> <pre><code>async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build text prompt using Llama-specific template.\n\n    Constructs a complete prompt string using the Jinja2 template,\n    incorporating conversation history, tool definitions, and model-specific\n    tokens.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the formatted text prompt for generation\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # Preprocess conversation history to flatten user message content\n    processed_history = [self._preprocess_message(msg).model_dump() for msg in conversation_history]\n\n    # Format tool definitions for template\n    formatted_tools = [\n        self._format_tool_for_template(tool)\n        for tool in tool_definitions\n    ] if tool_definitions else None\n\n    # Prepare template variables\n    template_vars = {\n        'messages': processed_history,\n        'tools': formatted_tools,\n        'tools_in_user_message': False,\n        'add_generation_prompt': True,\n        'date_string': datetime.now().strftime(\"%d %b %Y\"),\n        'bos_token': self.config['tokens']['begin_text'],\n        'tool_instructions': self.config['system_prompt']['tool_instructions']\n    }\n\n    return PromptBuilderOutput(text_prompt=self.template.render(**template_vars))\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/mistral/mistral_prompt_builder/","title":"Mistral","text":""},{"location":"reference/prompt_builders/watsonx/mistral/mistral_prompt_builder/#src.prompt_builders.watsonx.mistral.mistral_prompt_builder.WatsonXMistralPromptBuilder","title":"<code>src.prompt_builders.watsonx.mistral.mistral_prompt_builder.WatsonXMistralPromptBuilder</code>","text":"<p>               Bases: <code>BasePromptBuilder</code></p> <p>Prompt builder for the Mistral model architecture.</p> <p>This class handles the construction of prompts and chat messages specifically formatted for Mistral models, including support for tool definitions, multi-modal content, and Mistral-specific message formatting.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>Configuration loaded from prompt_builders.yaml.</p> <code>model_name</code> <code>str</code> <p>Name of the Mistral model to use.</p> <code>tokenizer</code> <code>MistralTokenizer</code> <p>Tokenizer instance for the specified model.</p> Source code in <code>src/prompt_builders/watsonx/mistral/mistral_prompt_builder.py</code> <pre><code>class WatsonXMistralPromptBuilder(BasePromptBuilder):\n    \"\"\"Prompt builder for the Mistral model architecture.\n\n    This class handles the construction of prompts and chat messages specifically\n    formatted for Mistral models, including support for tool definitions,\n    multi-modal content, and Mistral-specific message formatting.\n\n    Attributes:\n        config (Dict): Configuration loaded from prompt_builders.yaml.\n        model_name (str): Name of the Mistral model to use.\n        tokenizer (MistralTokenizer): Tokenizer instance for the specified model.\n    \"\"\"\n    def __init__(self, model_name: str = 'mistral-large'):\n        \"\"\"Initialize the Mistral prompt builder.\n\n        Args:\n            model_name (str): Name of the Mistral model to use.\n                Defaults to 'mistral-large'.\n        \"\"\"\n        super().__init__()\n        self.config = self._load_config()\n        self.model_name = model_name\n        self.tokenizer = MistralTokenizer.from_model(model_name)\n\n    async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build chat messages with tools in the last assistant message.\n\n        Modifies the conversation history to include tool definitions in the\n        most recent assistant message, or creates a new one if none exists.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the modified message list with tool information\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # If no tools, return history as is\n        if not tool_definitions:\n            return PromptBuilderOutput(chat_messages=conversation_history)\n\n        tool_info = self._format_tool_definitions(tool_definitions)\n        modified_history = conversation_history.copy()\n\n        last_assistant_idx = None\n        for idx, msg in enumerate(modified_history):\n            if msg.role == 'assistant':\n                last_assistant_idx = idx\n\n        if last_assistant_idx is not None:\n            # Add tool info to the last assistant message\n            last_assistant = modified_history[last_assistant_idx]\n            existing_content = last_assistant.content or \"\"\n            modified_history[last_assistant_idx] = AssistantMessage(\n                content=f\"{existing_content}\\n{tool_info}\",\n            )\n        else:\n            # Create new assistant message with tool info if none exists\n            assistant_msg = AssistantMessage(content=tool_info)\n            modified_history.append(assistant_msg)\n\n        return PromptBuilderOutput(chat_messages=modified_history)\n\n    async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n        \"\"\"Build prompt string using Mistral's chat completion format.\n\n        Converts the conversation history and tool definitions into Mistral's\n        format and generates a tokenized prompt string.\n\n        Args:\n            payload (PromptPayload): The structured input containing conversation history,\n                                   tool definitions, and other context-specific data\n\n        Returns:\n            PromptBuilderOutput: Contains the formatted text prompt for generation\n        \"\"\"\n        conversation_history = payload.conversation_history\n        tool_definitions = payload.tool_definitions or []\n\n        # Convert messages to Mistral format\n        mistral_messages = self._process_conversation_history(conversation_history)\n\n        # Convert tool definitions to Mistral format\n        mistral_tools = self._process_tool_definitions(tool_definitions)\n\n        chat_request = ChatCompletionRequest(\n            tools=mistral_tools,\n            messages=mistral_messages,\n            model=self.model_name,\n        )\n\n        # Tokenize and get prompt text\n        tokenized = self.tokenizer.encode_chat_completion(chat_request)\n        return PromptBuilderOutput(text_prompt=tokenized.text)\n\n    def _format_tool_definitions(self, tool_definitions: List[Tool]) -&gt; str:\n        \"\"\"Format tool definitions in Mistral's required format.\"\"\"\n        formatted_tools = []\n\n        for tool in tool_definitions:\n            tool_dict = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.function.name,\n                    \"description\": tool.function.description,\n                    \"parameters\": tool.function.parameters.model_dump()\n                }\n            }\n            formatted_tools.append(tool_dict)\n\n        # Create the final string with the required format\n        tool_names = [tool.function.name for tool in tool_definitions]\n        tool_section_header = self.config['system_prompt']['header'].format(\n            tools=\", \".join(tool_names),\n            date=datetime.now().strftime('%Y-%m-%d')\n        )\n        tool_instructions = self.config['system_prompt']['tool_instructions']\n        tool_json = json.dumps(formatted_tools, separators=(',', ':'))\n        return f\"[AVAILABLE_TOOLS]{tool_section_header}\\n\\n{tool_json}\\n\\n{tool_instructions}[/AVAILABLE_TOOLS]\"\n\n    def _process_conversation_history(self, conversation_history: List[TextChatMessage]) -&gt; List:\n        \"\"\"Convert conversation history to Mistral's message format.\"\"\"\n        messages = []\n        for msg in conversation_history:\n            if msg.role == 'system':\n                messages.append(MistralSystemMessage(content=msg.content))\n            elif msg.role == 'user':\n                mistral_content = self._convert_user_content(msg.content)\n                messages.append(MistralUserMessage(content=mistral_content))\n            elif msg.role == 'assistant':\n                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n                    logger.debug(f\"Tool calls: {msg.tool_calls}\")\n                    tool_calls = [self._create_tool_call(tc) for tc in msg.tool_calls]\n                    messages.append(MistralAssistantMessage(content=None, tool_calls=tool_calls))\n                else:\n                    messages.append(MistralAssistantMessage(content=msg.content))\n            elif msg.role == 'tool':\n                messages.append(MistralToolMessage(\n                    content=msg.content,\n                    tool_call_id=msg.tool_call_id\n                ))\n        return messages\n\n    def _convert_user_content(self, content) -&gt; List:\n        \"\"\"Convert user message content to Mistral's chunk format.\"\"\"\n        # Handle plain string content\n        if isinstance(content, str):\n            return [TextChunk(text=content)]\n\n        # Handle list content\n        if not isinstance(content, list):\n            raise ValueError(f\"Content must be either string or list, got {type(content)}\")\n\n        converted_content = []\n        for item in content:\n            # Handle string items in list\n            if isinstance(item, str):\n                converted_content.append(TextChunk(text=item))\n                continue\n\n            # Handle structured content items\n            if not hasattr(item, 'type'):\n                raise ValueError(f\"Content item missing 'type' attribute: {item}\")\n\n            if item.type == 'text':\n                converted_content.append(TextChunk(text=item.text))\n            elif item.type == 'image_url':\n                image_url = item.image_url\n                detail = getattr(item, 'detail', None)\n                image_url_chunk = ImageURLChunk(\n                    image_url=ImageURL(url=image_url, detail=detail) if detail else image_url\n                )\n                converted_content.append(image_url_chunk)\n            else:\n                raise ValueError(f\"Unsupported content type: {item.type}\")\n\n        return converted_content\n\n    def _process_tool_definitions(self, tool_definitions: List[Tool]) -&gt; List[MistralTool]:\n        \"\"\"Convert tool definitions to Mistral's Tool format.\"\"\"\n        return [\n            MistralTool(function=Function(\n                name=tool.function.name,\n                description=tool.function.description,\n                parameters=tool.function.parameters.model_dump(),\n            ))\n            for tool in tool_definitions\n        ]\n\n    def _create_tool_call(self, tool_call_data: ToolCall) -&gt; MistralToolCall:\n        \"\"\"Convert tool call data to Mistral's format.\"\"\"\n        function_name = tool_call_data.function.name\n        arguments = tool_call_data.function.arguments\n\n        # Convert arguments to JSON string if needed\n        arguments_str = (\n            json.dumps(arguments)\n            if not isinstance(arguments, str)\n            else arguments\n        )\n\n        return MistralToolCall(\n            function=MistralFunctionCall(\n                name=function_name,\n                arguments=arguments_str\n            )\n        )\n\n    @staticmethod\n    def _load_config() -&gt; Dict:\n        \"\"\"Load Mistral-specific configuration from YAML file.\"\"\"\n        config_path = Path(\"src/configs/prompt_builders.yaml\")\n        with config_path.open() as f:\n            config = yaml.safe_load(f)\n            return config.get('watsonx-mistral')\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/mistral/mistral_prompt_builder/#src.prompt_builders.watsonx.mistral.mistral_prompt_builder.WatsonXMistralPromptBuilder.__init__","title":"<code>__init__(model_name='mistral-large')</code>","text":"<p>Initialize the Mistral prompt builder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the Mistral model to use. Defaults to 'mistral-large'.</p> <code>'mistral-large'</code> Source code in <code>src/prompt_builders/watsonx/mistral/mistral_prompt_builder.py</code> <pre><code>def __init__(self, model_name: str = 'mistral-large'):\n    \"\"\"Initialize the Mistral prompt builder.\n\n    Args:\n        model_name (str): Name of the Mistral model to use.\n            Defaults to 'mistral-large'.\n    \"\"\"\n    super().__init__()\n    self.config = self._load_config()\n    self.model_name = model_name\n    self.tokenizer = MistralTokenizer.from_model(model_name)\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/mistral/mistral_prompt_builder/#src.prompt_builders.watsonx.mistral.mistral_prompt_builder.WatsonXMistralPromptBuilder.build_chat","title":"<code>build_chat(payload)</code>  <code>async</code>","text":"<p>Build chat messages with tools in the last assistant message.</p> <p>Modifies the conversation history to include tool definitions in the most recent assistant message, or creates a new one if none exists.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the modified message list with tool information</p> Source code in <code>src/prompt_builders/watsonx/mistral/mistral_prompt_builder.py</code> <pre><code>async def build_chat(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build chat messages with tools in the last assistant message.\n\n    Modifies the conversation history to include tool definitions in the\n    most recent assistant message, or creates a new one if none exists.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the modified message list with tool information\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # If no tools, return history as is\n    if not tool_definitions:\n        return PromptBuilderOutput(chat_messages=conversation_history)\n\n    tool_info = self._format_tool_definitions(tool_definitions)\n    modified_history = conversation_history.copy()\n\n    last_assistant_idx = None\n    for idx, msg in enumerate(modified_history):\n        if msg.role == 'assistant':\n            last_assistant_idx = idx\n\n    if last_assistant_idx is not None:\n        # Add tool info to the last assistant message\n        last_assistant = modified_history[last_assistant_idx]\n        existing_content = last_assistant.content or \"\"\n        modified_history[last_assistant_idx] = AssistantMessage(\n            content=f\"{existing_content}\\n{tool_info}\",\n        )\n    else:\n        # Create new assistant message with tool info if none exists\n        assistant_msg = AssistantMessage(content=tool_info)\n        modified_history.append(assistant_msg)\n\n    return PromptBuilderOutput(chat_messages=modified_history)\n</code></pre>"},{"location":"reference/prompt_builders/watsonx/mistral/mistral_prompt_builder/#src.prompt_builders.watsonx.mistral.mistral_prompt_builder.WatsonXMistralPromptBuilder.build_text","title":"<code>build_text(payload)</code>  <code>async</code>","text":"<p>Build prompt string using Mistral's chat completion format.</p> <p>Converts the conversation history and tool definitions into Mistral's format and generates a tokenized prompt string.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>PromptPayload</code> <p>The structured input containing conversation history,                    tool definitions, and other context-specific data</p> required <p>Returns:</p> Name Type Description <code>PromptBuilderOutput</code> <code>PromptBuilderOutput</code> <p>Contains the formatted text prompt for generation</p> Source code in <code>src/prompt_builders/watsonx/mistral/mistral_prompt_builder.py</code> <pre><code>async def build_text(self, payload: PromptPayload) -&gt; PromptBuilderOutput:\n    \"\"\"Build prompt string using Mistral's chat completion format.\n\n    Converts the conversation history and tool definitions into Mistral's\n    format and generates a tokenized prompt string.\n\n    Args:\n        payload (PromptPayload): The structured input containing conversation history,\n                               tool definitions, and other context-specific data\n\n    Returns:\n        PromptBuilderOutput: Contains the formatted text prompt for generation\n    \"\"\"\n    conversation_history = payload.conversation_history\n    tool_definitions = payload.tool_definitions or []\n\n    # Convert messages to Mistral format\n    mistral_messages = self._process_conversation_history(conversation_history)\n\n    # Convert tool definitions to Mistral format\n    mistral_tools = self._process_tool_definitions(tool_definitions)\n\n    chat_request = ChatCompletionRequest(\n        tools=mistral_tools,\n        messages=mistral_messages,\n        model=self.model_name,\n    )\n\n    # Tokenize and get prompt text\n    tokenized = self.tokenizer.encode_chat_completion(chat_request)\n    return PromptBuilderOutput(text_prompt=tokenized.text)\n</code></pre>"},{"location":"reference/tools/","title":"Tools Documentation","text":""},{"location":"reference/tools/#overview","title":"Overview","text":"<p>The Tools component provides a powerful and extensible framework for implementing and managing external tool integrations. This module serves as the backbone for all external interactions, enabling the system to perform specific actions and retrieve information from various services.</p> <pre><code>graph TB\n    A[Client Request] --&gt; Z[Tool Registry]\n    Z --&gt; B{Tool Selection}\n\n    B --&gt; F[\"REST Tool Implementations&lt;br&gt;&lt;small&gt;(e.g., WeatherTool, WikipediaTool,&lt;br&gt;DuckDuckGoSearchTool)&lt;/small&gt;\"]\n    B --&gt; G[\"Custom Tool Implementations&lt;br&gt;&lt;small&gt;(e.g., RAGTool, Custom Tools)&lt;/small&gt;\"]\n\n    F --&gt; H[External APIs]\n    G --&gt; I[External Services]\n    G --&gt; J[\"Local Operations&lt;br&gt;&lt;small&gt;(e.g., Calculator, Graphing,&lt;br&gt;Python Execution)&lt;/small&gt;\"]\n\n    style A stroke:#333,stroke-width:2px\n    style Z stroke:#333,stroke-width:2px\n    style B stroke:#333,stroke-width:2px\n    style F stroke:#333,stroke-width:1px\n    style G stroke:#333,stroke-width:1px\n    style J stroke:#333,stroke-width:1px</code></pre>"},{"location":"reference/tools/#component-architecture","title":"Component Architecture","text":""},{"location":"reference/tools/#tool-execution-flow-api-tool-example","title":"Tool Execution Flow (API Tool Example)","text":"<pre><code>sequenceDiagram\n    participant C as Client\n    participant R as ToolRegistry\n    participant T as Tool\n    participant E as External Service\n\n    C-&gt;&gt;R: Execute Tool Request\n    R-&gt;&gt;T: Execute Tool\n    T-&gt;&gt;E: API Request\n    E--&gt;&gt;T: Response\n    T--&gt;&gt;C: Processed Result</code></pre>"},{"location":"reference/tools/#core-components","title":"Core Components","text":""},{"location":"reference/tools/#base-tool-interfaces","title":"Base Tool Interfaces","text":"<p>Located in <code>src/tools/core/</code>:</p> <ul> <li> <p>BaseTool</p> <ul> <li>Foundation interface for all tools</li> <li>Defines standard execution patterns</li> <li>See Base Tool</li> </ul> </li> <li> <p>BaseRESTTool</p> <ul> <li>Extended functionality for REST APIs</li> <li>Built-in HTTP method handling</li> <li>See Base Rest Tool</li> </ul> </li> <li> <p>ToolRegistry</p> <ul> <li>Central tool management system</li> <li>Handles tool registration and access</li> <li>See Tool Registry</li> </ul> </li> </ul>"},{"location":"reference/tools/#example-tool-implementations","title":"Example Tool Implementations","text":"<p>Located in <code>src/tools/implementations/</code>:</p> <ul> <li> <p>RAGTool</p> <ul> <li>Retrieval-Augmented Generation</li> <li>Enhances responses with external knowledge</li> <li>See Rag Tool Example</li> </ul> </li> <li> <p>WeatherTool</p> <ul> <li>Weather information service integration</li> <li>Real-time weather data access</li> <li>See Weather Tool Example</li> </ul> </li> <li> <p>WikipediaTool</p> <ul> <li>Wikipedia article retrieval and summarization</li> <li>Access to comprehensive knowledge base</li> <li>See Wikipedia Tool Example</li> </ul> </li> </ul>"},{"location":"reference/tools/#utils","title":"Utils","text":"<p>Located in <code>src/tools/core/utils/</code>:</p> <ul> <li> <p>TokenManager</p> <ul> <li>OAuth2 credential management</li> <li>Secure token handling</li> <li>See Token Manager</li> </ul> </li> </ul>"},{"location":"reference/tools/#parsers","title":"Parsers","text":"<p>Located in <code>src/tools/core/parsers/</code>:</p> <ul> <li> <p>BaseToolCallParser</p> <ul> <li>Abstract parsing interface</li> <li>See Base Parser</li> </ul> </li> <li> <p>JSONToolCallParser</p> <ul> <li>JSON format handling</li> <li>See JSON Parser</li> </ul> </li> <li> <p>NonJSONToolCallParser</p> <ul> <li>Alternative format support</li> <li>See Non-JSON Parser</li> </ul> </li> </ul>"},{"location":"reference/tools/#further-documentation","title":"Further Documentation","text":"<ul> <li>See Base Tool for custom tool creation</li> <li>See Base REST Tool for REST implementation details</li> <li>Check Tool Implementations for specific tool documentation</li> <li>Visit Parsers for parsing documentation</li> <li>Explore Utils for utility references</li> </ul>"},{"location":"reference/tools/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Always use environment variables for sensitive credentials</li> <li>Implement proper request validation</li> <li>Use HTTPS for external communications</li> <li>Follow least privilege principle</li> <li>Regularly audit tool access patterns</li> </ol>"},{"location":"reference/tools/core/base_rest_tool/","title":"Base Rest Tool","text":""},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.HttpMethod","title":"<code>src.tools.core.base_rest_tool.HttpMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported HTTP methods</p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>class HttpMethod(Enum):\n    \"\"\"Supported HTTP methods\"\"\"\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    PATCH = \"PATCH\"\n    DELETE = \"DELETE\"\n    HEAD = \"HEAD\"\n    OPTIONS = \"OPTIONS\"\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.ResponseFormat","title":"<code>src.tools.core.base_rest_tool.ResponseFormat</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported response formats</p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>class ResponseFormat(Enum):\n    \"\"\"Supported response formats\"\"\"\n    JSON = \"json\"\n    TEXT = \"text\"\n    BINARY = \"binary\"\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool","title":"<code>src.tools.core.base_rest_tool.BaseRESTTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>class BaseRESTTool(BaseTool):\n    def __init__(self, config: Optional[Dict] = None):\n        super().__init__()\n        self.config = config or {}\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n        # Base configuration\n        self.endpoint: str = self.config.get(\"endpoint_url\")\n        self.token_url = self.config.get(\"token_url\")\n        self.api_key_env = self.config.get(\"api_key_env\")\n        self.client_secret_env = self.config.get(\"client_secret_env\")\n\n        # Additional OpenAI tool configuration\n        self.strict = self.config.get(\"strict\", False)\n\n        # Enhanced configuration (keep existing config...)\n        self.content_type = self.config.get(\"content_type\", \"application/json\")\n        self.rate_limit = self.config.get(\"rate_limit\", 0)\n        self.default_timeout = self.config.get(\"default_timeout\", 30)\n        self.max_retries = self.config.get(\"max_retries\", 3)\n        self.retry_delay = self.config.get(\"retry_delay\", 1.0)\n\n        # Initialize rate limiting\n        self._request_lock = Lock()\n        self._last_request_time = 0\n\n        # Validate required configuration\n        if not self.endpoint:\n            raise ValueError(\"The 'endpoint_url' is required in the configuration.\")\n\n        # Setup authentication\n        api_key = os.getenv(self.api_key_env) if self.api_key_env else None\n        client_secret = os.getenv(self.client_secret_env) if self.client_secret_env else None\n\n        if self.token_url and api_key and client_secret:\n            self.token_manager = OAuth2ClientCredentialsManager(\n                api_key=api_key,\n                client_secret_base64=client_secret,\n                token_url=self.token_url\n            )\n        else:\n            self.token_manager = None\n\n        # Initialize middleware hooks\n        self._request_middleware: List[callable] = []\n        self._response_middleware: List[callable] = []\n\n        # Ensure parameters has the correct structure\n        if not self.parameters:\n            self.parameters = {\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n                \"additionalProperties\": False\n            }\n\n    def add_request_middleware(self, middleware: callable):\n        \"\"\"Add middleware function to modify requests before sending.\n\n        Args:\n            middleware (callable): Function that takes and returns a request dict.\n        \"\"\"\n        self._request_middleware.append(middleware)\n\n    def add_response_middleware(self, middleware: callable):\n        \"\"\"Add middleware function to modify responses before returning.\n\n        Args:\n            middleware (callable): Function that takes and returns a response object.\n        \"\"\"\n        self._response_middleware.append(middleware)\n\n    async def _apply_request_middleware(self, request_data: Dict) -&gt; Dict:\n        \"\"\"Apply all request middleware functions in order.\"\"\"\n        for middleware in self._request_middleware:\n            request_data = await middleware(request_data)\n        return request_data\n\n    async def _apply_response_middleware(self, response_data: Any) -&gt; Any:\n        \"\"\"Apply all response middleware functions in order.\"\"\"\n        for middleware in self._response_middleware:\n            response_data = await middleware(response_data)\n        return response_data\n\n    async def _enforce_rate_limit(self):\n        \"\"\"Enforce the configured rate limit.\"\"\"\n        if self.rate_limit &gt; 0:\n            async with self._request_lock:\n                current_time = time.time()\n                elapsed = current_time - self._last_request_time\n                if elapsed &lt; 1.0 / self.rate_limit:\n                    await sleep(1.0 / self.rate_limit - elapsed)\n                self._last_request_time = time.time()\n\n    def _get_cache_key(self, method: str, endpoint_url: str, params: Optional[Dict], data: Optional[Dict]) -&gt; str:\n        \"\"\"Generate a cache key for the request.\"\"\"\n        return f\"{method}:{endpoint_url}:{hash(frozenset(params.items() if params else ()))}:{hash(frozenset(data.items() if data else ()))}\"\n\n    async def get_access_token(self) -&gt; Optional[str]:\n        \"\"\"Retrieve access token for API authentication.\n\n        Returns:\n            Optional[str]: Access token if available, None otherwise.\n\n        Raises:\n            RuntimeError: If token retrieval fails.\n        \"\"\"\n        if self.token_manager:\n            try:\n                access_token = await self.token_manager.get_token()\n                if not access_token:\n                    error_msg = \"Failed to retrieve access token - token manager returned None\"\n                    stack_trace = ''.join(traceback.format_stack()[:-1])\n                    raise RuntimeError(f\"{error_msg}\\nStack trace:\\n{stack_trace}\")\n                return access_token\n            except Exception as e:\n                stack_trace = ''.join(traceback.format_exception(type(e), e, e.__traceback__))\n                raise RuntimeError(f\"Error retrieving access token: {str(e)}\\nStack trace:\\n{stack_trace}\") from e\n        return None\n\n    async def make_request(\n            self,\n            method: Union[str, HttpMethod],\n            params: Optional[Dict] = None,\n            data: Optional[Dict] = None,\n            use_token: bool = True,\n            endpoint_url: Optional[str] = None,\n            additional_headers: Optional[Dict] = None,\n            response_format: Union[str, ResponseFormat] = ResponseFormat.JSON,\n            timeout: Optional[float] = None\n    ) -&gt; Any:\n        \"\"\"Make an HTTP request with enhanced features.\n\n        Args:\n            method: HTTP method to use.\n            params: Query parameters.\n            data: Request body data.\n            use_token: Whether to use authentication token.\n            endpoint_url: Override default endpoint URL.\n            additional_headers: Additional HTTP headers.\n            response_format: Desired response format.\n            timeout: Request timeout in seconds.\n\n        Returns:\n            Any: Response data in the specified format.\n\n        Raises:\n            ValueError: If response format is unsupported.\n            aiohttp.ClientError: On network errors.\n        \"\"\"\n        # Normalize parameters\n        if isinstance(method, HttpMethod):\n            method = method.value\n        if isinstance(response_format, ResponseFormat):\n            response_format = response_format.value\n        endpoint_url = endpoint_url or self.endpoint\n        timeout = timeout or self.default_timeout\n\n        # Prepare headers\n        headers = {\n            \"Content-Type\": self.content_type,\n            \"Cache-Control\": \"no-cache\"\n        }\n        if self.api_key_env:\n            headers[\"apikey\"] = os.getenv(self.api_key_env)\n        if additional_headers:\n            headers.update(additional_headers)\n\n        # Add authentication token if required\n        if use_token and self.token_manager:\n            headers[\"Authorization\"] = f\"Bearer {await self.get_access_token()}\"\n\n        # Prepare request data for middleware\n        request_data = {\n            \"method\": method,\n            \"url\": endpoint_url,\n            \"headers\": headers,\n            \"params\": params,\n            \"data\": data,\n            \"timeout\": timeout\n        }\n\n        # Apply request middleware\n        request_data = await self._apply_request_middleware(request_data)\n\n        # Enforce rate limit\n        await self._enforce_rate_limit()\n\n        # Make request with retry logic\n        for attempt in range(self.max_retries):\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.request(**request_data) as response:\n                        # Handle response based on status code\n                        if response.status == 200:\n                            if response_format == ResponseFormat.JSON.value:\n                                try:\n                                    result = await response.json()\n                                except json.JSONDecodeError:\n                                    self.logger.error(\"Failed to decode JSON from response\")\n                                    return {\"error\": \"Invalid JSON response from server\"}\n                            elif response_format == ResponseFormat.TEXT.value:\n                                result = await response.text()\n                            elif response_format == ResponseFormat.BINARY.value:\n                                result = await response.read()\n                            else:\n                                raise ValueError(f\"Unsupported response format: {response_format}\")\n\n                            # Apply response middleware\n                            result = await self._apply_response_middleware(result)\n\n                            return result\n\n                        # Handle error responses\n                        error_response = await response.text()\n                        if response.status == 400:\n                            return {\"error\": f\"Bad Request: {error_response}\"}\n                        elif response.status == 401:\n                            return {\"error\": \"Unauthorized access - check API key or token.\"}\n                        elif response.status == 403:\n                            return {\"error\": \"Forbidden - insufficient permissions.\"}\n                        elif response.status == 404:\n                            return {\"error\": \"Resource not found - verify endpoint URL.\"}\n                        elif response.status &gt;= 500:\n                            if attempt &lt; self.max_retries - 1:\n                                await sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff\n                                continue\n                            return {\"error\": \"Server error - the API is currently unavailable.\"}\n                        else:\n                            return {\"error\": f\"Unexpected status code {response.status}: {error_response}\"}\n\n            except aiohttp.ClientError as e:\n                if attempt &lt; self.max_retries - 1:\n                    await sleep(self.retry_delay * (2 ** attempt))\n                    continue\n                self.logger.error(f\"Network error: {str(e)}\", exc_info=True)\n                return {\"error\": f\"Network error: {str(e)}\"}\n\n            except Exception as e:\n                self.logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n                return {\"error\": f\"Unexpected error: {str(e)}\"}\n\n    @abstractmethod\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs):\n        \"\"\"Execute the tool's main functionality.\"\"\"\n        pass\n\n    @abstractmethod\n    def parse_output(self, output: str):\n        \"\"\"Parse the tool's output.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool.add_request_middleware","title":"<code>add_request_middleware(middleware)</code>","text":"<p>Add middleware function to modify requests before sending.</p> <p>Parameters:</p> Name Type Description Default <code>middleware</code> <code>callable</code> <p>Function that takes and returns a request dict.</p> required Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>def add_request_middleware(self, middleware: callable):\n    \"\"\"Add middleware function to modify requests before sending.\n\n    Args:\n        middleware (callable): Function that takes and returns a request dict.\n    \"\"\"\n    self._request_middleware.append(middleware)\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool.add_response_middleware","title":"<code>add_response_middleware(middleware)</code>","text":"<p>Add middleware function to modify responses before returning.</p> <p>Parameters:</p> Name Type Description Default <code>middleware</code> <code>callable</code> <p>Function that takes and returns a response object.</p> required Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>def add_response_middleware(self, middleware: callable):\n    \"\"\"Add middleware function to modify responses before returning.\n\n    Args:\n        middleware (callable): Function that takes and returns a response object.\n    \"\"\"\n    self._response_middleware.append(middleware)\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool.execute","title":"<code>execute(context=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute the tool's main functionality.</p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>@abstractmethod\nasync def execute(self, context: Optional[StreamContext] = None, **kwargs):\n    \"\"\"Execute the tool's main functionality.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool.get_access_token","title":"<code>get_access_token()</code>  <code>async</code>","text":"<p>Retrieve access token for API authentication.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: Access token if available, None otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If token retrieval fails.</p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>async def get_access_token(self) -&gt; Optional[str]:\n    \"\"\"Retrieve access token for API authentication.\n\n    Returns:\n        Optional[str]: Access token if available, None otherwise.\n\n    Raises:\n        RuntimeError: If token retrieval fails.\n    \"\"\"\n    if self.token_manager:\n        try:\n            access_token = await self.token_manager.get_token()\n            if not access_token:\n                error_msg = \"Failed to retrieve access token - token manager returned None\"\n                stack_trace = ''.join(traceback.format_stack()[:-1])\n                raise RuntimeError(f\"{error_msg}\\nStack trace:\\n{stack_trace}\")\n            return access_token\n        except Exception as e:\n            stack_trace = ''.join(traceback.format_exception(type(e), e, e.__traceback__))\n            raise RuntimeError(f\"Error retrieving access token: {str(e)}\\nStack trace:\\n{stack_trace}\") from e\n    return None\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool.make_request","title":"<code>make_request(method, params=None, data=None, use_token=True, endpoint_url=None, additional_headers=None, response_format=ResponseFormat.JSON, timeout=None)</code>  <code>async</code>","text":"<p>Make an HTTP request with enhanced features.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Union[str, HttpMethod]</code> <p>HTTP method to use.</p> required <code>params</code> <code>Optional[Dict]</code> <p>Query parameters.</p> <code>None</code> <code>data</code> <code>Optional[Dict]</code> <p>Request body data.</p> <code>None</code> <code>use_token</code> <code>bool</code> <p>Whether to use authentication token.</p> <code>True</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>Override default endpoint URL.</p> <code>None</code> <code>additional_headers</code> <code>Optional[Dict]</code> <p>Additional HTTP headers.</p> <code>None</code> <code>response_format</code> <code>Union[str, ResponseFormat]</code> <p>Desired response format.</p> <code>JSON</code> <code>timeout</code> <code>Optional[float]</code> <p>Request timeout in seconds.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Response data in the specified format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If response format is unsupported.</p> <code>ClientError</code> <p>On network errors.</p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>async def make_request(\n        self,\n        method: Union[str, HttpMethod],\n        params: Optional[Dict] = None,\n        data: Optional[Dict] = None,\n        use_token: bool = True,\n        endpoint_url: Optional[str] = None,\n        additional_headers: Optional[Dict] = None,\n        response_format: Union[str, ResponseFormat] = ResponseFormat.JSON,\n        timeout: Optional[float] = None\n) -&gt; Any:\n    \"\"\"Make an HTTP request with enhanced features.\n\n    Args:\n        method: HTTP method to use.\n        params: Query parameters.\n        data: Request body data.\n        use_token: Whether to use authentication token.\n        endpoint_url: Override default endpoint URL.\n        additional_headers: Additional HTTP headers.\n        response_format: Desired response format.\n        timeout: Request timeout in seconds.\n\n    Returns:\n        Any: Response data in the specified format.\n\n    Raises:\n        ValueError: If response format is unsupported.\n        aiohttp.ClientError: On network errors.\n    \"\"\"\n    # Normalize parameters\n    if isinstance(method, HttpMethod):\n        method = method.value\n    if isinstance(response_format, ResponseFormat):\n        response_format = response_format.value\n    endpoint_url = endpoint_url or self.endpoint\n    timeout = timeout or self.default_timeout\n\n    # Prepare headers\n    headers = {\n        \"Content-Type\": self.content_type,\n        \"Cache-Control\": \"no-cache\"\n    }\n    if self.api_key_env:\n        headers[\"apikey\"] = os.getenv(self.api_key_env)\n    if additional_headers:\n        headers.update(additional_headers)\n\n    # Add authentication token if required\n    if use_token and self.token_manager:\n        headers[\"Authorization\"] = f\"Bearer {await self.get_access_token()}\"\n\n    # Prepare request data for middleware\n    request_data = {\n        \"method\": method,\n        \"url\": endpoint_url,\n        \"headers\": headers,\n        \"params\": params,\n        \"data\": data,\n        \"timeout\": timeout\n    }\n\n    # Apply request middleware\n    request_data = await self._apply_request_middleware(request_data)\n\n    # Enforce rate limit\n    await self._enforce_rate_limit()\n\n    # Make request with retry logic\n    for attempt in range(self.max_retries):\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.request(**request_data) as response:\n                    # Handle response based on status code\n                    if response.status == 200:\n                        if response_format == ResponseFormat.JSON.value:\n                            try:\n                                result = await response.json()\n                            except json.JSONDecodeError:\n                                self.logger.error(\"Failed to decode JSON from response\")\n                                return {\"error\": \"Invalid JSON response from server\"}\n                        elif response_format == ResponseFormat.TEXT.value:\n                            result = await response.text()\n                        elif response_format == ResponseFormat.BINARY.value:\n                            result = await response.read()\n                        else:\n                            raise ValueError(f\"Unsupported response format: {response_format}\")\n\n                        # Apply response middleware\n                        result = await self._apply_response_middleware(result)\n\n                        return result\n\n                    # Handle error responses\n                    error_response = await response.text()\n                    if response.status == 400:\n                        return {\"error\": f\"Bad Request: {error_response}\"}\n                    elif response.status == 401:\n                        return {\"error\": \"Unauthorized access - check API key or token.\"}\n                    elif response.status == 403:\n                        return {\"error\": \"Forbidden - insufficient permissions.\"}\n                    elif response.status == 404:\n                        return {\"error\": \"Resource not found - verify endpoint URL.\"}\n                    elif response.status &gt;= 500:\n                        if attempt &lt; self.max_retries - 1:\n                            await sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff\n                            continue\n                        return {\"error\": \"Server error - the API is currently unavailable.\"}\n                    else:\n                        return {\"error\": f\"Unexpected status code {response.status}: {error_response}\"}\n\n        except aiohttp.ClientError as e:\n            if attempt &lt; self.max_retries - 1:\n                await sleep(self.retry_delay * (2 ** attempt))\n                continue\n            self.logger.error(f\"Network error: {str(e)}\", exc_info=True)\n            return {\"error\": f\"Network error: {str(e)}\"}\n\n        except Exception as e:\n            self.logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n            return {\"error\": f\"Unexpected error: {str(e)}\"}\n</code></pre>"},{"location":"reference/tools/core/base_rest_tool/#src.tools.core.base_rest_tool.BaseRESTTool.parse_output","title":"<code>parse_output(output)</code>  <code>abstractmethod</code>","text":"<p>Parse the tool's output.</p> Source code in <code>src/tools/core/base_rest_tool.py</code> <pre><code>@abstractmethod\ndef parse_output(self, output: str):\n    \"\"\"Parse the tool's output.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/tools/core/base_tool/","title":"Base Tool","text":""},{"location":"reference/tools/core/base_tool/#src.tools.core.base_tool.BaseTool","title":"<code>src.tools.core.base_tool.BaseTool</code>","text":"<p>Abstract base class for all tools in the system.</p> <p>Provides the foundation for tool implementation with standard interfaces for execution, definition retrieval, and output parsing.</p> Source code in <code>src/tools/core/base_tool.py</code> <pre><code>class BaseTool:\n    \"\"\"Abstract base class for all tools in the system.\n\n    Provides the foundation for tool implementation with standard interfaces\n    for execution, definition retrieval, and output parsing.\n    \"\"\"\n    name: str\n\n    def __init__(self, config: Optional[Dict] = None):\n        self.config = config or {}\n        self.description = None\n        self.parameters = {}\n        self.strict = False\n\n    @abstractmethod\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n        \"\"\"Execute the tool's main functionality.\"\"\"\n        pass\n\n    def get_definition(self) -&gt; Tool:\n        \"\"\"Get the tool's OpenAI-compatible definition.\n\n        Returns:\n            Tool: Tool definition including type, function details and parameters.\n        \"\"\"\n        return Tool(\n            type=\"function\",\n            function=Function(\n                name=self.name,\n                description=self.description,\n                parameters=FunctionParameters(\n                    type=\"object\",\n                    properties=self.parameters.get(\"properties\", {}),\n                    required=self.parameters.get(\"required\", []),\n                    additionalProperties=self.parameters.get(\"additionalProperties\", None)\n                ),\n                strict=self.strict\n            )\n        )\n\n    @abstractmethod\n    def parse_output(self, output: str):\n        \"\"\"Parse the tool's output.\"\"\"\n        pass\n\n    def get_tool_specific_instruction(self) -&gt; str:\n        \"\"\"Get formatted tool-specific instruction.\"\"\"\n        return \"\"\n</code></pre>"},{"location":"reference/tools/core/base_tool/#src.tools.core.base_tool.BaseTool.execute","title":"<code>execute(context=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute the tool's main functionality.</p> Source code in <code>src/tools/core/base_tool.py</code> <pre><code>@abstractmethod\nasync def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n    \"\"\"Execute the tool's main functionality.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/tools/core/base_tool/#src.tools.core.base_tool.BaseTool.get_definition","title":"<code>get_definition()</code>","text":"<p>Get the tool's OpenAI-compatible definition.</p> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>Tool definition including type, function details and parameters.</p> Source code in <code>src/tools/core/base_tool.py</code> <pre><code>def get_definition(self) -&gt; Tool:\n    \"\"\"Get the tool's OpenAI-compatible definition.\n\n    Returns:\n        Tool: Tool definition including type, function details and parameters.\n    \"\"\"\n    return Tool(\n        type=\"function\",\n        function=Function(\n            name=self.name,\n            description=self.description,\n            parameters=FunctionParameters(\n                type=\"object\",\n                properties=self.parameters.get(\"properties\", {}),\n                required=self.parameters.get(\"required\", []),\n                additionalProperties=self.parameters.get(\"additionalProperties\", None)\n            ),\n            strict=self.strict\n        )\n    )\n</code></pre>"},{"location":"reference/tools/core/base_tool/#src.tools.core.base_tool.BaseTool.get_tool_specific_instruction","title":"<code>get_tool_specific_instruction()</code>","text":"<p>Get formatted tool-specific instruction.</p> Source code in <code>src/tools/core/base_tool.py</code> <pre><code>def get_tool_specific_instruction(self) -&gt; str:\n    \"\"\"Get formatted tool-specific instruction.\"\"\"\n    return \"\"\n</code></pre>"},{"location":"reference/tools/core/base_tool/#src.tools.core.base_tool.BaseTool.parse_output","title":"<code>parse_output(output)</code>  <code>abstractmethod</code>","text":"<p>Parse the tool's output.</p> Source code in <code>src/tools/core/base_tool.py</code> <pre><code>@abstractmethod\ndef parse_output(self, output: str):\n    \"\"\"Parse the tool's output.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/tools/core/tool_registry/","title":"Tool Registry","text":""},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry","title":"<code>src.tools.core.tool_registry.ToolRegistry</code>","text":"<p>Registry for dynamically loading and storing tool instances.</p> Initialization is split into two phases <ol> <li>Construction: Stores provided configurations.</li> <li>Asynchronous initialization: Gathers tool infos (from local and, optionally, MCP sources),    registers tools, and logs summaries.</li> </ol> <p>Callers with MCP configurations must await <code>initialize_all_tools()</code> after construction.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"\n    Registry for dynamically loading and storing tool instances.\n\n    Initialization is split into two phases:\n      1. Construction: Stores provided configurations.\n      2. Asynchronous initialization: Gathers tool infos (from local and, optionally, MCP sources),\n         registers tools, and logs summaries.\n\n    Callers with MCP configurations must await `initialize_all_tools()` after construction.\n    \"\"\"\n\n    def __init__(self, tools_config: List[Any], mcp_config: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Args:\n            tools_config: A list of tool configurations for local tools.\n            mcp_config: A dictionary containing MCP configuration.\n        \"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.tools: Dict[str, BaseTool] = {}\n        self.hidden_tools: Dict[str, BaseTool] = {}\n        self.registration_results: Dict[str, List[RegistrationResult]] = {\n            REGISTRATION_SUCCESS: [],\n            REGISTRATION_HIDDEN_SUCCESS: [],\n            REGISTRATION_FAILED: []\n        }\n        self.tools_config = tools_config or []\n        self.mcp_config = mcp_config\n\n        # Synchronously gather local tool info.\n        self._local_tool_info: List[ToolInfo] = self._gather_local_tool_info()\n        self._mcp_tool_infos: List[ToolInfo] = []\n        self._lock = asyncio.Lock()  # thinking about mcp notifications, registering, initialization competing\n\n        if not self.mcp_config:\n            # In local-only mode, we expect the caller to await initialization.\n            self.logger.info(\"No MCP config provided. Call 'await initialize_all_tools()' to register local tools.\")\n\n    def _gather_local_tool_info(self) -&gt; List[ToolInfo]:\n        \"\"\"Collect local tool infos from tools_config.\"\"\"\n        infos = []\n        for cfg in self.tools_config:\n            info = {\n                \"name\": cfg.get(\"name\", \"&lt;unnamed&gt;\"),\n                \"source\": TOOL_SOURCE_LOCAL,\n                \"config\": cfg\n            }\n            infos.append(info)\n        return infos\n\n    async def _gather_mcp_tool_infos(self) -&gt; List[ToolInfo]:\n        \"\"\"Asynchronously gather tool infos from the MCP server.\"\"\"\n        infos = []\n        try:\n            from src.mcp.client import FlexoMCPClient\n\n            self.mcp_client = FlexoMCPClient(self.mcp_config)\n            await self.mcp_client.connect()\n            self.logger.info(\"MCP client connected successfully.\")\n\n            mcp_tools_result = await self.mcp_client.session.list_tools()\n            if mcp_tools_result and mcp_tools_result.tools:\n                for mcp_tool_def in mcp_tools_result.tools:\n                    info = {\n                        \"name\": mcp_tool_def.name,\n                        \"source\": TOOL_SOURCE_MCP,\n                        \"config\": mcp_tool_def\n                    }\n                    infos.append(info)\n                self.logger.info(f\"Gathered {len(infos)} MCP tool(s).\")\n            else:\n                self.logger.info(\"No MCP tools received from the MCP server.\")\n\n            # Subscribe to MCP tool updates.\n            if hasattr(self.mcp_client, \"observer\"):\n                self.mcp_client.observer.subscribe(self.update_tools_from_mcp)\n                self.logger.info(\"Subscribed to MCP tool update events.\")\n        except Exception as e:\n            self.logger.error(f\"Failed to gather MCP tools: {e}\", exc_info=True)\n        return infos\n\n    def _log_discovered_summary(self, local_count: int, mcp_count: int):\n        \"\"\"Log the discovered tools summary.\"\"\"\n        total = local_count + mcp_count\n        summary = \"\\n\" + \"=\" * 60 + \"\\n\"\n        summary += f\"DISCOVERED TOOLS SUMMARY: Total: {total} (Local: {local_count}, MCP: {mcp_count})\\n\"\n        summary += \"=\" * 60 + \"\\n\"\n        self.logger.info(summary)\n\n    async def _register_tools(self, tool_infos: List[ToolInfo]):\n        \"\"\"\n        Process the aggregated tool infos and register each tool.\n        This method clears previous registration results and registers each tool.\n        \"\"\"\n        async with self._lock:\n            self.registration_results = {\n                REGISTRATION_SUCCESS: [],\n                REGISTRATION_HIDDEN_SUCCESS: [],\n                REGISTRATION_FAILED: []\n            }\n        discovered_tools = discover_custom_tools()\n\n        # Import MCP tool adapter once.\n        from src.mcp.mcp_tool_adapter import convert_mcp_tool_to_flexo_tool\n\n        for info in tool_infos:\n            name = info.get(\"name\", \"&lt;unnamed&gt;\")\n            source = info.get(\"source\", TOOL_SOURCE_LOCAL)\n            try:\n                if source == TOOL_SOURCE_LOCAL:\n                    instance = create_tool_from_config(info[\"config\"], discovered_tools=discovered_tools)\n                elif source == TOOL_SOURCE_MCP:\n                    instance = convert_mcp_tool_to_flexo_tool(info[\"config\"])\n                else:\n                    raise ValueError(f\"Unknown tool source: {source}\")\n\n                await self.register_tool(name, instance, source=source)\n                self.logger.debug(f\"Successfully registered {source} tool: {name}\")\n            except Exception as e:\n                self.logger.error(f\"Failed to register tool '{name}' from {source}: {e}\", exc_info=True)\n                async with self._lock:\n                    self.registration_results[REGISTRATION_FAILED].append((name, str(e)))\n\n    async def register_tool(self, name: str, tool: BaseTool, source: str = TOOL_SOURCE_LOCAL, hidden: bool = False):\n        \"\"\"Register a tool instance with the registry, recording its source.\"\"\"\n        async with self._lock:\n            if name in self.tools or name in self.hidden_tools:\n                error_msg = f\"Tool registration error: Tool with name '{name}' is already registered.\"\n                self.logger.error(error_msg)\n                self.registration_results[REGISTRATION_FAILED].append((name, \"Already registered\"))\n                raise ValueError(error_msg)\n\n            if hidden:\n                self.hidden_tools[name] = tool\n                self.registration_results[REGISTRATION_HIDDEN_SUCCESS].append((name, source))\n                self.logger.debug(f\"Registered hidden tool: {name} from {source}\")\n            else:\n                self.tools[name] = tool\n                self.registration_results[REGISTRATION_SUCCESS].append((name, source))\n                self.logger.debug(f\"Registered tool: {name} from {source}\")\n\n    def _log_registration_summary(self):\n        \"\"\"Log the final registration summary (displays discovered and registered info).\"\"\"\n        # Reading shared state without a lock is acceptable here given our controlled update context.\n        succ_list: List[RegistrationResult] = self.registration_results.get(REGISTRATION_SUCCESS, [])\n        hidden_list: List[RegistrationResult] = self.registration_results.get(REGISTRATION_HIDDEN_SUCCESS, [])\n        failed_list: List[RegistrationResult] = self.registration_results.get(REGISTRATION_FAILED, [])\n\n        total = len(succ_list) + len(hidden_list) + len(failed_list)\n        summary = \"\\n\" + \"=\" * 60 + \"\\n\"\n        summary += f\"REGISTERED TOOLS SUMMARY ({total} TOTAL)\\n\"\n        summary += \"=\" * 60 + \"\\n\"\n        summary += f\"SUCCESS: {len(succ_list)} public tool(s)\"\n        if hidden_list:\n            summary += f\", {len(hidden_list)} hidden tool(s)\"\n        if failed_list:\n            summary += f\"\\nFAILED: {len(failed_list)} tool(s)\"\n        if succ_list:\n            summary += \"\\n\\nREGISTERED TOOLS:\"\n            for i, (name, source) in enumerate(sorted(succ_list, key=lambda x: x[0]), 1):\n                display_name = f\"{name} (MCP)\" if source == TOOL_SOURCE_MCP else name\n                summary += f\"\\n  {i}. {display_name}\"\n        if hidden_list:\n            summary += \"\\n\\nREGISTERED HIDDEN TOOLS:\"\n            for i, (name, source) in enumerate(sorted(hidden_list, key=lambda x: x[0]), 1):\n                display_name = f\"{name} (MCP)\" if source == TOOL_SOURCE_MCP else name\n                summary += f\"\\n  {i}. {display_name}\"\n        if failed_list:\n            summary += \"\\n\\nFAILED REGISTRATIONS:\"\n            for i, (name, err) in enumerate(failed_list, 1):\n                summary += f\"\\n  {i}. {name} - Error: {err}\"\n        summary += \"\\n\" + \"=\" * 60 + \"\\n\"\n        self.logger.info(summary)\n\n    async def initialize_all_tools(self):\n        \"\"\"\n        Asynchronously gather tool infos from both local and MCP sources,\n        register all tools, and log a single, combined summary.\n        \"\"\"\n        if self.mcp_config:\n            mcp_infos = await self._gather_mcp_tool_infos()\n            self._mcp_tool_infos = mcp_infos\n        else:\n            mcp_infos = []\n\n        all_infos = self._local_tool_info + mcp_infos\n        self.logger.info(\n            f\"Total tool infos gathered: {len(all_infos)} (Local: {len(self._local_tool_info)}, MCP: {len(mcp_infos)})\"\n        )\n        self._log_discovered_summary(local_count=len(self._local_tool_info), mcp_count=len(mcp_infos))\n        await self._register_tools(all_infos)\n        self._log_registration_summary()\n\n    async def get_tool(self, name: str) -&gt; Optional[BaseTool]:\n        \"\"\"Retrieve a registered tool by name.\"\"\"\n        async with self._lock:\n            return self.tools.get(name)\n\n    async def get_hidden_tool(self, name: str) -&gt; Optional[BaseTool]:\n        \"\"\"Retrieve a hidden tool by name.\"\"\"\n        async with self._lock:\n            return self.hidden_tools.get(name)\n\n    async def get_tool_definitions(\n            self,\n            allowed: Optional[List[str]] = None,\n            disallowed: Optional[List[str]] = None\n    ) -&gt; List[Tool]:\n        \"\"\"Get definitions for all registered non-hidden tools with optional filtering.\"\"\"\n        definitions = []\n        async with self._lock:\n            tools_copy = list(self.tools.items())\n        for tool_name, tool in tools_copy:\n            if allowed is not None and tool_name not in allowed:\n                continue\n            if disallowed is not None and tool_name in disallowed:\n                continue\n            try:\n                definitions.append(tool.get_definition())\n            except Exception as e:\n                self.logger.error(f\"Error getting definition for tool '{tool_name}': {e}\", exc_info=True)\n        return definitions\n\n    def update_tools_from_mcp(self, event: ToolUpdateEvent):\n        \"\"\"\n        Synchronous callback for MCP tool update events.\n        Defers handling to an asynchronous task.\n        \"\"\"\n        asyncio.create_task(self._handle_mcp_update(event))\n\n    async def _handle_mcp_update(self, event: ToolUpdateEvent):\n        \"\"\"\n        Asynchronously process MCP tool update events for additions, removals, and updates.\n        \"\"\"\n        self.logger.info(f\"Processing tool update event: {event}\")\n        from src.mcp.mcp_tool_adapter import convert_mcp_tool_to_flexo_tool\n\n        # Handle tool additions.\n        if event.new_tool_defs:\n            self.logger.info(f\"Processing {len(event.new_tool_defs)} new tools from MCP\")\n            for tool_def in event.new_tool_defs:\n                try:\n                    name = tool_def.name\n                    async with self._lock:\n                        if name in self.tools or name in self.hidden_tools:\n                            self.logger.warning(f\"Tool '{name}' already registered, skipping\")\n                            continue\n                    instance = convert_mcp_tool_to_flexo_tool(tool_def)\n                    await self.register_tool(name, instance, source=TOOL_SOURCE_MCP)\n                    self.logger.info(f\"Successfully registered new MCP tool: {name}\")\n                except Exception as e:\n                    self.logger.error(f\"Failed to register new MCP tool '{tool_def.name}': {e}\", exc_info=True)\n\n        # Handle tool removals.\n        if event.removed_tool_names:\n            self.logger.info(f\"Processing {len(event.removed_tool_names)} removed tools from MCP\")\n            async with self._lock:\n                for name in event.removed_tool_names:\n                    if name in self.tools:\n                        self.logger.info(f\"Removing MCP tool: {name}\")\n                        del self.tools[name]\n                    elif name in self.hidden_tools:\n                        self.logger.info(f\"Removing hidden MCP tool: {name}\")\n                        del self.hidden_tools[name]\n                    else:\n                        self.logger.warning(f\"Cannot remove unknown tool: {name}\")\n\n        # Handle tool updates.\n        if event.updated_tool_defs:\n            self.logger.info(f\"Processing {len(event.updated_tool_defs)} updated tools from MCP\")\n            for tool_def in event.updated_tool_defs:\n                try:\n                    name = tool_def.name\n                    instance = convert_mcp_tool_to_flexo_tool(tool_def)\n                    async with self._lock:\n                        if name in self.tools:\n                            self.tools[name] = instance\n                            self.logger.info(f\"Updated MCP tool: {name}\")\n                        elif name in self.hidden_tools:\n                            self.hidden_tools[name] = instance\n                            self.logger.info(f\"Updated hidden MCP tool: {name}\")\n                        else:\n                            self.logger.warning(f\"Cannot update unknown tool: {name}\")\n                except Exception as e:\n                    self.logger.error(f\"Failed to update MCP tool '{tool_def.name}': {e}\", exc_info=True)\n\n        # Log updated registration summary.\n        if event.new_tool_defs or event.removed_tool_names or event.updated_tool_defs:\n            self._log_registration_summary()\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.__init__","title":"<code>__init__(tools_config, mcp_config=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tools_config</code> <code>List[Any]</code> <p>A list of tool configurations for local tools.</p> required <code>mcp_config</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary containing MCP configuration.</p> <code>None</code> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>def __init__(self, tools_config: List[Any], mcp_config: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Args:\n        tools_config: A list of tool configurations for local tools.\n        mcp_config: A dictionary containing MCP configuration.\n    \"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self.tools: Dict[str, BaseTool] = {}\n    self.hidden_tools: Dict[str, BaseTool] = {}\n    self.registration_results: Dict[str, List[RegistrationResult]] = {\n        REGISTRATION_SUCCESS: [],\n        REGISTRATION_HIDDEN_SUCCESS: [],\n        REGISTRATION_FAILED: []\n    }\n    self.tools_config = tools_config or []\n    self.mcp_config = mcp_config\n\n    # Synchronously gather local tool info.\n    self._local_tool_info: List[ToolInfo] = self._gather_local_tool_info()\n    self._mcp_tool_infos: List[ToolInfo] = []\n    self._lock = asyncio.Lock()  # thinking about mcp notifications, registering, initialization competing\n\n    if not self.mcp_config:\n        # In local-only mode, we expect the caller to await initialization.\n        self.logger.info(\"No MCP config provided. Call 'await initialize_all_tools()' to register local tools.\")\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.get_hidden_tool","title":"<code>get_hidden_tool(name)</code>  <code>async</code>","text":"<p>Retrieve a hidden tool by name.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>async def get_hidden_tool(self, name: str) -&gt; Optional[BaseTool]:\n    \"\"\"Retrieve a hidden tool by name.\"\"\"\n    async with self._lock:\n        return self.hidden_tools.get(name)\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.get_tool","title":"<code>get_tool(name)</code>  <code>async</code>","text":"<p>Retrieve a registered tool by name.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>async def get_tool(self, name: str) -&gt; Optional[BaseTool]:\n    \"\"\"Retrieve a registered tool by name.\"\"\"\n    async with self._lock:\n        return self.tools.get(name)\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.get_tool_definitions","title":"<code>get_tool_definitions(allowed=None, disallowed=None)</code>  <code>async</code>","text":"<p>Get definitions for all registered non-hidden tools with optional filtering.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>async def get_tool_definitions(\n        self,\n        allowed: Optional[List[str]] = None,\n        disallowed: Optional[List[str]] = None\n) -&gt; List[Tool]:\n    \"\"\"Get definitions for all registered non-hidden tools with optional filtering.\"\"\"\n    definitions = []\n    async with self._lock:\n        tools_copy = list(self.tools.items())\n    for tool_name, tool in tools_copy:\n        if allowed is not None and tool_name not in allowed:\n            continue\n        if disallowed is not None and tool_name in disallowed:\n            continue\n        try:\n            definitions.append(tool.get_definition())\n        except Exception as e:\n            self.logger.error(f\"Error getting definition for tool '{tool_name}': {e}\", exc_info=True)\n    return definitions\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.initialize_all_tools","title":"<code>initialize_all_tools()</code>  <code>async</code>","text":"<p>Asynchronously gather tool infos from both local and MCP sources, register all tools, and log a single, combined summary.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>async def initialize_all_tools(self):\n    \"\"\"\n    Asynchronously gather tool infos from both local and MCP sources,\n    register all tools, and log a single, combined summary.\n    \"\"\"\n    if self.mcp_config:\n        mcp_infos = await self._gather_mcp_tool_infos()\n        self._mcp_tool_infos = mcp_infos\n    else:\n        mcp_infos = []\n\n    all_infos = self._local_tool_info + mcp_infos\n    self.logger.info(\n        f\"Total tool infos gathered: {len(all_infos)} (Local: {len(self._local_tool_info)}, MCP: {len(mcp_infos)})\"\n    )\n    self._log_discovered_summary(local_count=len(self._local_tool_info), mcp_count=len(mcp_infos))\n    await self._register_tools(all_infos)\n    self._log_registration_summary()\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.register_tool","title":"<code>register_tool(name, tool, source=TOOL_SOURCE_LOCAL, hidden=False)</code>  <code>async</code>","text":"<p>Register a tool instance with the registry, recording its source.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>async def register_tool(self, name: str, tool: BaseTool, source: str = TOOL_SOURCE_LOCAL, hidden: bool = False):\n    \"\"\"Register a tool instance with the registry, recording its source.\"\"\"\n    async with self._lock:\n        if name in self.tools or name in self.hidden_tools:\n            error_msg = f\"Tool registration error: Tool with name '{name}' is already registered.\"\n            self.logger.error(error_msg)\n            self.registration_results[REGISTRATION_FAILED].append((name, \"Already registered\"))\n            raise ValueError(error_msg)\n\n        if hidden:\n            self.hidden_tools[name] = tool\n            self.registration_results[REGISTRATION_HIDDEN_SUCCESS].append((name, source))\n            self.logger.debug(f\"Registered hidden tool: {name} from {source}\")\n        else:\n            self.tools[name] = tool\n            self.registration_results[REGISTRATION_SUCCESS].append((name, source))\n            self.logger.debug(f\"Registered tool: {name} from {source}\")\n</code></pre>"},{"location":"reference/tools/core/tool_registry/#src.tools.core.tool_registry.ToolRegistry.update_tools_from_mcp","title":"<code>update_tools_from_mcp(event)</code>","text":"<p>Synchronous callback for MCP tool update events. Defers handling to an asynchronous task.</p> Source code in <code>src/tools/core/tool_registry.py</code> <pre><code>def update_tools_from_mcp(self, event: ToolUpdateEvent):\n    \"\"\"\n    Synchronous callback for MCP tool update events.\n    Defers handling to an asynchronous task.\n    \"\"\"\n    asyncio.create_task(self._handle_mcp_update(event))\n</code></pre>"},{"location":"reference/tools/core/parsers/","title":"Tool Parsers Documentation","text":"<pre><code>graph TD\n    A[Tool Call] --&gt; B[BaseToolCallParser]\n    B --&gt; C[JSONToolCallParser]\n    B --&gt; D[NonJSONToolCallParser]\n    C --&gt; E[Parsed JSON Tool Call]\n    D --&gt; F[Parsed Non-JSON Tool Call]\n\n    style B stroke:#333,stroke-width:2px\n    style C stroke:#333,stroke-width:2px\n    style D stroke:#333,stroke-width:2px</code></pre>"},{"location":"reference/tools/core/parsers/#overview","title":"Overview","text":"<p>The parsers module provides a robust framework for parsing and validating tool calls in different formats. It ensures consistent handling of tool invocations regardless of the input format, making the system more flexible and maintainable.</p>"},{"location":"reference/tools/core/parsers/#key-components","title":"Key Components","text":""},{"location":"reference/tools/core/parsers/#basetoolcallparser","title":"BaseToolCallParser","text":"<p>The abstract base class that defines the core interface for all tool call parsers. It provides:</p> <ul> <li>Common validation logic</li> <li>Error handling patterns</li> <li>Standard parsing workflow</li> <li>Type checking utilities</li> </ul>"},{"location":"reference/tools/core/parsers/#jsontoolcallparser","title":"JSONToolCallParser","text":"<p>Specialized parser for handling JSON-formatted tool calls. Features include:</p> <ul> <li>Strict JSON schema validation</li> <li>Type coercion for parameters</li> <li>Nested object support</li> <li>Error messages for malformed JSON</li> <li>Parameter validation against tool specifications</li> </ul>"},{"location":"reference/tools/core/parsers/#nonjsontoolcallparser","title":"NonJSONToolCallParser","text":"<p>Parser implementation for handling tool calls in alternative formats. Capabilities include:</p> <ul> <li>Support for structured text formats</li> <li>Parameter extraction from non-JSON inputs</li> <li>Flexible format handling</li> <li>Conversion to standardized internal format</li> </ul>"},{"location":"reference/tools/core/parsers/#related-documentation","title":"Related Documentation","text":"<ul> <li>See base_tool_call_parser for the base parser interface</li> <li>See json_tool_call_parser for JSON parsing details</li> <li>See non_json_tool_call_parser for alternative format handling</li> </ul>"},{"location":"reference/tools/core/parsers/base_tool_call_parser/","title":"Base Parser","text":""},{"location":"reference/tools/core/parsers/base_tool_call_parser/#src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser","title":"<code>src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/tools/core/parsers/base_tool_call_parser.py</code> <pre><code>class BaseToolCallParser(ABC):\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"Abstract base class for parsing tool calls from text.\n\n        This class provides a framework for implementing tool call parsers\n        with common functionality for cleaning, validation, and error handling.\n\n        Attributes:\n            config (Dict[str, Any]): Configuration dictionary for parsing options.\n\n        Example:\n            ```python\n            class MyParser(BaseToolCallParser):\n                def extract(self, text: str) -&gt; Dict[str, Any]:\n                    # Implementation\n                    return {\"tool_calls\": [...]}\n\n            parser = MyParser({\"clean_tokens\": [\"&lt;START&gt;\", \"&lt;END&gt;\"]})\n            result = parser.parse(\"some text with tool calls\")\n            ```\n        \"\"\"\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n\n    def parse(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Main entry point for parsing tool calls from text.\n\n        Orchestrates the parsing process through cleaning, extraction,\n        and validation steps.\n\n        Args:\n            text (str): Raw input text containing tool calls.\n\n        Returns:\n            Dict[str, Any]: Parsed tool calls or error information.\n                Success format: {\"tool_calls\": [...]}\n                Error format: {\"error\": \"error message\"}\n        \"\"\"\n        cleaned_text = self.clean_text(text)\n        try:\n            extracted_data = self.extract(cleaned_text)\n            if \"tool_calls\" in extracted_data:\n                for tool_call in extracted_data[\"tool_calls\"]:\n                    if \"parameters\" in tool_call:\n                        tool_call[\"arguments\"] = tool_call.pop(\"parameters\")\n            if \"parameters\" in extracted_data:\n                extracted_data[\"arguments\"] = extracted_data.pop(\"parameters\")\n                print(f\"updated extracted_data: {extracted_data}\")\n            if \"error\" not in extracted_data:\n                if self.validate(extracted_data):\n                    return extracted_data\n            return extracted_data\n        except ValueError as e:\n            self.logger.error(f\"Validation error: {str(e)}\")\n            return {\"error\": str(e)}\n        except Exception as e:\n            self.logger.error(\"Unexpected error during parsing\", exc_info=True)\n            return {\"error\": f\"Unexpected error: {str(e)}\"}\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"Clean input text by removing specified tokens.\n\n        Args:\n            text (str): Raw input text to clean.\n\n        Returns:\n            str: Cleaned text with tokens removed and whitespace stripped.\n        \"\"\"\n        tokens = self.config.get(\"clean_tokens\", [])\n        for token in tokens:\n            text = text.replace(token, \"\")\n        return text.strip()\n\n    @staticmethod\n    def validate(data: Dict[str, Any]) -&gt; bool:\n        \"\"\"Validate the structure of extracted tool calls.\n\n        Args:\n            data (Dict[str, Any]): Extracted tool call data to validate.\n\n        Returns:\n            bool: True if validation passes.\n\n        Raises:\n            ValueError: If validation fails with specific reason.\n        \"\"\"\n        tool_calls = data.get(\"tool_calls\")\n        if not isinstance(tool_calls, list):\n            raise ValueError(\"Expected a list of tool calls\")\n\n        for call in tool_calls:\n            if not isinstance(call, dict):\n                raise ValueError(\"Each tool call must be a dictionary\")\n            if \"name\" not in call or \"arguments\" not in call:\n                raise ValueError(\"Each tool call must contain 'name' and ('arguments' or 'parameters') keys)\")\n\n        return True\n\n    @abstractmethod\n    def extract(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Extract tool calls from cleaned text.\n\n        Must be implemented by subclasses to define specific extraction logic.\n\n        Args:\n            text (str): Cleaned input text.\n\n        Returns:\n            Dict[str, Any]: Extracted tool calls or error information.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/tools/core/parsers/base_tool_call_parser/#src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser.__init__","title":"<code>__init__(config)</code>","text":"<p>Abstract base class for parsing tool calls from text.</p> <p>This class provides a framework for implementing tool call parsers with common functionality for cleaning, validation, and error handling.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict[str, Any]</code> <p>Configuration dictionary for parsing options.</p> Example <pre><code>class MyParser(BaseToolCallParser):\n    def extract(self, text: str) -&gt; Dict[str, Any]:\n        # Implementation\n        return {\"tool_calls\": [...]}\n\nparser = MyParser({\"clean_tokens\": [\"&lt;START&gt;\", \"&lt;END&gt;\"]})\nresult = parser.parse(\"some text with tool calls\")\n</code></pre> Source code in <code>src/tools/core/parsers/base_tool_call_parser.py</code> <pre><code>def __init__(self, config: Dict[str, Any]):\n    \"\"\"Abstract base class for parsing tool calls from text.\n\n    This class provides a framework for implementing tool call parsers\n    with common functionality for cleaning, validation, and error handling.\n\n    Attributes:\n        config (Dict[str, Any]): Configuration dictionary for parsing options.\n\n    Example:\n        ```python\n        class MyParser(BaseToolCallParser):\n            def extract(self, text: str) -&gt; Dict[str, Any]:\n                # Implementation\n                return {\"tool_calls\": [...]}\n\n        parser = MyParser({\"clean_tokens\": [\"&lt;START&gt;\", \"&lt;END&gt;\"]})\n        result = parser.parse(\"some text with tool calls\")\n        ```\n    \"\"\"\n    self.config = config\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/tools/core/parsers/base_tool_call_parser/#src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser.clean_text","title":"<code>clean_text(text)</code>","text":"<p>Clean input text by removing specified tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw input text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Cleaned text with tokens removed and whitespace stripped.</p> Source code in <code>src/tools/core/parsers/base_tool_call_parser.py</code> <pre><code>def clean_text(self, text: str) -&gt; str:\n    \"\"\"Clean input text by removing specified tokens.\n\n    Args:\n        text (str): Raw input text to clean.\n\n    Returns:\n        str: Cleaned text with tokens removed and whitespace stripped.\n    \"\"\"\n    tokens = self.config.get(\"clean_tokens\", [])\n    for token in tokens:\n        text = text.replace(token, \"\")\n    return text.strip()\n</code></pre>"},{"location":"reference/tools/core/parsers/base_tool_call_parser/#src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser.extract","title":"<code>extract(text)</code>  <code>abstractmethod</code>","text":"<p>Extract tool calls from cleaned text.</p> <p>Must be implemented by subclasses to define specific extraction logic.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Cleaned input text.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Extracted tool calls or error information.</p> Source code in <code>src/tools/core/parsers/base_tool_call_parser.py</code> <pre><code>@abstractmethod\ndef extract(self, text: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract tool calls from cleaned text.\n\n    Must be implemented by subclasses to define specific extraction logic.\n\n    Args:\n        text (str): Cleaned input text.\n\n    Returns:\n        Dict[str, Any]: Extracted tool calls or error information.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tools/core/parsers/base_tool_call_parser/#src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser.parse","title":"<code>parse(text)</code>","text":"<p>Main entry point for parsing tool calls from text.</p> <p>Orchestrates the parsing process through cleaning, extraction, and validation steps.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw input text containing tool calls.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Parsed tool calls or error information. Success format: {\"tool_calls\": [...]} Error format: {\"error\": \"error message\"}</p> Source code in <code>src/tools/core/parsers/base_tool_call_parser.py</code> <pre><code>def parse(self, text: str) -&gt; Dict[str, Any]:\n    \"\"\"Main entry point for parsing tool calls from text.\n\n    Orchestrates the parsing process through cleaning, extraction,\n    and validation steps.\n\n    Args:\n        text (str): Raw input text containing tool calls.\n\n    Returns:\n        Dict[str, Any]: Parsed tool calls or error information.\n            Success format: {\"tool_calls\": [...]}\n            Error format: {\"error\": \"error message\"}\n    \"\"\"\n    cleaned_text = self.clean_text(text)\n    try:\n        extracted_data = self.extract(cleaned_text)\n        if \"tool_calls\" in extracted_data:\n            for tool_call in extracted_data[\"tool_calls\"]:\n                if \"parameters\" in tool_call:\n                    tool_call[\"arguments\"] = tool_call.pop(\"parameters\")\n        if \"parameters\" in extracted_data:\n            extracted_data[\"arguments\"] = extracted_data.pop(\"parameters\")\n            print(f\"updated extracted_data: {extracted_data}\")\n        if \"error\" not in extracted_data:\n            if self.validate(extracted_data):\n                return extracted_data\n        return extracted_data\n    except ValueError as e:\n        self.logger.error(f\"Validation error: {str(e)}\")\n        return {\"error\": str(e)}\n    except Exception as e:\n        self.logger.error(\"Unexpected error during parsing\", exc_info=True)\n        return {\"error\": f\"Unexpected error: {str(e)}\"}\n</code></pre>"},{"location":"reference/tools/core/parsers/base_tool_call_parser/#src.tools.core.parsers.base_tool_call_parser.BaseToolCallParser.validate","title":"<code>validate(data)</code>  <code>staticmethod</code>","text":"<p>Validate the structure of extracted tool calls.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Extracted tool call data to validate.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if validation passes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If validation fails with specific reason.</p> Source code in <code>src/tools/core/parsers/base_tool_call_parser.py</code> <pre><code>@staticmethod\ndef validate(data: Dict[str, Any]) -&gt; bool:\n    \"\"\"Validate the structure of extracted tool calls.\n\n    Args:\n        data (Dict[str, Any]): Extracted tool call data to validate.\n\n    Returns:\n        bool: True if validation passes.\n\n    Raises:\n        ValueError: If validation fails with specific reason.\n    \"\"\"\n    tool_calls = data.get(\"tool_calls\")\n    if not isinstance(tool_calls, list):\n        raise ValueError(\"Expected a list of tool calls\")\n\n    for call in tool_calls:\n        if not isinstance(call, dict):\n            raise ValueError(\"Each tool call must be a dictionary\")\n        if \"name\" not in call or \"arguments\" not in call:\n            raise ValueError(\"Each tool call must contain 'name' and ('arguments' or 'parameters') keys)\")\n\n    return True\n</code></pre>"},{"location":"reference/tools/core/parsers/json_tool_call_parser/","title":"JSON Parser","text":""},{"location":"reference/tools/core/parsers/json_tool_call_parser/#src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser","title":"<code>src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser</code>","text":"<p>               Bases: <code>BaseToolCallParser</code></p> <p>Enhanced parser for extracting and processing JSON tool calls from raw text.</p> <p>This parser handles common LLM JSON generation errors including:</p> <ul> <li>Semicolons instead of commas between array items</li> <li>Missing or extra commas</li> <li>Unquoted keys</li> <li>Single quotes instead of double quotes</li> <li>Trailing commas</li> <li>Other common JSON syntax errors</li> </ul> Source code in <code>src/tools/core/parsers/json_tool_call_parser.py</code> <pre><code>class JSONToolCallParser(BaseToolCallParser):\n    \"\"\"Enhanced parser for extracting and processing JSON tool calls from raw text.\n\n    This parser handles common LLM JSON generation errors including:\n\n    - Semicolons instead of commas between array items\n    - Missing or extra commas\n    - Unquoted keys\n    - Single quotes instead of double quotes\n    - Trailing commas\n    - Other common JSON syntax errors\n    \"\"\"\n    # Precompiled regex patterns for common JSON errors\n    SEMICOLON_PATTERN = re.compile(r'(\\}|\\])\\s*;\\s*(\\{|\\[)')\n    TRAILING_COMMA_PATTERN = re.compile(r',\\s*(\\}|\\])')\n    MISSING_COMMA_PATTERN = re.compile(r'(\\}|\\])\\s*(\\{|\\[)')\n    UNQUOTED_PROPERTY_PATTERN = re.compile(r'([{,])\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*:')\n\n    def extract(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Extract and parse JSON tool calls from the input text with enhanced error recovery.\n\n        Args:\n            text (str): The input text containing JSON tool calls.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing parsed tool calls or error information.\n\n            - Success format: `{\"tool_calls\": [{\"name\": \"...\", \"arguments\": {...}}, ...]}`\n            - Error format: `{\"error\": \"error message\"}`\n        \"\"\"\n        try:\n            # Try to find JSON-like content\n            json_strings = self.find_json_content(text)\n            valid_calls = []\n\n            for json_str in json_strings:\n                try:\n                    # First attempt: Standard parsing with json5\n                    parsed = json5.loads(json_str)\n                    parsed = self.parse_nested_json(parsed)\n\n                    if isinstance(parsed, dict):\n                        valid_calls.append(parsed)\n                    elif isinstance(parsed, list):\n                        valid_calls.extend(parsed)\n                except Exception:\n                    # Second attempt: Apply preprocessing to fix common issues\n                    try:\n                        fixed_json = self.preprocess_json(json_str)\n                        parsed = json5.loads(fixed_json)\n                        parsed = self.parse_nested_json(parsed)\n\n                        if isinstance(parsed, dict):\n                            valid_calls.append(parsed)\n                        elif isinstance(parsed, list):\n                            valid_calls.extend(parsed)\n                    except Exception:\n                        # If all else fails, try a more aggressive approach for lists\n                        if json_str.startswith('[') and json_str.endswith(']'):\n                            items = self.split_json_list_items(json_str)\n                            for item in items:\n                                try:\n                                    parsed_item = json5.loads(item)\n                                    parsed_item = self.parse_nested_json(parsed_item)\n                                    valid_calls.append(parsed_item)\n                                except Exception:\n                                    continue\n\n            return {\"tool_calls\": valid_calls} if valid_calls else {\"error\": \"No valid tool calls found\"}\n        except Exception as e:\n            return {\"error\": f\"Unexpected error: {str(e)}\"}\n\n    @staticmethod\n    def find_json_content(text: str) -&gt; List[str]:\n        \"\"\"Extract potential JSON content from raw text using balanced delimiter matching.\n\n        This method scans the text character by character to identify and extract\n        valid JSON objects or arrays, handling nested structures correctly.\n\n        Args:\n            text (str): The raw text to search for JSON content.\n\n        Returns:\n            List[str]: A list of extracted JSON string segments.\n        \"\"\"\n        results = []\n        start = None\n        depth = 0\n        in_string = False\n        escape_next = False\n\n        for i, char in enumerate(text):\n            # Handle string literals correctly\n            if char == '\\\\' and not escape_next:\n                escape_next = True\n                continue\n\n            if char == '\"' and not escape_next:\n                in_string = not in_string\n\n            escape_next = False\n\n            # Only process delimiters when not inside a string\n            if not in_string:\n                if char in '{[' and depth == 0:\n                    start = i\n                    depth += 1\n                elif char in '{[':\n                    depth += 1\n                elif char in '}]':\n                    depth -= 1\n                    if depth == 0 and start is not None:\n                        results.append(text[start:i + 1])\n                        start = None\n\n        return results\n\n    def preprocess_json(self, json_str: str) -&gt; str:\n        \"\"\"Preprocess JSON string to fix common LLM-generated syntax errors.\n\n        Args:\n            json_str (str): The potentially malformed JSON string.\n\n        Returns:\n            str: A corrected JSON string.\n        \"\"\"\n        # Replace semicolons with commas between objects/arrays\n        json_str = self.SEMICOLON_PATTERN.sub(r'\\1,\\2', json_str)\n\n        # Fix trailing commas\n        json_str = self.TRAILING_COMMA_PATTERN.sub(r'\\1', json_str)\n\n        # Fix missing commas between objects/arrays\n        json_str = self.MISSING_COMMA_PATTERN.sub(r'\\1,\\2', json_str)\n\n        # Fix unquoted property names\n        json_str = self.UNQUOTED_PROPERTY_PATTERN.sub(r'\\1\"\\2\":', json_str)\n\n        return json_str\n\n    def split_json_list_items(self, json_list: str) -&gt; List[str]:\n        \"\"\"Split a JSON array string into individual item strings for separate parsing.\n\n        This handles cases where items are separated by semicolons or have other issues.\n\n        Args:\n            json_list (str): A string containing a JSON array with possibly invalid separators.\n\n        Returns:\n            List[str]: List of individual item strings.\n        \"\"\"\n        # Remove the outer brackets\n        content = json_list[1:-1].strip()\n\n        items = []\n        depth = 0\n        start = 0\n        in_string = False\n        escape_next = False\n\n        for i, char in enumerate(content):\n            # Handle string literals correctly\n            if char == '\\\\' and not escape_next:\n                escape_next = True\n                continue\n\n            if char == '\"' and not escape_next:\n                in_string = not in_string\n\n            escape_next = False\n\n            # Track nesting level\n            if not in_string:\n                if char in '{[':\n                    depth += 1\n                elif char in '}]':\n                    depth -= 1\n\n                # When at top level, check for separators (comma or semicolon)\n                if depth == 0 and char in ',;' and i &gt;= start:\n                    items.append(content[start:i].strip())\n                    start = i + 1\n\n        # Don't forget the last item\n        if start &lt; len(content):\n            items.append(content[start:].strip())\n\n        return items\n\n    def parse_nested_json(self, value: Any) -&gt; Any:\n        \"\"\"Recursively parses stringified JSON within a JSON structure.\n\n        Args:\n            value (Any): The input value to check and potentially parse.\n\n        Returns:\n            Any: The processed value, either as a parsed JSON object or as its original type.\n        \"\"\"\n        if isinstance(value, str):\n            trimmed = value.strip()\n            if trimmed and trimmed[0] in ['{', '[']:\n                try:\n                    # Try to fix common issues and then parse\n                    fixed_str = self.preprocess_json(trimmed)\n                    parsed = json5.loads(fixed_str)\n                    return self.parse_nested_json(parsed)\n                except Exception:\n                    # If that fails, return as is\n                    return value\n            else:\n                return value\n        elif isinstance(value, dict):\n            return {k: self.parse_nested_json(v) for k, v in value.items()}\n        elif isinstance(value, list):\n            return [self.parse_nested_json(item) for item in value]\n        else:\n            return value\n</code></pre>"},{"location":"reference/tools/core/parsers/json_tool_call_parser/#src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser.extract","title":"<code>extract(text)</code>","text":"<p>Extract and parse JSON tool calls from the input text with enhanced error recovery.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing JSON tool calls.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing parsed tool calls or error information.</p> <code>Dict[str, Any]</code> <ul> <li>Success format: <code>{\"tool_calls\": [{\"name\": \"...\", \"arguments\": {...}}, ...]}</code></li> </ul> <code>Dict[str, Any]</code> <ul> <li>Error format: <code>{\"error\": \"error message\"}</code></li> </ul> Source code in <code>src/tools/core/parsers/json_tool_call_parser.py</code> <pre><code>def extract(self, text: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract and parse JSON tool calls from the input text with enhanced error recovery.\n\n    Args:\n        text (str): The input text containing JSON tool calls.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing parsed tool calls or error information.\n\n        - Success format: `{\"tool_calls\": [{\"name\": \"...\", \"arguments\": {...}}, ...]}`\n        - Error format: `{\"error\": \"error message\"}`\n    \"\"\"\n    try:\n        # Try to find JSON-like content\n        json_strings = self.find_json_content(text)\n        valid_calls = []\n\n        for json_str in json_strings:\n            try:\n                # First attempt: Standard parsing with json5\n                parsed = json5.loads(json_str)\n                parsed = self.parse_nested_json(parsed)\n\n                if isinstance(parsed, dict):\n                    valid_calls.append(parsed)\n                elif isinstance(parsed, list):\n                    valid_calls.extend(parsed)\n            except Exception:\n                # Second attempt: Apply preprocessing to fix common issues\n                try:\n                    fixed_json = self.preprocess_json(json_str)\n                    parsed = json5.loads(fixed_json)\n                    parsed = self.parse_nested_json(parsed)\n\n                    if isinstance(parsed, dict):\n                        valid_calls.append(parsed)\n                    elif isinstance(parsed, list):\n                        valid_calls.extend(parsed)\n                except Exception:\n                    # If all else fails, try a more aggressive approach for lists\n                    if json_str.startswith('[') and json_str.endswith(']'):\n                        items = self.split_json_list_items(json_str)\n                        for item in items:\n                            try:\n                                parsed_item = json5.loads(item)\n                                parsed_item = self.parse_nested_json(parsed_item)\n                                valid_calls.append(parsed_item)\n                            except Exception:\n                                continue\n\n        return {\"tool_calls\": valid_calls} if valid_calls else {\"error\": \"No valid tool calls found\"}\n    except Exception as e:\n        return {\"error\": f\"Unexpected error: {str(e)}\"}\n</code></pre>"},{"location":"reference/tools/core/parsers/json_tool_call_parser/#src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser.find_json_content","title":"<code>find_json_content(text)</code>  <code>staticmethod</code>","text":"<p>Extract potential JSON content from raw text using balanced delimiter matching.</p> <p>This method scans the text character by character to identify and extract valid JSON objects or arrays, handling nested structures correctly.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The raw text to search for JSON content.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of extracted JSON string segments.</p> Source code in <code>src/tools/core/parsers/json_tool_call_parser.py</code> <pre><code>@staticmethod\ndef find_json_content(text: str) -&gt; List[str]:\n    \"\"\"Extract potential JSON content from raw text using balanced delimiter matching.\n\n    This method scans the text character by character to identify and extract\n    valid JSON objects or arrays, handling nested structures correctly.\n\n    Args:\n        text (str): The raw text to search for JSON content.\n\n    Returns:\n        List[str]: A list of extracted JSON string segments.\n    \"\"\"\n    results = []\n    start = None\n    depth = 0\n    in_string = False\n    escape_next = False\n\n    for i, char in enumerate(text):\n        # Handle string literals correctly\n        if char == '\\\\' and not escape_next:\n            escape_next = True\n            continue\n\n        if char == '\"' and not escape_next:\n            in_string = not in_string\n\n        escape_next = False\n\n        # Only process delimiters when not inside a string\n        if not in_string:\n            if char in '{[' and depth == 0:\n                start = i\n                depth += 1\n            elif char in '{[':\n                depth += 1\n            elif char in '}]':\n                depth -= 1\n                if depth == 0 and start is not None:\n                    results.append(text[start:i + 1])\n                    start = None\n\n    return results\n</code></pre>"},{"location":"reference/tools/core/parsers/json_tool_call_parser/#src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser.parse_nested_json","title":"<code>parse_nested_json(value)</code>","text":"<p>Recursively parses stringified JSON within a JSON structure.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value to check and potentially parse.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The processed value, either as a parsed JSON object or as its original type.</p> Source code in <code>src/tools/core/parsers/json_tool_call_parser.py</code> <pre><code>def parse_nested_json(self, value: Any) -&gt; Any:\n    \"\"\"Recursively parses stringified JSON within a JSON structure.\n\n    Args:\n        value (Any): The input value to check and potentially parse.\n\n    Returns:\n        Any: The processed value, either as a parsed JSON object or as its original type.\n    \"\"\"\n    if isinstance(value, str):\n        trimmed = value.strip()\n        if trimmed and trimmed[0] in ['{', '[']:\n            try:\n                # Try to fix common issues and then parse\n                fixed_str = self.preprocess_json(trimmed)\n                parsed = json5.loads(fixed_str)\n                return self.parse_nested_json(parsed)\n            except Exception:\n                # If that fails, return as is\n                return value\n        else:\n            return value\n    elif isinstance(value, dict):\n        return {k: self.parse_nested_json(v) for k, v in value.items()}\n    elif isinstance(value, list):\n        return [self.parse_nested_json(item) for item in value]\n    else:\n        return value\n</code></pre>"},{"location":"reference/tools/core/parsers/json_tool_call_parser/#src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser.preprocess_json","title":"<code>preprocess_json(json_str)</code>","text":"<p>Preprocess JSON string to fix common LLM-generated syntax errors.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The potentially malformed JSON string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A corrected JSON string.</p> Source code in <code>src/tools/core/parsers/json_tool_call_parser.py</code> <pre><code>def preprocess_json(self, json_str: str) -&gt; str:\n    \"\"\"Preprocess JSON string to fix common LLM-generated syntax errors.\n\n    Args:\n        json_str (str): The potentially malformed JSON string.\n\n    Returns:\n        str: A corrected JSON string.\n    \"\"\"\n    # Replace semicolons with commas between objects/arrays\n    json_str = self.SEMICOLON_PATTERN.sub(r'\\1,\\2', json_str)\n\n    # Fix trailing commas\n    json_str = self.TRAILING_COMMA_PATTERN.sub(r'\\1', json_str)\n\n    # Fix missing commas between objects/arrays\n    json_str = self.MISSING_COMMA_PATTERN.sub(r'\\1,\\2', json_str)\n\n    # Fix unquoted property names\n    json_str = self.UNQUOTED_PROPERTY_PATTERN.sub(r'\\1\"\\2\":', json_str)\n\n    return json_str\n</code></pre>"},{"location":"reference/tools/core/parsers/json_tool_call_parser/#src.tools.core.parsers.json_tool_call_parser.JSONToolCallParser.split_json_list_items","title":"<code>split_json_list_items(json_list)</code>","text":"<p>Split a JSON array string into individual item strings for separate parsing.</p> <p>This handles cases where items are separated by semicolons or have other issues.</p> <p>Parameters:</p> Name Type Description Default <code>json_list</code> <code>str</code> <p>A string containing a JSON array with possibly invalid separators.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of individual item strings.</p> Source code in <code>src/tools/core/parsers/json_tool_call_parser.py</code> <pre><code>def split_json_list_items(self, json_list: str) -&gt; List[str]:\n    \"\"\"Split a JSON array string into individual item strings for separate parsing.\n\n    This handles cases where items are separated by semicolons or have other issues.\n\n    Args:\n        json_list (str): A string containing a JSON array with possibly invalid separators.\n\n    Returns:\n        List[str]: List of individual item strings.\n    \"\"\"\n    # Remove the outer brackets\n    content = json_list[1:-1].strip()\n\n    items = []\n    depth = 0\n    start = 0\n    in_string = False\n    escape_next = False\n\n    for i, char in enumerate(content):\n        # Handle string literals correctly\n        if char == '\\\\' and not escape_next:\n            escape_next = True\n            continue\n\n        if char == '\"' and not escape_next:\n            in_string = not in_string\n\n        escape_next = False\n\n        # Track nesting level\n        if not in_string:\n            if char in '{[':\n                depth += 1\n            elif char in '}]':\n                depth -= 1\n\n            # When at top level, check for separators (comma or semicolon)\n            if depth == 0 and char in ',;' and i &gt;= start:\n                items.append(content[start:i].strip())\n                start = i + 1\n\n    # Don't forget the last item\n    if start &lt; len(content):\n        items.append(content[start:].strip())\n\n    return items\n</code></pre>"},{"location":"reference/tools/core/parsers/non_json_tool_call_parser/","title":"Non-JSON Parser","text":""},{"location":"reference/tools/core/parsers/non_json_tool_call_parser/#src.tools.core.parsers.non_json_tool_call_parser.NonJSONToolCallParser","title":"<code>src.tools.core.parsers.non_json_tool_call_parser.NonJSONToolCallParser</code>","text":"<p>               Bases: <code>BaseToolCallParser</code></p> <p>Parser for extracting non-JSON formatted tool calls from text.</p> <p>Specializes in parsing tool calls that use custom formats like {args}. Example <pre><code>config = {\n    \"formats\": {\n        \"non_json_format\": {\n            \"function_call_pattern\": r'&lt;function=(.*?)&gt;{(.*?)}&lt;/function&gt;'\n        }\n    }\n}\nparser = NonJSONToolCallParser(config)\nresult = parser.parse('&lt;function=my_tool&gt;{\"arg1\": \"value\"}&lt;/function&gt;')\n</code></pre> Source code in <code>src/tools/core/parsers/non_json_tool_call_parser.py</code> <pre><code>class NonJSONToolCallParser(BaseToolCallParser):\n    \"\"\"Parser for extracting non-JSON formatted tool calls from text.\n\n    Specializes in parsing tool calls that use custom formats like\n    &lt;function=name&gt;{args}&lt;/function&gt;.\n\n    Example:\n        ```python\n        config = {\n            \"formats\": {\n                \"non_json_format\": {\n                    \"function_call_pattern\": r'&lt;function=(.*?)&gt;{(.*?)}&lt;/function&gt;'\n                }\n            }\n        }\n        parser = NonJSONToolCallParser(config)\n        result = parser.parse('&lt;function=my_tool&gt;{\"arg1\": \"value\"}&lt;/function&gt;')\n        ```\n    \"\"\"\n    def extract(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Extract non-JSON tool calls using regex patterns.\n\n        Searches for tool calls using configured regex patterns and parses\n        their arguments as JSON.\n\n        Args:\n            text (str): Cleaned input text containing tool calls.\n\n        Returns:\n            Dict[str, Any]: Parsed tool calls or error information.\n\n                - Success format: {\"tool_calls\": [{\"name\": \"...\", \"arguments\": {...}}, ...]}\n                - Error format: {\"error\": \"error message\"}\n                - No tool calls: {\"content\": \"original text\"}\n        \"\"\"\n        pattern = self.config.get(\"formats\").get(\"non_json_format\").get(\"function_call_pattern\")\n        matches = re.findall(pattern, text)\n\n        if not matches:\n            return {\"content\": text}\n\n        tool_calls = []\n        for match in matches:\n            try:\n                tool_calls.append({\n                    \"name\": match[0],\n                    \"arguments\": json.loads(match[1])  # Parse arguments as JSON\n                })\n            except json.JSONDecodeError:\n                return {\"error\": f\"Failed to parse arguments for function: {match[0]}\"}\n\n        return {\"tool_calls\": tool_calls}\n</code></pre>"},{"location":"reference/tools/core/parsers/non_json_tool_call_parser/#src.tools.core.parsers.non_json_tool_call_parser.NonJSONToolCallParser.extract","title":"<code>extract(text)</code>","text":"<p>Extract non-JSON tool calls using regex patterns.</p> <p>Searches for tool calls using configured regex patterns and parses their arguments as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Cleaned input text containing tool calls.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Parsed tool calls or error information.</p> <ul> <li>Success format: {\"tool_calls\": [{\"name\": \"...\", \"arguments\": {...}}, ...]}</li> <li>Error format: {\"error\": \"error message\"}</li> <li>No tool calls: {\"content\": \"original text\"}</li> </ul> Source code in <code>src/tools/core/parsers/non_json_tool_call_parser.py</code> <pre><code>def extract(self, text: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract non-JSON tool calls using regex patterns.\n\n    Searches for tool calls using configured regex patterns and parses\n    their arguments as JSON.\n\n    Args:\n        text (str): Cleaned input text containing tool calls.\n\n    Returns:\n        Dict[str, Any]: Parsed tool calls or error information.\n\n            - Success format: {\"tool_calls\": [{\"name\": \"...\", \"arguments\": {...}}, ...]}\n            - Error format: {\"error\": \"error message\"}\n            - No tool calls: {\"content\": \"original text\"}\n    \"\"\"\n    pattern = self.config.get(\"formats\").get(\"non_json_format\").get(\"function_call_pattern\")\n    matches = re.findall(pattern, text)\n\n    if not matches:\n        return {\"content\": text}\n\n    tool_calls = []\n    for match in matches:\n        try:\n            tool_calls.append({\n                \"name\": match[0],\n                \"arguments\": json.loads(match[1])  # Parse arguments as JSON\n            })\n        except json.JSONDecodeError:\n            return {\"error\": f\"Failed to parse arguments for function: {match[0]}\"}\n\n    return {\"tool_calls\": tool_calls}\n</code></pre>"},{"location":"reference/tools/core/utils/","title":"Tools Utilities","text":""},{"location":"reference/tools/core/utils/#overview","title":"Overview","text":"<p>The Tools Utilities module provides essential support functions for the tools system, including:</p> <ul> <li>Token management and authentication</li> <li>Tool discovery and instantiation</li> <li>Configuration handling</li> </ul> <p>Example using token manager: <pre><code>graph LR\n    A[Tool Implementation] --&gt; B[Token Manager]\n    B --&gt; C[OAuth2 Flow]\n    C --&gt; D[External Service]\n\n    style B stroke:#333,stroke-width:2px\n    style C stroke:#333,stroke-width:2px</code></pre></p>"},{"location":"reference/tools/core/utils/#components","title":"Components","text":""},{"location":"reference/tools/core/utils/#token-management-token_managerpy","title":"Token Management (<code>token_manager.py</code>)","text":"<p>Handles OAuth2 authentication and token lifecycle:</p> <ul> <li>Client credentials flow for secure API access</li> <li>Automatic token refresh before expiration</li> <li>Thread-safe operations with asyncio</li> <li>Comprehensive error handling and logging</li> </ul>"},{"location":"reference/tools/core/utils/#tool-builder-tool_builderpy","title":"Tool Builder (<code>tool_builder.py</code>)","text":"<p>Creates tool instances from configuration:</p> <ul> <li>Supports both custom and config-based tools</li> <li>Dynamic class loading and instantiation</li> <li>Configuration validation and error handling</li> </ul>"},{"location":"reference/tools/core/utils/#tool-discovery-tool_discoverypy","title":"Tool Discovery (<code>tool_discovery.py</code>)","text":"<p>Automatically discovers available tool implementations:</p> <ul> <li>Scans the implementations package for tool classes</li> <li>Maps tool names to their implementing classes</li> <li>Provides a registry of available tools to the factory</li> </ul>"},{"location":"reference/tools/core/utils/token_manager/","title":"Token Manager","text":""},{"location":"reference/tools/core/utils/token_manager/#src.tools.core.utils.token_manager.OAuth2ClientCredentialsManager","title":"<code>src.tools.core.utils.token_manager.OAuth2ClientCredentialsManager</code>","text":"<p>Manages OAuth2 token lifecycle including acquisition and refresh.</p> <p>Handles token expiration and thread-safe token refresh using asyncio locks. Implements proper logging for debugging and monitoring token lifecycle events.</p> Source code in <code>src/tools/core/utils/token_manager.py</code> <pre><code>class OAuth2ClientCredentialsManager:\n    \"\"\"\n    Manages OAuth2 token lifecycle including acquisition and refresh.\n\n    Handles token expiration and thread-safe token refresh using asyncio locks.\n    Implements proper logging for debugging and monitoring token lifecycle events.\n    \"\"\"\n\n    def __init__(\n            self,\n            api_key: str,\n            client_secret_base64: str,\n            token_url: str,\n            refresh_buffer: int = 60,\n            logger: Optional[logging.Logger] = None\n    ) -&gt; None:\n        \"\"\"\n        Initialize the TokenManager.\n\n        Args:\n            api_key: API key for authentication\n            client_secret_base64: Base64 encoded client secret\n            token_url: OAuth2 token endpoint URL\n            refresh_buffer: Seconds before expiry to trigger refresh\n            logger: Optional custom logger instance\n\n        Raises:\n            ValueError: If required parameters are missing or invalid\n        \"\"\"\n        # Validate inputs\n        if not all([api_key, client_secret_base64, token_url]):\n            raise ValueError(\"api_key, client_secret_base64, and token_url are required\")\n        if not token_url.startswith(('http://', 'https://')):\n            raise ValueError(\"token_url must be a valid HTTP(S) URL\")\n        if refresh_buffer &lt; 0:\n            raise ValueError(\"refresh_buffer must be non-negative\")\n\n        self.api_key = api_key\n        self.client_secret_base64 = client_secret_base64\n        self.token_url = token_url\n        self.refresh_buffer = refresh_buffer\n\n        # Token state\n        self.access_token: Optional[str] = None\n        self.expiry_time: float = 0\n        self.lock = asyncio.Lock()\n\n        # Set up logging\n        self.logger = logger or logging.getLogger(__name__)\n\n    async def _is_token_expired(self) -&gt; bool:\n        \"\"\"\n        Check if the current token is expired or close to expiration.\n\n        Returns:\n            bool: True if token is expired or close to expiry, False otherwise\n        \"\"\"\n        current_time = time.time()\n        token_expired = (\n                self.access_token is None or\n                current_time &gt; (self.expiry_time - self.refresh_buffer)\n        )\n\n        if token_expired:\n            self.logger.debug(\n                \"Token status: expired or near expiry. \"\n                f\"Current time: {current_time}, Expiry time: {self.expiry_time}\"\n            )\n        return token_expired\n\n    async def _refresh_token(self) -&gt; None:\n        \"\"\"\n        Refresh the OAuth token by making an async API request.\n\n        Raises:\n            aiohttp.ClientError: If network request fails\n            ValueError: If authentication fails\n            Exception: For unexpected errors during token refresh\n        \"\"\"\n        async with self.lock:\n            # Double-check expiration after acquiring lock\n            if not await self._is_token_expired():\n                self.logger.debug(\"Token was refreshed by another task\")\n                return\n\n            try:\n                headers = {\n                    \"Content-Type\": \"application/x-www-form-urlencoded\",\n                    \"apikey\": self.api_key,\n                    \"Authorization\": f\"Basic {self.client_secret_base64}\"\n                }\n                payload = {\n                    \"grant_type\": \"client_credentials\",\n                    \"scope\": \"public\"\n                }\n\n                self.logger.debug(f\"Attempting to refresh token from {self.token_url}\")\n                async with aiohttp.ClientSession() as session:\n                    async with session.post(\n                            self.token_url,\n                            headers=headers,\n                            data=payload\n                    ) as response:\n                        if response.status == 401:\n                            self.logger.error(\n                                \"Authentication failed during token refresh. \"\n                                \"Check credentials.\"\n                            )\n                            raise ValueError(\"Authentication failed\")\n\n                        response.raise_for_status()\n                        token_info: Dict[str, Any] = await response.json()\n\n                        self.access_token = token_info[\"access_token\"]\n                        expires_in = int(token_info[\"expires_in\"])\n                        self.expiry_time = time.time() + expires_in\n\n                        self.logger.info(f\"Token refreshed successfully. Expires in {expires_in} seconds.\")\n\n            except aiohttp.ClientError as e:\n                self.logger.error(f\"Network error during token refresh: {str(e)}\")\n                self.access_token = None\n                raise\n            except Exception as e:\n                self.logger.error(f\"Unexpected error during token refresh: {str(e)}\")\n                self.access_token = None\n                raise\n\n    async def get_token(self) -&gt; Optional[str]:\n        \"\"\"\n        Get the current access token, refreshing if necessary.\n\n        Returns:\n            str: The current access token if valid\n            None: If token refresh fails\n\n        Raises:\n            May raise exceptions from _refresh_token() if refresh fails\n        \"\"\"\n        if await self._is_token_expired():\n            self.logger.debug(\"Token expired, initiating refresh\")\n            await self._refresh_token()\n        return self.access_token\n</code></pre>"},{"location":"reference/tools/core/utils/token_manager/#src.tools.core.utils.token_manager.OAuth2ClientCredentialsManager.__init__","title":"<code>__init__(api_key, client_secret_base64, token_url, refresh_buffer=60, logger=None)</code>","text":"<p>Initialize the TokenManager.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication</p> required <code>client_secret_base64</code> <code>str</code> <p>Base64 encoded client secret</p> required <code>token_url</code> <code>str</code> <p>OAuth2 token endpoint URL</p> required <code>refresh_buffer</code> <code>int</code> <p>Seconds before expiry to trigger refresh</p> <code>60</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing or invalid</p> Source code in <code>src/tools/core/utils/token_manager.py</code> <pre><code>def __init__(\n        self,\n        api_key: str,\n        client_secret_base64: str,\n        token_url: str,\n        refresh_buffer: int = 60,\n        logger: Optional[logging.Logger] = None\n) -&gt; None:\n    \"\"\"\n    Initialize the TokenManager.\n\n    Args:\n        api_key: API key for authentication\n        client_secret_base64: Base64 encoded client secret\n        token_url: OAuth2 token endpoint URL\n        refresh_buffer: Seconds before expiry to trigger refresh\n        logger: Optional custom logger instance\n\n    Raises:\n        ValueError: If required parameters are missing or invalid\n    \"\"\"\n    # Validate inputs\n    if not all([api_key, client_secret_base64, token_url]):\n        raise ValueError(\"api_key, client_secret_base64, and token_url are required\")\n    if not token_url.startswith(('http://', 'https://')):\n        raise ValueError(\"token_url must be a valid HTTP(S) URL\")\n    if refresh_buffer &lt; 0:\n        raise ValueError(\"refresh_buffer must be non-negative\")\n\n    self.api_key = api_key\n    self.client_secret_base64 = client_secret_base64\n    self.token_url = token_url\n    self.refresh_buffer = refresh_buffer\n\n    # Token state\n    self.access_token: Optional[str] = None\n    self.expiry_time: float = 0\n    self.lock = asyncio.Lock()\n\n    # Set up logging\n    self.logger = logger or logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/tools/core/utils/token_manager/#src.tools.core.utils.token_manager.OAuth2ClientCredentialsManager.get_token","title":"<code>get_token()</code>  <code>async</code>","text":"<p>Get the current access token, refreshing if necessary.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The current access token if valid</p> <code>None</code> <code>Optional[str]</code> <p>If token refresh fails</p> Source code in <code>src/tools/core/utils/token_manager.py</code> <pre><code>async def get_token(self) -&gt; Optional[str]:\n    \"\"\"\n    Get the current access token, refreshing if necessary.\n\n    Returns:\n        str: The current access token if valid\n        None: If token refresh fails\n\n    Raises:\n        May raise exceptions from _refresh_token() if refresh fails\n    \"\"\"\n    if await self._is_token_expired():\n        self.logger.debug(\"Token expired, initiating refresh\")\n        await self._refresh_token()\n    return self.access_token\n</code></pre>"},{"location":"reference/tools/core/utils/tool_builder/","title":"Tool Builder","text":""},{"location":"reference/tools/core/utils/tool_builder/#src.tools.core.utils.tool_builder.create_tool_from_config","title":"<code>src.tools.core.utils.tool_builder.create_tool_from_config(tool_def, discovered_tools=None)</code>","text":"<p>Create and return an instance of a tool based on its configuration.</p> <p>Supports two modes:</p> <ol> <li>Custom tools: Directly instantiate by name from discovered tools</li> <li>Config-based tools: Create based on base_tool + configuration</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tool_def</code> <code>dict</code> <p>A dictionary containing tool configuration. Expected keys:</p> <ul> <li>\"name\": Unique identifier for this tool instance.</li> <li>Optional: \"base_tool\": The key to determine the underlying tool type.</li> <li>Optional: \"class\": Explicit class name to use.</li> <li>Plus any other tool-specific settings.</li> </ul> required <code>discovered_tools</code> <code>Optional[Dict[str, Type[BaseTool]]]</code> <p>A mapping from tool names to tool classes, typically provided by discover_custom_tools().</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BaseTool</code> <code>BaseTool</code> <p>An instantiated tool configured as specified.</p> Source code in <code>src/tools/core/utils/tool_builder.py</code> <pre><code>def create_tool_from_config(\n        tool_def: dict,\n        discovered_tools: Optional[Dict[str, Type[BaseTool]]] = None\n) -&gt; BaseTool:\n    \"\"\"\n    Create and return an instance of a tool based on its configuration.\n\n    Supports two modes:\n\n    1. Custom tools: Directly instantiate by name from discovered tools\n    2. Config-based tools: Create based on base_tool + configuration\n\n    Parameters:\n        tool_def (dict): A dictionary containing tool configuration. Expected keys:\n\n            - \"name\": Unique identifier for this tool instance.\n            - Optional: \"base_tool\": The key to determine the underlying tool type.\n            - Optional: \"class\": Explicit class name to use.\n            - Plus any other tool-specific settings.\n        discovered_tools (Optional[Dict[str, Type[BaseTool]]]):\n            A mapping from tool names to tool classes, typically provided by discover_custom_tools().\n\n    Returns:\n        BaseTool: An instantiated tool configured as specified.\n    \"\"\"\n    tool_name = tool_def[\"name\"]\n    logger.debug(f\"Creating tool instance for '{tool_name}' with config: {tool_def}\")\n\n    # Check if this is a custom tool that's already been discovered\n    if discovered_tools is not None and tool_name in discovered_tools:\n        tool_class = discovered_tools[tool_name]\n        logger.debug(f\"Found discovered tool class: {tool_class.__name__} for key '{tool_name}'\")\n\n    # Otherwise, look for base_tool for configurable tools\n    elif \"base_tool\" in tool_def:\n        base_tool_key = tool_def[\"base_tool\"]\n\n        if discovered_tools is not None and base_tool_key in discovered_tools:\n            tool_class = discovered_tools[base_tool_key]\n            logger.debug(f\"Found discovered base tool class: {tool_class.__name__} for key '{base_tool_key}'\")\n        else:\n            try:\n                module = importlib.import_module(f\"src.tools.implementations.{base_tool_key}\")\n                class_name = tool_def.get(\"class\") or \"\".join(\n                    part.capitalize() for part in base_tool_key.split(\"_\")) + \"Tool\"\n                tool_class = getattr(module, class_name)\n                logger.debug(\n                    f\"Loaded tool class via dynamic import: {tool_class.__name__} from module src.tools.implementations.{base_tool_key}\")\n            except Exception as e:\n                logger.error(f\"Error importing tool '{base_tool_key}': {e}\")\n                raise\n    else:\n        raise ValueError(f\"Tool '{tool_name}' is not a discovered custom tool and has no 'base_tool' defined\")\n\n    try:\n        instance = tool_class(config=tool_def)\n        logger.debug(f\"Created instance of {tool_class.__name__} for tool '{tool_def.get('name')}'\")\n        return instance\n    except Exception as e:\n        logger.error(f\"Error instantiating tool '{tool_def.get('name')}' with class {tool_class.__name__}: {e}\")\n        raise\n</code></pre>"},{"location":"reference/tools/core/utils/tool_discovery/","title":"Tool Discovery","text":""},{"location":"reference/tools/core/utils/tool_discovery/#src.tools.core.utils.tool_discovery.discover_custom_tools","title":"<code>src.tools.core.utils.tool_discovery.discover_custom_tools()</code>","text":"<p>Scans the tools.implementations package to find all tool classes that:</p> <ul> <li>Inherit from BaseTool (excluding BaseTool itself)</li> <li>Have a class attribute <code>name</code> that uniquely identifies them.</li> </ul> <p>Returns:</p> Type Description <code>Dict[str, Type[BaseTool]]</code> <p>A dictionary mapping tool_name (str) to the tool class.</p> Source code in <code>src/tools/core/utils/tool_discovery.py</code> <pre><code>def discover_custom_tools() -&gt; Dict[str, Type[BaseTool]]:\n    \"\"\"\n    Scans the tools.implementations package to find all tool classes that:\n\n      - Inherit from BaseTool (excluding BaseTool itself)\n      - Have a class attribute `name` that uniquely identifies them.\n\n    Returns:\n      A dictionary mapping tool_name (str) to the tool class.\n    \"\"\"\n    discovered = {}\n    try:\n        import src.tools.implementations  # Ensure the package is imported\n        logger.debug(\"Scanning tools.implementations for custom tools.\")\n    except Exception as e:\n        logger.error(f\"Failed to import tools.implementations package: {e}\")\n        return discovered\n\n    for finder, module_name, ispkg in pkgutil.iter_modules(src.tools.implementations.__path__):\n        try:\n            module = importlib.import_module(f\"src.tools.implementations.{module_name}\")\n            logger.debug(f\"Imported module: src.tools.implementations.{module_name}\")\n        except Exception as e:\n            logger.error(f\"Failed to import module {module_name}: {e}\")\n            continue\n\n        for attr_name in dir(module):\n            attr = getattr(module, attr_name)\n            if (\n                    inspect.isclass(attr)\n                    and issubclass(attr, BaseTool)\n                    and attr is not BaseTool\n                    and hasattr(attr, \"name\")\n            ):\n                tool_key = getattr(attr, \"name\")\n                discovered[tool_key] = attr\n                logger.debug(f\"Discovered tool: '{tool_key}' -&gt; {attr.__name__}\")\n\n    # Build a single log message for the summary\n    if discovered:\n        discovered_count = len(discovered)\n        tool_names = sorted(discovered.keys())\n\n        log_message = \"\\n\" + \"-\" * 50\n        log_message += f\"\\nTOOL DISCOVERY: Found {discovered_count} custom tool(s)\"\n        log_message += \"\\n\" + \"-\" * 50\n\n        # List discovered tools with their class names\n        for i, tool_name in enumerate(tool_names, 1):\n            tool_class = discovered[tool_name].__name__\n            log_message += f\"\\n  {i}. {tool_name:&lt;20} -&gt; {tool_class}\"\n\n        log_message += f\"\\n{'-' * 50}\"\n    else:\n        log_message = \"\\n\" + \"-\" * 50\n        log_message += \"\\nTOOL DISCOVERY: No custom tools found\"\n        log_message += f\"\\n{'-' * 50}\"\n\n    logger.debug(log_message)\n\n    return discovered\n</code></pre>"},{"location":"reference/tools/implementations/","title":"Tool Implementations","text":""},{"location":"reference/tools/implementations/#overview","title":"Overview","text":"<p>The implementations directory contains concrete tool implementations that extend the base tool classes to provide specific functionalities. Each tool is designed to handle particular use cases while adhering to the common interfaces defined in the base classes.</p> <pre><code>graph LR\n    A[BaseTool] --&gt; B[RAGTool]\n    A --&gt; F[BaseRESTTool]\n    F --&gt; C[WeatherTool]\n    F --&gt; G[WikipediaTool]\n    F --&gt; I[DuckDuckGoSearchTool]\n\n    subgraph Implementations\n        B\n        C\n        G\n        I\n    end\n\n    B --&gt; D[Knowledge Base]\n    C --&gt; E[Weather API]\n    G --&gt; H[Wikipedia API]\n    I --&gt; J[DuckDuckGo API]\n\n    style A stroke:#333,stroke-width:2px\n    style F stroke:#333,stroke-width:2px\n    style B stroke:#333,stroke-width:2px\n    style C stroke:#333,stroke-width:2px\n    style D stroke:#333,stroke-width:2px\n    style E stroke:#333,stroke-width:2px\n    style G stroke:#333,stroke-width:2px\n    style H stroke:#333,stroke-width:2px\n    style I stroke:#333,stroke-width:2px\n    style J stroke:#333,stroke-width:2px</code></pre>"},{"location":"reference/tools/implementations/#provided-example-tools","title":"Provided Example Tools","text":""},{"location":"reference/tools/implementations/#rag-tool","title":"RAG Tool","text":"<p>The RAG (Retrieval-Augmented Generation) tool enables context-aware responses by incorporating information from external knowledge bases.</p> <p>Key Features:</p> <ul> <li>Dynamic knowledge retrieval</li> <li>Contextual response generation</li> <li>Configurable similarity thresholds</li> <li>Support for multiple knowledge base formats</li> </ul>"},{"location":"reference/tools/implementations/#weather-tool","title":"Weather Tool","text":"<p>The Weather tool provides access to weather information through integration with weather service APIs.</p> <p>Key Features:</p> <ul> <li>Current weather conditions</li> <li>Weather forecasts</li> <li>Location-based queries</li> <li>Multiple unit support (metric/imperial)</li> </ul>"},{"location":"reference/tools/implementations/#wikipedia-tool","title":"Wikipedia Tool","text":"<p>The Wikipedia tool enables direct access to Wikipedia's vast knowledge base through its API integration.</p> <p>Key Features:</p> <ul> <li>Article retrieval and summarization</li> <li>Cross-reference link extraction</li> <li>Multi-language support</li> <li>Search functionality with relevance ranking</li> </ul>"},{"location":"reference/tools/implementations/#duckduckgo-search-tool","title":"DuckDuckGo Search Tool","text":"<p>The DuckDuckGo Search Tool provides web search capabilities through DuckDuckGo's search engine.</p> <p>Key Features:</p> <ul> <li>Web search for up-to-date information</li> <li>Result extraction with titles, snippets, and URLs</li> <li>Markdown-formatted search results</li> <li>Configurable result limit</li> <li>Robust error handling</li> </ul>"},{"location":"reference/tools/implementations/#implementation-guidelines","title":"Implementation Guidelines","text":"<p>When creating new tool implementations, follow these guidelines:</p> <ol> <li> <p>Inheritance</p> <ul> <li>Extend either <code>BaseTool</code> or <code>BaseRESTTool</code></li> <li>Implement all required abstract methods</li> <li>Call <code>super().__init__()</code> in constructor</li> </ul> </li> <li> <p>Stream Context Integration: StreamContext enables tools to access and interact with the conversation session state. It provides:</p> <ul> <li>Access to conversation history and message buffers for context-aware processing and responses</li> <li>Tool definitions and current tool calls to coordinate complex multistep operations</li> <li>Session metadata and configuration for customized tool behavior</li> <li>LLM factory methods for dynamic model access and generation </li> <li>For detailed documentation, see Agent Data Models.</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Use appropriate exception classes</li> <li>Provide meaningful error messages</li> <li>Handle API-specific error cases</li> <li>Include logging</li> </ul> </li> <li> <p>Configuration</p> <ul> <li>Use environment variables for sensitive data</li> <li>Make tool behavior configurable</li> <li>Document all configuration options</li> </ul> </li> <li> <p>Response Format</p> <ul> <li>Return structured data</li> <li>Follow consistent response patterns</li> <li>Include status indicators</li> <li>Provide error details when needed</li> </ul> </li> </ol>"},{"location":"reference/tools/implementations/#creating-new-tools","title":"Creating New Tools","text":""},{"location":"reference/tools/implementations/#tool-configuration-example","title":"Tool Configuration Example","text":"<p>Below is an example of how tools are configured in the agent.yaml file:</p> <pre><code>tools_config:\n  # Weather API Integration\n  - name: \"weather\"\n    endpoint_url: \"https://api.openweathermap.org/data/2.5/weather\"\n    api_key_env: \"OWM_API_KEY\"\n\n  # Wikipedia Summary Tool\n  - name: \"wikipedia\"\n    endpoint_url: \"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{encoded_query}\"\n\n  # DuckDuckGo Search Tool\n  - name: \"duckduckgo_search\"\n\n  # RAG Tool with Elasticsearch\n  - name: \"medicare_search\"\n    connector_config:\n      connector_type: elasticsearch\n      index_name: my_index\n      api_key_env: ES_API_KEY\n      endpoint_env: ES_ENDPOINT\n      top_k: 5\n      # Additional configuration options...\n</code></pre> <p>The configuration specifies which tools should be registered and provides any necessary configuration parameters for each tool.</p>"},{"location":"reference/tools/implementations/#registration-process","title":"Registration Process","text":"<p>Tools are registered through the Flexo <code>agent.yaml</code> configuration file. Here's the step-by-step guide:</p> <ol> <li> <p>Create Tool Class File</p> <ul> <li>Create a new Python file in the <code>implementations/</code> directory</li> <li>Import required base classes:    <pre><code>from src.tools.core.base_tool import BaseTool  # or BaseRESTTool\n</code></pre></li> </ul> </li> <li> <p>Define Your Tool Class</p> <ul> <li>Create a class that extends the appropriate base class</li> <li>Define a unique <code>name</code> class attribute (this must match the tool name in your config)    <pre><code>class MyTool(BaseTool):\n    name = \"my_tool\"  # Required: unique identifier for the tool\n\n    def __init__(self, config: Optional[Dict] = None):\n        super().__init__(config=config)\n        self.description = 'Description of what your tool does'\n</code></pre></li> </ul> </li> <li> <p>Add Tool Configuration to agent.yaml</p> <ul> <li>Add your tool configuration under the <code>tools_config</code> section:    <pre><code>tools_config:\n  # Other tools...\n  - name: \"my_tool\"\n    # Optional tool-specific configuration\n    endpoint_url: \"https://api.example.com/v1/endpoint\"\n    api_key_env: \"MY_TOOL_API_KEY\"\n</code></pre></li> </ul> </li> <li> <p>Implementation Requirements</p> <ul> <li>Implement all required abstract methods from the base class</li> <li>Add error handling and logging</li> <li>Include documentation</li> <li>Write unit tests (recommended)</li> </ul> </li> </ol>"},{"location":"reference/tools/implementations/#example-implementation","title":"Example Implementation","text":"<p>Here's an example using the Weather Tool:</p>"},{"location":"reference/tools/implementations/#python-implementation","title":"Python Implementation","text":"<pre><code>from typing import Optional, Dict\nfrom src.tools.core.base_rest_tool import BaseRESTTool\n\nclass WeatherTool(BaseRESTTool):\n    name = \"weather\"  # Must match the name in tools_config\n\n    def __init__(self, config: Optional[Dict] = None):\n        super().__init__(config=config)\n        self.description = 'Get current weather information for a location'\n        self.strict = False\n\n    # Implement required methods...\n</code></pre>"},{"location":"reference/tools/implementations/#configuration-in-agentyaml","title":"Configuration in agent.yaml","text":"<pre><code>tools_config:\n  - name: \"weather\"\n    endpoint_url: \"https://api.openweathermap.org/data/2.5/weather\"\n    api_key_env: \"OWM_API_KEY\"\n</code></pre>"},{"location":"reference/tools/implementations/#configuration-options","title":"Configuration Options","text":"<p>The tools configuration in agent.yaml supports various options depending on the tool type:</p> <ul> <li>Basic API Tools: Configure endpoints, API keys, and other connection parameters</li> <li>RAG Tools: Specify connector configurations, vector databases, and search parameters  </li> <li>Hidden Tools: Tools can be made available only to the system by excluding them from the <code>tools_config</code></li> </ul>"},{"location":"reference/tools/implementations/#implementation-checklist","title":"Implementation Checklist","text":"<ol> <li>\u2705 Create new file in implementations directory</li> <li>\u2705 Import appropriate base class</li> <li>\u2705 Define unique name class attribute (must match configuration)</li> <li>\u2705 Implement required methods</li> <li>\u2705 Add documentation</li> <li>\u2705 Add tool to your <code>tools_config</code> list in <code>agent.yaml</code></li> <li>\u2705 Write unit tests (recommended)</li> </ol>"},{"location":"reference/tools/implementations/#quick-links","title":"Quick Links:","text":"<ul> <li>See Example RAG Tool (medicare handbook)</li> <li>See Example API tool (weather)</li> <li>See Example API tool (wikipedia)</li> <li>See Example API tool (duckduckgo search)</li> </ul>"},{"location":"reference/tools/implementations/duckduckgo_tool/","title":"DuckDuckGo Tool","text":""},{"location":"reference/tools/implementations/duckduckgo_tool/#src.tools.implementations.duck_tool.DuckDuckGoSearchTool","title":"<code>src.tools.implementations.duck_tool.DuckDuckGoSearchTool</code>","text":"<p>               Bases: <code>BaseRESTTool</code></p> <p>A tool for searching the web using DuckDuckGo.</p> <p>This tool sends search queries to DuckDuckGo's HTML interface and extracts search results, including titles, snippets, and URLs. It returns up to 5 search results formatted in Markdown.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the tool, used for registry and invocation.</p> <code>description</code> <code>str</code> <p>Human-readable description of the tool's purpose.</p> <code>parameters</code> <code>dict</code> <p>JSON schema for the tool's parameters.</p> Source code in <code>src/tools/implementations/duck_tool.py</code> <pre><code>class DuckDuckGoSearchTool(BaseRESTTool):\n    \"\"\"\n    A tool for searching the web using DuckDuckGo.\n\n    This tool sends search queries to DuckDuckGo's HTML interface and extracts\n    search results, including titles, snippets, and URLs. It returns up to 5\n    search results formatted in Markdown.\n\n    Attributes:\n        name (str): The name of the tool, used for registry and invocation.\n        description (str): Human-readable description of the tool's purpose.\n        parameters (dict): JSON schema for the tool's parameters.\n    \"\"\"\n\n    name = \"duckduckgo_search\"\n\n    def __init__(self, config: Optional[Dict] = None):\n        \"\"\"\n        Initialize the DuckDuckGo Search Tool.\n\n        Args:\n            config (Optional[Dict]): Configuration options for the tool,\n                which will override the default configuration.\n\n        Returns:\n            None\n        \"\"\"\n        default_config = {\n            \"endpoint_url\": \"https://html.duckduckgo.com/html/\",\n            \"strict\": False\n        }\n        if config:\n            default_config.update(config)\n\n        super().__init__(config=default_config)\n\n        self.description = \"Search DuckDuckGo for information on a specific query.\"\n        self.parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The search query to look up on DuckDuckGo.\"\n                }\n            },\n            \"required\": [\"query\"],\n            \"additionalProperties\": False\n        }\n\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n        \"\"\"\n        Execute a DuckDuckGo search.\n\n        This method takes a query parameter and searches DuckDuckGo for relevant\n        results, then formats them for display.\n\n        Args:\n            context (Optional[StreamContext]): Execution context for the tool.\n            **kwargs: Keyword arguments containing the search parameters.\n                Required:\n                    query (str): The search query to send to DuckDuckGo.\n\n        Returns:\n            ToolResponse: A response object containing the search results\n                formatted in Markdown.\n\n        Raises:\n            ValueError: If the required 'query' parameter is missing.\n        \"\"\"\n        query = kwargs.get(\"query\")\n        if not query:\n            raise ValueError(\"The 'query' parameter is required.\")\n\n        self.logger.info(f\"Searching DuckDuckGo for: {query}\")\n\n        # Perform the search\n        result = await self._search_duckduckgo(query)\n\n        return ToolResponse(result=result, context=None)\n\n    def parse_output(self, output: str) -&gt; str:\n        \"\"\"\n        Parse the raw output from DuckDuckGo.\n\n        This method processes the HTML response from DuckDuckGo to extract\n        relevant information. In this implementation, it is not directly used\n        as the parsing is done in the _search_duckduckgo method for better\n        error handling and control.\n\n        Args:\n            output (str): The raw HTML response from DuckDuckGo.\n\n        Returns:\n            str: Processed output as a string.\n\n        Note:\n            This method is required to implement the BaseRESTTool abstract class,\n            but in this implementation, the actual parsing is done in _search_duckduckgo.\n        \"\"\"\n        # The actual parsing is done in _search_duckduckgo for better control\n        # This method is here to satisfy the BaseRESTTool abstract class requirement\n        return output\n\n    async def _search_duckduckgo(self, query: str) -&gt; str:\n        \"\"\"\n        Search DuckDuckGo and parse the results.\n\n        This method sends a POST request to DuckDuckGo's HTML interface,\n        parses the returned HTML to extract search results, and formats\n        them in Markdown.\n\n        Args:\n            query (str): The search query to send to DuckDuckGo.\n\n        Returns:\n            str: Markdown-formatted search results, including titles,\n                snippets, and links. Returns an error message if the\n                search fails.\n\n        Raises:\n            No exceptions are raised; errors are caught and returned\n            as error messages in the result string.\n        \"\"\"\n        import aiohttp\n\n        url = \"https://html.duckduckgo.com/html/\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n            \"Origin\": \"https://html.duckduckgo.com\",\n            \"Referer\": \"https://html.duckduckgo.com/\",\n            \"Connection\": \"keep-alive\"\n        }\n        data = {\n            \"q\": query,\n            \"b\": \"\"\n        }\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(url, headers=headers, data=data) as response:\n                    if response.status != 200:\n                        return f\"Error: DuckDuckGo returned status code {response.status}\"\n\n                    html_content = await response.text()\n\n                    # Parse the DuckDuckGo search results\n                    results = []\n                    soup = BeautifulSoup(html_content, \"html.parser\")\n\n                    for result in soup.select(\".result\"):\n                        try:\n                            title_element = result.select_one(\".result__a\")\n                            snippet_element = result.select_one(\".result__snippet\")\n\n                            if not title_element:\n                                continue\n\n                            title = title_element.get_text().strip()\n                            url = title_element.get(\"href\", \"\")\n\n                            # Extract the actual URL from DuckDuckGo's redirect URL\n                            if url.startswith(\"/\"):\n                                url_match = re.search(r'uddg=([^&amp;]+)', url)\n                                if url_match:\n                                    url = unquote(url_match.group(1))\n\n                            snippet = snippet_element.get_text().strip() if snippet_element else \"No description available.\"\n\n                            results.append({\n                                \"title\": title,\n                                \"url\": url,\n                                \"snippet\": snippet\n                            })\n\n                            if len(results) &gt;= 5:\n                                break\n                        except Exception as e:\n                            self.logger.error(f\"Error parsing result: {str(e)}\")\n                            continue\n\n                    if not results:\n                        return \"No search results found on DuckDuckGo.\"\n\n                    # Format the results\n                    formatted_output = \"## DuckDuckGo Search Results\\n\\n\"\n                    for i, result in enumerate(results, 1):\n                        formatted_output += f\"### {i}. {result['title']}\\n\"\n                        formatted_output += f\"{result['snippet']}\\n\"\n                        formatted_output += f\"[Link]({result['url']})\\n\\n\"\n\n                    formatted_output += \"These results are from DuckDuckGo and may not reflect the latest information.\"\n                    return formatted_output\n        except Exception as e:\n            self.logger.error(f\"DuckDuckGo search error: {str(e)}\")\n            return f\"Error searching DuckDuckGo: {str(e)}\"\n</code></pre>"},{"location":"reference/tools/implementations/duckduckgo_tool/#src.tools.implementations.duck_tool.DuckDuckGoSearchTool.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the DuckDuckGo Search Tool.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Dict]</code> <p>Configuration options for the tool, which will override the default configuration.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/tools/implementations/duck_tool.py</code> <pre><code>def __init__(self, config: Optional[Dict] = None):\n    \"\"\"\n    Initialize the DuckDuckGo Search Tool.\n\n    Args:\n        config (Optional[Dict]): Configuration options for the tool,\n            which will override the default configuration.\n\n    Returns:\n        None\n    \"\"\"\n    default_config = {\n        \"endpoint_url\": \"https://html.duckduckgo.com/html/\",\n        \"strict\": False\n    }\n    if config:\n        default_config.update(config)\n\n    super().__init__(config=default_config)\n\n    self.description = \"Search DuckDuckGo for information on a specific query.\"\n    self.parameters = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"The search query to look up on DuckDuckGo.\"\n            }\n        },\n        \"required\": [\"query\"],\n        \"additionalProperties\": False\n    }\n</code></pre>"},{"location":"reference/tools/implementations/duckduckgo_tool/#src.tools.implementations.duck_tool.DuckDuckGoSearchTool.execute","title":"<code>execute(context=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a DuckDuckGo search.</p> <p>This method takes a query parameter and searches DuckDuckGo for relevant results, then formats them for display.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[StreamContext]</code> <p>Execution context for the tool.</p> <code>None</code> <code>**kwargs</code> <p>Keyword arguments containing the search parameters. Required:     query (str): The search query to send to DuckDuckGo.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ToolResponse</code> <code>ToolResponse</code> <p>A response object containing the search results formatted in Markdown.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the required 'query' parameter is missing.</p> Source code in <code>src/tools/implementations/duck_tool.py</code> <pre><code>async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n    \"\"\"\n    Execute a DuckDuckGo search.\n\n    This method takes a query parameter and searches DuckDuckGo for relevant\n    results, then formats them for display.\n\n    Args:\n        context (Optional[StreamContext]): Execution context for the tool.\n        **kwargs: Keyword arguments containing the search parameters.\n            Required:\n                query (str): The search query to send to DuckDuckGo.\n\n    Returns:\n        ToolResponse: A response object containing the search results\n            formatted in Markdown.\n\n    Raises:\n        ValueError: If the required 'query' parameter is missing.\n    \"\"\"\n    query = kwargs.get(\"query\")\n    if not query:\n        raise ValueError(\"The 'query' parameter is required.\")\n\n    self.logger.info(f\"Searching DuckDuckGo for: {query}\")\n\n    # Perform the search\n    result = await self._search_duckduckgo(query)\n\n    return ToolResponse(result=result, context=None)\n</code></pre>"},{"location":"reference/tools/implementations/duckduckgo_tool/#src.tools.implementations.duck_tool.DuckDuckGoSearchTool.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parse the raw output from DuckDuckGo.</p> <p>This method processes the HTML response from DuckDuckGo to extract relevant information. In this implementation, it is not directly used as the parsing is done in the _search_duckduckgo method for better error handling and control.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The raw HTML response from DuckDuckGo.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Processed output as a string.</p> Note <p>This method is required to implement the BaseRESTTool abstract class, but in this implementation, the actual parsing is done in _search_duckduckgo.</p> Source code in <code>src/tools/implementations/duck_tool.py</code> <pre><code>def parse_output(self, output: str) -&gt; str:\n    \"\"\"\n    Parse the raw output from DuckDuckGo.\n\n    This method processes the HTML response from DuckDuckGo to extract\n    relevant information. In this implementation, it is not directly used\n    as the parsing is done in the _search_duckduckgo method for better\n    error handling and control.\n\n    Args:\n        output (str): The raw HTML response from DuckDuckGo.\n\n    Returns:\n        str: Processed output as a string.\n\n    Note:\n        This method is required to implement the BaseRESTTool abstract class,\n        but in this implementation, the actual parsing is done in _search_duckduckgo.\n    \"\"\"\n    # The actual parsing is done in _search_duckduckgo for better control\n    # This method is here to satisfy the BaseRESTTool abstract class requirement\n    return output\n</code></pre>"},{"location":"reference/tools/implementations/rag_tool/","title":"RAG Tool","text":""},{"location":"reference/tools/implementations/rag_tool/#src.tools.implementations.rag_tool.RAGTool","title":"<code>src.tools.implementations.rag_tool.RAGTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> Source code in <code>src/tools/implementations/rag_tool.py</code> <pre><code>class RAGTool(BaseTool):\n    name = \"medicare_search\"\n\n    def __init__(self, config: Optional[Dict] = None):\n        super().__init__()\n        self.config = config or {}\n        self.strict = True\n\n        self.description = (\"Tool used to retrieve information from the 'Medicare &amp; You 2025' handbook \"\n                            \"using natural language search. Use this tool when you need information about \"\n                            \"Medicare coverage, enrollment, costs, and benefits.\")\n\n        self.parameters = {\n            'type': 'object',\n            'properties': {\n                'query': {\n                    'type': 'string',\n                    'description': ('Search terms related to Medicare coverage, benefits, enrollment, '\n                                    'costs, or other topics from the Medicare &amp; You 2025 handbook. '\n                                    'Example: \"Medicare Part B coverage limits\" or \"prescription drug plans\"'),\n                }\n            },\n            'required': ['query'],\n            'additionalProperties': False\n        }\n\n        self.logger = logging.getLogger(self.__class__.__name__)\n        elasticsearch_config = self.config.get('connector_config', {})\n\n        # TODO(security): Temporary SSL verification bypass for development.\n        # Must be updated with proper certificate verification before production deployment.\n        self.search_client = ElasticsearchClient(verify_certs=False)\n\n        self.query_builder = ElasticQueryBuilder(elasticsearch_config)\n        self.top_k = self.config.get('top_k', 3)\n        self.query_name = self.config.get('query_name', 'basic_match')\n        self.elasticsearch_timeout = self.config.get('timeouts', {}).get('elasticsearch_timeout', 30)\n        self.max_retries = elasticsearch_config.get('max_retries', 3)\n        self.index_name = elasticsearch_config.get('index_name', 'medicare_handbook_2025')\n\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n        \"\"\"Executes the RAG (Retrieval-Augmented Generation) tool to retrieve Medicare-related content.\n\n        This method performs a search query against an Elasticsearch index to retrieve relevant\n        Medicare documentation based on the provided query string.\n\n        Args:\n            context (Optional[ContextModel], optional): Context information for the execution.\n                Defaults to None.\n            **kwargs: Arbitrary keyword arguments.\n                Required:\n                    query (str): The search query string to find Medicare-related content.\n\n        Returns:\n            ToolResponse: A structured response containing:\n                - result (str): The parsed and formatted retrieved documents\n                - context (Optional[Dict]): Additional execution context (None in this implementation)\n\n        Raises:\n            ValueError: If the 'query' parameter is missing or empty.\n\n        Examples:\n            ```python\n            tool = RAGTool(config=rag_config)\n            response = await tool.execute(query=\"What are Medicare Part B premiums?\")\n            print(response.result)\n            ```\n        \"\"\"\n        query = kwargs.get('query', '')\n        if not query:\n            raise ValueError(\"The 'query' parameter is required.\")\n\n        self.logger.info(\"Executing RAG Tool with query about Medicare: %s\", query)\n\n        # Retrieve content from Elasticsearch\n        retrieved_documents = await self._retrieve_content(\n            user_input=query,\n            index_name=self.index_name,\n            top_k=self.top_k\n        )\n        response = ToolResponse(\n            result=self.parse_output(retrieved_documents),\n            context=None,\n        )\n        return response\n\n    async def _retrieve_content(self, user_input: str, index_name: str, top_k: int = None) -&gt; str:\n        \"\"\"Retrieve content from Elasticsearch based on user query.\n\n        Args:\n            user_input (str): User's query about Medicare.\n            index_name (str): Name of the Elasticsearch index.\n            top_k (int, optional): Number of results to return.\n\n        Returns:\n            str: Concatenated string of retrieved handbook sections.\n\n        Raises:\n            RuntimeError: If retrieval fails.\n            asyncio.TimeoutError: If query times out.\n        \"\"\"\n        self.logger.info(\"Querying Elasticsearch for Medicare handbook content\")\n        top_k = top_k if top_k is not None else self.top_k\n        query_body = self.query_builder.get_query(user_input)\n        self.logger.debug(f\"Elastic query body for Medicare query: {json.dumps(query_body)}\")\n        query_results = None\n\n        # Perform Elasticsearch query with retries and timeout\n        for attempt in range(self.max_retries):\n            try:\n                query_results = await asyncio.wait_for(\n                    self.search_client.search(query_body, index_name, top_k),\n                    timeout=self.elasticsearch_timeout\n                )\n                break  # Exit the loop if successful\n            except asyncio.TimeoutError:\n                self.logger.error(f\"Elasticsearch query timed out (attempt {attempt + 1}/{self.max_retries})\")\n                if attempt + 1 == self.max_retries:\n                    raise  # Raise the exception if max retries reached\n\n        if query_results is None:\n            raise RuntimeError(\"Failed to retrieve Elasticsearch query results for Medicare handbook.\")\n\n        # Extract and sort hits\n        extracted_hits = self.extract_and_sort_hits(query_results, \"text\")\n\n        # Concatenate up to top_k results\n        retrieved_content = \"\\n\\n\".join(extracted_hits[:top_k]) + \"\\n\\n\" + self. get_tool_specific_instruction()\n        return retrieved_content\n\n    @staticmethod\n    def extract_and_sort_hits(response, field_name):\n        \"\"\"Extract and sort hits from Elasticsearch response.\n\n        Args:\n            response: Elasticsearch query response.\n            field_name (str): Field name to extract from hits.\n\n        Returns:\n            List[str]: Sorted list of extracted field values.\n        \"\"\"\n        result = []\n\n        def extract_fields(hit, score):\n            extracted_values = []\n            if field_name in hit[\"fields\"]:\n                extracted_values = hit[\"fields\"][field_name]\n            else:\n                for key, values in hit[\"fields\"].items():\n                    if isinstance(values, list):\n                        for value in values:\n                            if isinstance(value, dict) and field_name in value:\n                                extracted_values = value[field_name]\n                                break\n\n            for value in extracted_values:\n                result.append({field_name: value, \"_score\": score})\n\n        def process_hits(hits):\n            for hit in hits:\n                score = hit[\"_score\"] if hit[\"_score\"] is not None else 0\n                if \"inner_hits\" in hit:\n                    for _, inner_hit_value in hit[\"inner_hits\"].items():\n                        process_hits(inner_hit_value[\"hits\"][\"hits\"])\n                else:\n                    extract_fields(hit, score)\n\n        process_hits(response[\"hits\"][\"hits\"])\n        sorted_result = sorted(result, key=lambda x: x[\"_score\"], reverse=True)\n        return [entry[field_name] for entry in sorted_result]\n\n    def parse_output(self, output: str):\n        \"\"\"Parse and format the retrieved content.\n\n        Args:\n            output (str): Raw content from Elasticsearch.\n\n        Returns:\n            str: Formatted content with context header.\n        \"\"\"\n        if not output:\n            return \"No relevant information found in the Medicare &amp; You 2025 handbook.\"\n\n        # Return the output with a context header\n        return (\n            \"## Retrieved Content from 'Medicare &amp; You 2025' Handbook ##\\n\\n\"\n            f\"{output}\\n\\n\"\n            \"Note: This content is retrieved directly from the Medicare &amp; You 2025 handbook. \"\n            \"For the most up-to-date information, please visit Medicare.gov or call 1-800-MEDICARE.\"\n        )\n\n    def get_tool_specific_instruction(self) -&gt; str:\n        return (\n            \"This tool searches through the content of the 'Medicare &amp; You 2025' \"\n            \"handbook. Please be concise and direct in your answers, basing them off of the retrieved content.\"\n        )\n</code></pre>"},{"location":"reference/tools/implementations/rag_tool/#src.tools.implementations.rag_tool.RAGTool.execute","title":"<code>execute(context=None, **kwargs)</code>  <code>async</code>","text":"<p>Executes the RAG (Retrieval-Augmented Generation) tool to retrieve Medicare-related content.</p> <p>This method performs a search query against an Elasticsearch index to retrieve relevant Medicare documentation based on the provided query string.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[ContextModel]</code> <p>Context information for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments. Required:     query (str): The search query string to find Medicare-related content.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ToolResponse</code> <code>ToolResponse</code> <p>A structured response containing: - result (str): The parsed and formatted retrieved documents - context (Optional[Dict]): Additional execution context (None in this implementation)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'query' parameter is missing or empty.</p> <p>Examples:</p> <pre><code>tool = RAGTool(config=rag_config)\nresponse = await tool.execute(query=\"What are Medicare Part B premiums?\")\nprint(response.result)\n</code></pre> Source code in <code>src/tools/implementations/rag_tool.py</code> <pre><code>async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n    \"\"\"Executes the RAG (Retrieval-Augmented Generation) tool to retrieve Medicare-related content.\n\n    This method performs a search query against an Elasticsearch index to retrieve relevant\n    Medicare documentation based on the provided query string.\n\n    Args:\n        context (Optional[ContextModel], optional): Context information for the execution.\n            Defaults to None.\n        **kwargs: Arbitrary keyword arguments.\n            Required:\n                query (str): The search query string to find Medicare-related content.\n\n    Returns:\n        ToolResponse: A structured response containing:\n            - result (str): The parsed and formatted retrieved documents\n            - context (Optional[Dict]): Additional execution context (None in this implementation)\n\n    Raises:\n        ValueError: If the 'query' parameter is missing or empty.\n\n    Examples:\n        ```python\n        tool = RAGTool(config=rag_config)\n        response = await tool.execute(query=\"What are Medicare Part B premiums?\")\n        print(response.result)\n        ```\n    \"\"\"\n    query = kwargs.get('query', '')\n    if not query:\n        raise ValueError(\"The 'query' parameter is required.\")\n\n    self.logger.info(\"Executing RAG Tool with query about Medicare: %s\", query)\n\n    # Retrieve content from Elasticsearch\n    retrieved_documents = await self._retrieve_content(\n        user_input=query,\n        index_name=self.index_name,\n        top_k=self.top_k\n    )\n    response = ToolResponse(\n        result=self.parse_output(retrieved_documents),\n        context=None,\n    )\n    return response\n</code></pre>"},{"location":"reference/tools/implementations/rag_tool/#src.tools.implementations.rag_tool.RAGTool.extract_and_sort_hits","title":"<code>extract_and_sort_hits(response, field_name)</code>  <code>staticmethod</code>","text":"<p>Extract and sort hits from Elasticsearch response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <p>Elasticsearch query response.</p> required <code>field_name</code> <code>str</code> <p>Field name to extract from hits.</p> required <p>Returns:</p> Type Description <p>List[str]: Sorted list of extracted field values.</p> Source code in <code>src/tools/implementations/rag_tool.py</code> <pre><code>@staticmethod\ndef extract_and_sort_hits(response, field_name):\n    \"\"\"Extract and sort hits from Elasticsearch response.\n\n    Args:\n        response: Elasticsearch query response.\n        field_name (str): Field name to extract from hits.\n\n    Returns:\n        List[str]: Sorted list of extracted field values.\n    \"\"\"\n    result = []\n\n    def extract_fields(hit, score):\n        extracted_values = []\n        if field_name in hit[\"fields\"]:\n            extracted_values = hit[\"fields\"][field_name]\n        else:\n            for key, values in hit[\"fields\"].items():\n                if isinstance(values, list):\n                    for value in values:\n                        if isinstance(value, dict) and field_name in value:\n                            extracted_values = value[field_name]\n                            break\n\n        for value in extracted_values:\n            result.append({field_name: value, \"_score\": score})\n\n    def process_hits(hits):\n        for hit in hits:\n            score = hit[\"_score\"] if hit[\"_score\"] is not None else 0\n            if \"inner_hits\" in hit:\n                for _, inner_hit_value in hit[\"inner_hits\"].items():\n                    process_hits(inner_hit_value[\"hits\"][\"hits\"])\n            else:\n                extract_fields(hit, score)\n\n    process_hits(response[\"hits\"][\"hits\"])\n    sorted_result = sorted(result, key=lambda x: x[\"_score\"], reverse=True)\n    return [entry[field_name] for entry in sorted_result]\n</code></pre>"},{"location":"reference/tools/implementations/rag_tool/#src.tools.implementations.rag_tool.RAGTool.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parse and format the retrieved content.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Raw content from Elasticsearch.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Formatted content with context header.</p> Source code in <code>src/tools/implementations/rag_tool.py</code> <pre><code>def parse_output(self, output: str):\n    \"\"\"Parse and format the retrieved content.\n\n    Args:\n        output (str): Raw content from Elasticsearch.\n\n    Returns:\n        str: Formatted content with context header.\n    \"\"\"\n    if not output:\n        return \"No relevant information found in the Medicare &amp; You 2025 handbook.\"\n\n    # Return the output with a context header\n    return (\n        \"## Retrieved Content from 'Medicare &amp; You 2025' Handbook ##\\n\\n\"\n        f\"{output}\\n\\n\"\n        \"Note: This content is retrieved directly from the Medicare &amp; You 2025 handbook. \"\n        \"For the most up-to-date information, please visit Medicare.gov or call 1-800-MEDICARE.\"\n    )\n</code></pre>"},{"location":"reference/tools/implementations/weather_tool/","title":"Weather Tool","text":""},{"location":"reference/tools/implementations/weather_tool/#src.tools.implementations.weather_tool.WeatherTool","title":"<code>src.tools.implementations.weather_tool.WeatherTool</code>","text":"<p>               Bases: <code>BaseRESTTool</code></p> Source code in <code>src/tools/implementations/weather_tool.py</code> <pre><code>class WeatherTool(BaseRESTTool):\n    name = \"weather\"\n\n    def __init__(self, config: Optional[Dict] = None):\n        super().__init__(config=config)\n        self.description = 'Get current temperature and weather information for a specified location.'\n        self.strict = False\n\n        self.parameters = {\n            'type': 'object',\n            'properties': {\n                'city': {\n                    'type': 'string',\n                    'description': 'City name (e.g., \"London\", \"New York\")'\n                },\n                'state_code': {\n                    'type': 'string',\n                    'description': 'US state code (e.g., \"NY\", \"CA\"). Only applicable for US locations.'\n                },\n                'country_code': {\n                    'type': 'string',\n                    'description': 'Two-letter country code (e.g., \"US\", \"GB\"). Optional, helps disambiguate cities.'\n                },\n                'units': {\n                    'type': 'string',\n                    'description': 'Temperature units: Use \"metric\" for Celsius, \"imperial\" for Fahrenheit, or \"standard\" for Kelvin. Defaults to metric.'\n                },\n                'lang': {\n                    'type': 'string',\n                    'description': 'Language for weather descriptions (e.g., \"en\", \"es\", \"fr\"). Defaults to \"en\".'\n                }\n            },\n            'required': ['city'],\n            'additionalProperties': False\n        }\n\n        self.content_type = \"application/json\"\n        self.rate_limit = 60\n        self.default_timeout = 10\n        self.max_retries = 3\n        self.retry_delay = 1.0\n\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n        \"\"\"Execute a weather data request.\n\n        Args:\n            context (Optional[Dict]): Additional context for the request.\n            **kwargs: Must include:\n\n                - city (str): City name\n                Optional:\n                - state_code (str): US state code\n                - country_code (str): Two-letter country code\n                - units (str): 'metric', 'imperial', or 'standard'\n                - lang (str): Language code\n\n        Returns:\n            str: Formatted weather information.\n\n        Raises:\n            ValueError: If required parameters or API key are missing.\n        \"\"\"\n        # Extract and validate parameters\n        city = kwargs.get('city')\n        if not city:\n            raise ValueError(\"City parameter is required\")\n\n        # Get API key from environment\n        api_key = os.getenv(self.api_key_env)\n        if not api_key:\n            raise ValueError(f\"API key not found in environment variable: {self.api_key_env}\")\n\n        state_code = kwargs.get('state_code')\n        country_code = kwargs.get('country_code')\n        units = kwargs.get('units', 'metric')\n        lang = kwargs.get('lang', 'en')\n\n        # Construct the location query\n        location_parts = [city]\n        if state_code and country_code == 'US':\n            location_parts.append(state_code)\n        if country_code:\n            location_parts.append(country_code)\n\n        location = ','.join(location_parts)\n\n        # Prepare query parameters\n        query_params = {\n            'q': location,\n            'units': units,\n            'lang': lang,\n            'appid': api_key  # OpenWeatherMap expects the API key as 'appid'\n        }\n\n        # Log the request details\n        self.logger.info(f\"Making weather request for location: {location}\")\n        self.logger.info(f\"Query parameters: {query_params}\")\n        self.logger.debug(f\"Context: {context}\")\n\n        # Make the API request using parent class method\n        response = await self.make_request(\n            method=\"GET\",\n            params=query_params,\n            endpoint_url=self.endpoint,\n            response_format=ResponseFormat.JSON,\n            use_token=False,  # OpenWeatherMap uses an API key\n            additional_headers={\n                \"Accept\": \"application/json\",\n                \"User-Agent\": \"WeatherTool/1.0\"\n            }\n        )\n        response = ToolResponse(\n            result=self.parse_output(response),\n            context=None,\n        )\n        return response\n\n    def parse_output(self, output: Any) -&gt; str:\n        \"\"\"Parse and format the weather API response.\n\n        Args:\n            output (Any): Raw API response data.\n\n        Returns:\n            str: Formatted weather information including location,\n                temperature, humidity, wind, etc.\n\n        Note:\n            Handles error responses and includes unit-specific information\n            in the output.\n        \"\"\"\n        try:\n            if not isinstance(output, dict):\n                return str(output)\n\n            if 'error' in output:\n                return f\"Error: {output['error']}\"\n\n            if output.get('cod') != 200:\n                error_msg = output.get('message', 'Unknown error')\n                return f\"Error: {error_msg}\"\n\n            # Format the weather data into a more readable structure\n            weather_info = {\n                'location': {\n                    'city': output.get('name'),\n                    'country': output.get('sys', {}).get('country'),\n                    'coordinates': {\n                        'latitude': output.get('coord', {}).get('lat'),\n                        'longitude': output.get('coord', {}).get('lon')\n                    }\n                },\n                'current_weather': {\n                    'temperature': output.get('main', {}).get('temp'),\n                    'feels_like': output.get('main', {}).get('feels_like'),\n                    'humidity': output.get('main', {}).get('humidity'),\n                    'pressure': output.get('main', {}).get('pressure'),\n                    'description': output.get('weather', [{}])[0].get('description'),\n                    'main': output.get('weather', [{}])[0].get('main'),\n                    'wind': {\n                        'speed': output.get('wind', {}).get('speed'),\n                        'direction': output.get('wind', {}).get('deg')\n                    },\n                    'clouds': output.get('clouds', {}).get('all'),\n                    'visibility': output.get('visibility')\n                }\n            }\n\n            formatted_output = format_json_to_document(weather_info)\n            formatted_output += self.get_tool_specific_instruction()\n\n            return formatted_output\n\n        except Exception as e:\n            self.logger.error(f\"Failed to parse weather data: {e}\", exc_info=True)\n            return \"An error occurred while parsing the weather information.\"\n\n    def get_tool_specific_instruction(self) -&gt; str:\n        \"\"\"Get tool-specific instructions about weather data units.\n\n        Returns:\n            str: Instructions about weather data units and measurements.\n        \"\"\"\n        return (\n            \"\\n\\n\"\n            \"## Weather Information Notes: ##\\n\"\n            \"- Temperature and feels_like are in the requested units (Celsius for metric, Fahrenheit for imperial)\\n\"\n            \"- Wind speed is in meters/sec for metric, miles/hour for imperial\\n\"\n            \"- Visibility is in meters\\n\"\n            \"- Pressure is in hPa (hectopascals)\"\n        )\n</code></pre>"},{"location":"reference/tools/implementations/weather_tool/#src.tools.implementations.weather_tool.WeatherTool.execute","title":"<code>execute(context=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a weather data request.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[Dict]</code> <p>Additional context for the request.</p> <code>None</code> <code>**kwargs</code> <p>Must include:</p> <ul> <li>city (str): City name Optional:</li> <li>state_code (str): US state code</li> <li>country_code (str): Two-letter country code</li> <li>units (str): 'metric', 'imperial', or 'standard'</li> <li>lang (str): Language code</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>ToolResponse</code> <p>Formatted weather information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters or API key are missing.</p> Source code in <code>src/tools/implementations/weather_tool.py</code> <pre><code>async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n    \"\"\"Execute a weather data request.\n\n    Args:\n        context (Optional[Dict]): Additional context for the request.\n        **kwargs: Must include:\n\n            - city (str): City name\n            Optional:\n            - state_code (str): US state code\n            - country_code (str): Two-letter country code\n            - units (str): 'metric', 'imperial', or 'standard'\n            - lang (str): Language code\n\n    Returns:\n        str: Formatted weather information.\n\n    Raises:\n        ValueError: If required parameters or API key are missing.\n    \"\"\"\n    # Extract and validate parameters\n    city = kwargs.get('city')\n    if not city:\n        raise ValueError(\"City parameter is required\")\n\n    # Get API key from environment\n    api_key = os.getenv(self.api_key_env)\n    if not api_key:\n        raise ValueError(f\"API key not found in environment variable: {self.api_key_env}\")\n\n    state_code = kwargs.get('state_code')\n    country_code = kwargs.get('country_code')\n    units = kwargs.get('units', 'metric')\n    lang = kwargs.get('lang', 'en')\n\n    # Construct the location query\n    location_parts = [city]\n    if state_code and country_code == 'US':\n        location_parts.append(state_code)\n    if country_code:\n        location_parts.append(country_code)\n\n    location = ','.join(location_parts)\n\n    # Prepare query parameters\n    query_params = {\n        'q': location,\n        'units': units,\n        'lang': lang,\n        'appid': api_key  # OpenWeatherMap expects the API key as 'appid'\n    }\n\n    # Log the request details\n    self.logger.info(f\"Making weather request for location: {location}\")\n    self.logger.info(f\"Query parameters: {query_params}\")\n    self.logger.debug(f\"Context: {context}\")\n\n    # Make the API request using parent class method\n    response = await self.make_request(\n        method=\"GET\",\n        params=query_params,\n        endpoint_url=self.endpoint,\n        response_format=ResponseFormat.JSON,\n        use_token=False,  # OpenWeatherMap uses an API key\n        additional_headers={\n            \"Accept\": \"application/json\",\n            \"User-Agent\": \"WeatherTool/1.0\"\n        }\n    )\n    response = ToolResponse(\n        result=self.parse_output(response),\n        context=None,\n    )\n    return response\n</code></pre>"},{"location":"reference/tools/implementations/weather_tool/#src.tools.implementations.weather_tool.WeatherTool.get_tool_specific_instruction","title":"<code>get_tool_specific_instruction()</code>","text":"<p>Get tool-specific instructions about weather data units.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Instructions about weather data units and measurements.</p> Source code in <code>src/tools/implementations/weather_tool.py</code> <pre><code>def get_tool_specific_instruction(self) -&gt; str:\n    \"\"\"Get tool-specific instructions about weather data units.\n\n    Returns:\n        str: Instructions about weather data units and measurements.\n    \"\"\"\n    return (\n        \"\\n\\n\"\n        \"## Weather Information Notes: ##\\n\"\n        \"- Temperature and feels_like are in the requested units (Celsius for metric, Fahrenheit for imperial)\\n\"\n        \"- Wind speed is in meters/sec for metric, miles/hour for imperial\\n\"\n        \"- Visibility is in meters\\n\"\n        \"- Pressure is in hPa (hectopascals)\"\n    )\n</code></pre>"},{"location":"reference/tools/implementations/weather_tool/#src.tools.implementations.weather_tool.WeatherTool.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parse and format the weather API response.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>Raw API response data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted weather information including location, temperature, humidity, wind, etc.</p> Note <p>Handles error responses and includes unit-specific information in the output.</p> Source code in <code>src/tools/implementations/weather_tool.py</code> <pre><code>def parse_output(self, output: Any) -&gt; str:\n    \"\"\"Parse and format the weather API response.\n\n    Args:\n        output (Any): Raw API response data.\n\n    Returns:\n        str: Formatted weather information including location,\n            temperature, humidity, wind, etc.\n\n    Note:\n        Handles error responses and includes unit-specific information\n        in the output.\n    \"\"\"\n    try:\n        if not isinstance(output, dict):\n            return str(output)\n\n        if 'error' in output:\n            return f\"Error: {output['error']}\"\n\n        if output.get('cod') != 200:\n            error_msg = output.get('message', 'Unknown error')\n            return f\"Error: {error_msg}\"\n\n        # Format the weather data into a more readable structure\n        weather_info = {\n            'location': {\n                'city': output.get('name'),\n                'country': output.get('sys', {}).get('country'),\n                'coordinates': {\n                    'latitude': output.get('coord', {}).get('lat'),\n                    'longitude': output.get('coord', {}).get('lon')\n                }\n            },\n            'current_weather': {\n                'temperature': output.get('main', {}).get('temp'),\n                'feels_like': output.get('main', {}).get('feels_like'),\n                'humidity': output.get('main', {}).get('humidity'),\n                'pressure': output.get('main', {}).get('pressure'),\n                'description': output.get('weather', [{}])[0].get('description'),\n                'main': output.get('weather', [{}])[0].get('main'),\n                'wind': {\n                    'speed': output.get('wind', {}).get('speed'),\n                    'direction': output.get('wind', {}).get('deg')\n                },\n                'clouds': output.get('clouds', {}).get('all'),\n                'visibility': output.get('visibility')\n            }\n        }\n\n        formatted_output = format_json_to_document(weather_info)\n        formatted_output += self.get_tool_specific_instruction()\n\n        return formatted_output\n\n    except Exception as e:\n        self.logger.error(f\"Failed to parse weather data: {e}\", exc_info=True)\n        return \"An error occurred while parsing the weather information.\"\n</code></pre>"},{"location":"reference/tools/implementations/wikipedia_tool/","title":"Wikipedia tool","text":""},{"location":"reference/tools/implementations/wikipedia_tool/#src.tools.implementations.wikipedia_tool.WikipediaTool","title":"<code>src.tools.implementations.wikipedia_tool.WikipediaTool</code>","text":"<p>               Bases: <code>BaseRESTTool</code></p> Source code in <code>src/tools/implementations/wikipedia_tool.py</code> <pre><code>class WikipediaTool(BaseRESTTool):\n    name = \"wikipedia\"\n\n    def __init__(self, config: Optional[Dict] = None):\n        \"\"\"\n        Initialize the WikipediaTool with configuration options.\n        \"\"\"\n        super().__init__(config=config)\n        self.description = \"Fetch a summary of a Wikipedia page for a given query.\"\n        self.strict = False\n\n        self.parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The Wikipedia page title or search query to retrieve the summary for.\"\n                },\n                \"lang\": {\n                    \"type\": \"string\",\n                    \"description\": \"The language code for Wikipedia (e.g., 'en', 'es', 'fr'). Defaults to 'en'.\"\n                }\n            },\n            \"required\": [\"query\"],\n            \"additionalProperties\": False\n        }\n\n        # Set request settings\n        self.content_type = \"application/json\"\n        self.rate_limit = 30\n        self.default_timeout = 10\n        self.max_retries = 3\n        self.retry_delay = 1.0\n\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n        \"\"\"\n        Execute the Wikipedia API call.\n\n        Args:\n            context (Optional[ContextModel]): Additional context for the request.\n            **kwargs: Should include:\n                - query (str): The Wikipedia page title or search query.\n                - lang (str, optional): The language code (default is \"en\").\n\n        Returns:\n            ToolResponse: The tool's response with the Wikipedia summary.\n        \"\"\"\n        query = kwargs.get(\"query\")\n        if not query:\n            raise ValueError(\"The 'query' parameter is required.\")\n\n        lang = kwargs.get(\"lang\", \"en\")\n        encoded_query = quote(query)\n        endpoint_url = self.endpoint.format(**{\"lang\": lang, \"encoded_query\": encoded_query})\n\n        self.logger.info(f\"Fetching Wikipedia summary for query: '{query}' in language: '{lang}'\")\n        self.logger.debug(f\"Endpoint URL: {endpoint_url}\")\n        self.logger.debug(f\"Context: {context}\")\n\n        response = await self.make_request(\n            method=\"GET\",\n            endpoint_url=endpoint_url,\n            response_format=ResponseFormat.JSON,\n            use_token=False,  # No token required for the Wikipedia API.\n            additional_headers={\n                \"Accept\": \"application/json\",\n                \"User-Agent\": \"WikipediaTool/1.0\"\n            }\n        )\n\n        tool_response = ToolResponse(\n            result=self.parse_output(response),\n            context=None,\n        )\n        return tool_response\n\n    def parse_output(self, output: Any) -&gt; str:\n        \"\"\"\n        Parse and format the Wikipedia API response.\n\n        Args:\n            output (Any): Raw API response data.\n\n        Returns:\n            str: A formatted string containing the page title, summary, and a link to the full page.\n        \"\"\"\n        try:\n            if not isinstance(output, dict):\n                return str(output)\n\n            # Check if the API indicates a missing page or error\n            if output.get(\"type\") == \"https://mediawiki.org/wiki/HyperSwitch/errors/not_found\":\n                return f\"Error: The page for the given query was not found on Wikipedia.\"\n\n            if \"detail\" in output:\n                return f\"Error: {output['detail']}\"\n\n            summary_data = {\n                \"title\": output.get(\"title\"),\n                \"summary\": output.get(\"extract\"),\n                \"page_url\": output.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\"),\n            }\n\n            formatted_output = format_json_to_document(summary_data)\n            formatted_output += self.get_tool_specific_instruction()\n            return formatted_output\n\n        except Exception as e:\n            self.logger.error(f\"Failed to parse Wikipedia data: {e}\", exc_info=True)\n            return \"An error occurred while parsing the Wikipedia summary.\"\n\n    def get_tool_specific_instruction(self) -&gt; str:\n        \"\"\"\n        Optional instructions to append to the output.\n\n        Returns:\n            str: Additional information.\n        \"\"\"\n        return \"\\n\\nFor more details, please visit the Wikipedia page.\"\n</code></pre>"},{"location":"reference/tools/implementations/wikipedia_tool/#src.tools.implementations.wikipedia_tool.WikipediaTool.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the WikipediaTool with configuration options.</p> Source code in <code>src/tools/implementations/wikipedia_tool.py</code> <pre><code>def __init__(self, config: Optional[Dict] = None):\n    \"\"\"\n    Initialize the WikipediaTool with configuration options.\n    \"\"\"\n    super().__init__(config=config)\n    self.description = \"Fetch a summary of a Wikipedia page for a given query.\"\n    self.strict = False\n\n    self.parameters = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"The Wikipedia page title or search query to retrieve the summary for.\"\n            },\n            \"lang\": {\n                \"type\": \"string\",\n                \"description\": \"The language code for Wikipedia (e.g., 'en', 'es', 'fr'). Defaults to 'en'.\"\n            }\n        },\n        \"required\": [\"query\"],\n        \"additionalProperties\": False\n    }\n\n    # Set request settings\n    self.content_type = \"application/json\"\n    self.rate_limit = 30\n    self.default_timeout = 10\n    self.max_retries = 3\n    self.retry_delay = 1.0\n</code></pre>"},{"location":"reference/tools/implementations/wikipedia_tool/#src.tools.implementations.wikipedia_tool.WikipediaTool.execute","title":"<code>execute(context=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute the Wikipedia API call.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[ContextModel]</code> <p>Additional context for the request.</p> <code>None</code> <code>**kwargs</code> <p>Should include: - query (str): The Wikipedia page title or search query. - lang (str, optional): The language code (default is \"en\").</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ToolResponse</code> <code>ToolResponse</code> <p>The tool's response with the Wikipedia summary.</p> Source code in <code>src/tools/implementations/wikipedia_tool.py</code> <pre><code>async def execute(self, context: Optional[StreamContext] = None, **kwargs) -&gt; ToolResponse:\n    \"\"\"\n    Execute the Wikipedia API call.\n\n    Args:\n        context (Optional[ContextModel]): Additional context for the request.\n        **kwargs: Should include:\n            - query (str): The Wikipedia page title or search query.\n            - lang (str, optional): The language code (default is \"en\").\n\n    Returns:\n        ToolResponse: The tool's response with the Wikipedia summary.\n    \"\"\"\n    query = kwargs.get(\"query\")\n    if not query:\n        raise ValueError(\"The 'query' parameter is required.\")\n\n    lang = kwargs.get(\"lang\", \"en\")\n    encoded_query = quote(query)\n    endpoint_url = self.endpoint.format(**{\"lang\": lang, \"encoded_query\": encoded_query})\n\n    self.logger.info(f\"Fetching Wikipedia summary for query: '{query}' in language: '{lang}'\")\n    self.logger.debug(f\"Endpoint URL: {endpoint_url}\")\n    self.logger.debug(f\"Context: {context}\")\n\n    response = await self.make_request(\n        method=\"GET\",\n        endpoint_url=endpoint_url,\n        response_format=ResponseFormat.JSON,\n        use_token=False,  # No token required for the Wikipedia API.\n        additional_headers={\n            \"Accept\": \"application/json\",\n            \"User-Agent\": \"WikipediaTool/1.0\"\n        }\n    )\n\n    tool_response = ToolResponse(\n        result=self.parse_output(response),\n        context=None,\n    )\n    return tool_response\n</code></pre>"},{"location":"reference/tools/implementations/wikipedia_tool/#src.tools.implementations.wikipedia_tool.WikipediaTool.get_tool_specific_instruction","title":"<code>get_tool_specific_instruction()</code>","text":"<p>Optional instructions to append to the output.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Additional information.</p> Source code in <code>src/tools/implementations/wikipedia_tool.py</code> <pre><code>def get_tool_specific_instruction(self) -&gt; str:\n    \"\"\"\n    Optional instructions to append to the output.\n\n    Returns:\n        str: Additional information.\n    \"\"\"\n    return \"\\n\\nFor more details, please visit the Wikipedia page.\"\n</code></pre>"},{"location":"reference/tools/implementations/wikipedia_tool/#src.tools.implementations.wikipedia_tool.WikipediaTool.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parse and format the Wikipedia API response.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>Raw API response data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the page title, summary, and a link to the full page.</p> Source code in <code>src/tools/implementations/wikipedia_tool.py</code> <pre><code>def parse_output(self, output: Any) -&gt; str:\n    \"\"\"\n    Parse and format the Wikipedia API response.\n\n    Args:\n        output (Any): Raw API response data.\n\n    Returns:\n        str: A formatted string containing the page title, summary, and a link to the full page.\n    \"\"\"\n    try:\n        if not isinstance(output, dict):\n            return str(output)\n\n        # Check if the API indicates a missing page or error\n        if output.get(\"type\") == \"https://mediawiki.org/wiki/HyperSwitch/errors/not_found\":\n            return f\"Error: The page for the given query was not found on Wikipedia.\"\n\n        if \"detail\" in output:\n            return f\"Error: {output['detail']}\"\n\n        summary_data = {\n            \"title\": output.get(\"title\"),\n            \"summary\": output.get(\"extract\"),\n            \"page_url\": output.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\"),\n        }\n\n        formatted_output = format_json_to_document(summary_data)\n        formatted_output += self.get_tool_specific_instruction()\n        return formatted_output\n\n    except Exception as e:\n        self.logger.error(f\"Failed to parse Wikipedia data: {e}\", exc_info=True)\n        return \"An error occurred while parsing the Wikipedia summary.\"\n</code></pre>"},{"location":"reference/utils/","title":"Utils Documentation","text":""},{"location":"reference/utils/#overview","title":"Overview","text":"<p>The Utils module provides essential factory patterns and utility classes that support the core functionality of the system. These utilities focus on creating and managing different types of builders, parsers, and formatters used throughout the application.</p> <pre><code>graph TB\n    A[Factory Module] --&gt; B[PromptBuilderFactory]\n    A --&gt; C[ToolCallParserFactory]\n\n    B --&gt; D[AnthropicPromptBuilder]\n    B --&gt; E[OpenAIPromptBuilder]\n    B --&gt; F[MistralAIPromptBuilder]\n    B --&gt; G[WatsonX Builders]\n\n    C --&gt; H[JSONToolCallParser]\n    C --&gt; I[NonJSONToolCallParser]\n\n    G --&gt; J[GranitePromptBuilder]\n    G --&gt; K[LlamaPromptBuilder]\n    G --&gt; L[MistralPromptBuilder]\n\n    style A stroke:#333,stroke-width:2px\n    style B stroke:#333,stroke-width:2px\n    style C stroke:#333,stroke-width:2px</code></pre>"},{"location":"reference/utils/#components","title":"Components","text":""},{"location":"reference/utils/#factory-classes","title":"Factory Classes","text":"<p><code>PromptBuilderFactory</code></p> <ul> <li>Creates appropriate prompt builders based on vendor type</li> <li>Supports multiple LLM providers:<ul> <li>Anthropic</li> <li>OpenAI</li> <li>MistralAI</li> <li>WatsonX (Granite, Llama, Mistral)</li> </ul> </li> </ul> <p><code>ToolCallParserFactory</code></p> <ul> <li>Generates parser instances for tool calls</li> <li>Supports:<ul> <li>JSON format parsing</li> <li>Non-JSON format parsing</li> </ul> </li> </ul>"},{"location":"reference/utils/#enums","title":"Enums","text":"<p><code>FormatType</code></p> <ul> <li>Defines supported format types for tool calls</li> <li>Used by parser factories</li> <li>Located in <code>factory.md</code></li> </ul>"},{"location":"reference/utils/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/utils/#factory-pattern-benefits","title":"Factory Pattern Benefits","text":"<ol> <li>Abstraction: Hides complex instantiation logic</li> <li>Flexibility: Easy to add new builders and parsers</li> <li>Consistency: Ensures uniform object creation</li> <li>Maintainability: Centralizes creation logic</li> </ol>"},{"location":"reference/utils/#further-documentation","title":"Further Documentation","text":"<ul> <li>See individual builder documentation in prompt_builders directory</li> <li>Check parser implementations in parsers directory</li> <li>Review format types and enums in factory</li> </ul>"},{"location":"reference/utils/factory/","title":"Factory","text":""},{"location":"reference/utils/factory/#src.utils.factory.PromptBuilderFactory","title":"<code>src.utils.factory.PromptBuilderFactory</code>","text":"Source code in <code>src/utils/factory.py</code> <pre><code>class PromptBuilderFactory:\n    @staticmethod\n    def get_prompt_builder(vendor: str) -&gt; BasePromptBuilder:\n        \"\"\"Get a prompt builder instance for the specified vendor.\n\n        Args:\n            vendor (str): Vendor identifier (e.g., 'openai', 'anthropic', 'watsonx', 'mistral_ai')\n\n        Returns:\n            BasePromptBuilder: Appropriate prompt builder instance\n\n        Raises:\n            ValueError: If no prompt builder is available for the specified vendor\n        \"\"\"\n        match vendor.lower():\n            case \"watsonx-granite\":\n                return WatsonXGranitePromptBuilder()\n            case \"watsonx-llama\":\n                return WatsonXLlamaPromptBuilder()\n            case \"watsonx-mistral\":\n                return WatsonXMistralPromptBuilder()\n            case \"openai\":\n                return OpenAIPromptBuilder()\n            case \"anthropic\":\n                return AnthropicPromptBuilder()\n            case \"mistral-ai\":\n                return MistralAIPromptBuilder()\n            case \"openai-compat-granite\":\n                return OpenAICompatGranitePromptBuilder()\n            case \"openai-compat-llama\":\n                return OpenAICompatLlamaPromptBuilder()\n            case \"xai\":\n                return XAIPromptBuilder()\n            case _:\n                raise ValueError(f\"No prompt builder available for vendor: {vendor}\")\n</code></pre>"},{"location":"reference/utils/factory/#src.utils.factory.PromptBuilderFactory.get_prompt_builder","title":"<code>get_prompt_builder(vendor)</code>  <code>staticmethod</code>","text":"<p>Get a prompt builder instance for the specified vendor.</p> <p>Parameters:</p> Name Type Description Default <code>vendor</code> <code>str</code> <p>Vendor identifier (e.g., 'openai', 'anthropic', 'watsonx', 'mistral_ai')</p> required <p>Returns:</p> Name Type Description <code>BasePromptBuilder</code> <code>BasePromptBuilder</code> <p>Appropriate prompt builder instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no prompt builder is available for the specified vendor</p> Source code in <code>src/utils/factory.py</code> <pre><code>@staticmethod\ndef get_prompt_builder(vendor: str) -&gt; BasePromptBuilder:\n    \"\"\"Get a prompt builder instance for the specified vendor.\n\n    Args:\n        vendor (str): Vendor identifier (e.g., 'openai', 'anthropic', 'watsonx', 'mistral_ai')\n\n    Returns:\n        BasePromptBuilder: Appropriate prompt builder instance\n\n    Raises:\n        ValueError: If no prompt builder is available for the specified vendor\n    \"\"\"\n    match vendor.lower():\n        case \"watsonx-granite\":\n            return WatsonXGranitePromptBuilder()\n        case \"watsonx-llama\":\n            return WatsonXLlamaPromptBuilder()\n        case \"watsonx-mistral\":\n            return WatsonXMistralPromptBuilder()\n        case \"openai\":\n            return OpenAIPromptBuilder()\n        case \"anthropic\":\n            return AnthropicPromptBuilder()\n        case \"mistral-ai\":\n            return MistralAIPromptBuilder()\n        case \"openai-compat-granite\":\n            return OpenAICompatGranitePromptBuilder()\n        case \"openai-compat-llama\":\n            return OpenAICompatLlamaPromptBuilder()\n        case \"xai\":\n            return XAIPromptBuilder()\n        case _:\n            raise ValueError(f\"No prompt builder available for vendor: {vendor}\")\n</code></pre>"},{"location":"reference/utils/factory/#src.utils.factory.FormatType","title":"<code>src.utils.factory.FormatType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of supported tool call format types.</p> <p>Attributes:</p> Name Type Description <code>JSON</code> <p>For JSON-formatted tool calls.</p> <code>NON_JSON</code> <p>For non-JSON formatted tool calls.</p> Source code in <code>src/utils/factory.py</code> <pre><code>class FormatType(Enum):\n    \"\"\"Enumeration of supported tool call format types.\n\n    Attributes:\n        JSON: For JSON-formatted tool calls.\n        NON_JSON: For non-JSON formatted tool calls.\n    \"\"\"\n    JSON = \"json_format\"\n    NON_JSON = \"non_json_format\"\n</code></pre>"},{"location":"reference/utils/factory/#src.utils.factory.ToolCallParserFactory","title":"<code>src.utils.factory.ToolCallParserFactory</code>","text":"<p>Factory for creating tool call parser instances.</p> <p>Manages the creation of parsers for different tool call formats, including JSON and non-JSON formats.</p> <p>Attributes:</p> Name Type Description <code>registry</code> <code>Dict</code> <p>Mapping of format types to their parser classes.</p> Example <pre><code>parser = ToolCallParserFactory.get_parser(\n    FormatType.JSON,\n    config={\"clean_tokens\": []}\n)\n</code></pre> Source code in <code>src/utils/factory.py</code> <pre><code>class ToolCallParserFactory:\n    \"\"\"Factory for creating tool call parser instances.\n\n    Manages the creation of parsers for different tool call formats,\n    including JSON and non-JSON formats.\n\n    Attributes:\n        registry (Dict): Mapping of format types to their parser classes.\n\n    Example:\n        ```python\n        parser = ToolCallParserFactory.get_parser(\n            FormatType.JSON,\n            config={\"clean_tokens\": []}\n        )\n        ```\n    \"\"\"\n    registry = {\n        FormatType.JSON: JSONToolCallParser,\n        FormatType.NON_JSON: NonJSONToolCallParser,\n    }\n\n    @staticmethod\n    def get_parser(format_type: FormatType, config: Dict[str, Any]):\n        \"\"\"Get appropriate parser for the specified format type.\n\n        Args:\n            format_type (FormatType): Type of format to parse.\n            config (Dict[str, Any]): Configuration for the parser.\n\n        Returns:\n            BaseToolCallParser: Instance of appropriate parser.\n\n        Raises:\n            ValueError: If format type is not supported.\n        \"\"\"\n        if format_type in ToolCallParserFactory.registry:\n            return ToolCallParserFactory.registry[format_type](config)\n        raise ValueError(f\"Unsupported format: {format_type}\")\n</code></pre>"},{"location":"reference/utils/factory/#src.utils.factory.ToolCallParserFactory.get_parser","title":"<code>get_parser(format_type, config)</code>  <code>staticmethod</code>","text":"<p>Get appropriate parser for the specified format type.</p> <p>Parameters:</p> Name Type Description Default <code>format_type</code> <code>FormatType</code> <p>Type of format to parse.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>Configuration for the parser.</p> required <p>Returns:</p> Name Type Description <code>BaseToolCallParser</code> <p>Instance of appropriate parser.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If format type is not supported.</p> Source code in <code>src/utils/factory.py</code> <pre><code>@staticmethod\ndef get_parser(format_type: FormatType, config: Dict[str, Any]):\n    \"\"\"Get appropriate parser for the specified format type.\n\n    Args:\n        format_type (FormatType): Type of format to parse.\n        config (Dict[str, Any]): Configuration for the parser.\n\n    Returns:\n        BaseToolCallParser: Instance of appropriate parser.\n\n    Raises:\n        ValueError: If format type is not supported.\n    \"\"\"\n    if format_type in ToolCallParserFactory.registry:\n        return ToolCallParserFactory.registry[format_type](config)\n    raise ValueError(f\"Unsupported format: {format_type}\")\n</code></pre>"}]}